{"cells":[{"cell_type":"markdown","metadata":{"id":"Jue2GsHOjcF3"},"source":["# **UNITY XR Streaming - Dataset Playground**"]},{"cell_type":"markdown","metadata":{"id":"652i80gCdCcc"},"source":["# **HOW TO**\n"]},{"cell_type":"markdown","metadata":{"id":"m-syjqt1on9y"},"source":["\n","***Parameters:***\n","\n","    This section contains the parameters that need to be set before running the code.\n","\n","1.   **Variables:** The following parameters need to be set:\n","\n","\n","*   Server and client IP@\n","*   Analysis Interval\n","*   Zoom Interval\n","*   Grouping Time: if desired to group packets by time\n","*   Google Drive folder path\n","\n","\n","\n","***Build:***\n","\n","    This section contains the code that needs to be executed to set up the environment.\n","\n","1.   **Configuration**\n","2.   **File build**\n","\n","Required for wireshark traces results:\n","3.  **Wireshark Traces Load**\n","4.  **Wireshark Figures build**\n","5.  **Wireshark Computations build**\n","\n","Required for WebRTC statistics results:\n","6.  **WebRTC Statistics Load**\n","7.  **WebRTC Figures build**\n","8.  **WebRTC Computations build**\n","\n","Others:\n","9.   **Color palette**\n","\n","***Support:***\n","\n","    This section contains helper functions.\n","\n","1.   **File Demo Helper:** Find existing files in Google Drive and get file paths.\n","\n","***Results:***\n","\n","    This section contains the code for running the analysis and generating the results.\n","\n","Wireshark results:\n","1.   **Run Wireshark Demo**:\n","\n","    Set the list of file paths containing Wireshark traces. The file should have been extracted using Tshark and saved in a csv file containing the following fields:\n","    * ip.src\n","    * ip.dst\n","    * frame.time_relative\n","    * frame.time_delta\n","    * frame.len\n","    * udp.length\n","2.   **Wireshark Demo Figures**:\n","\n","    Generate figures from Wireshark traces.\n","3.   **Wireshark Demo Computations**:\n","\n","    Compute metrics from Wireshark traces.\n","\n","WebRTC statistics results:\n","1.   **Run WebRTC statistics Demo**:\n","\n","    Set the list of file paths containing WebRTC statistics\n","\n","2.   **WebRTC statistics Demo Figures**:\n","\n","    Generate figures from WebRTC statistics.\n","3.   **WebRTC statistics Demo Computations**:\n","\n","    Compute metrics from WebRTC statistics."]},{"cell_type":"markdown","metadata":{"id":"S04ivCJ_n6b9"},"source":["# **PARAMETERS**"]},{"cell_type":"markdown","metadata":{"id":"DTxndlK7qpB_"},"source":["## **Variables**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686864157205,"user":{"displayName":"MIGUEL CASASNOVAS BIELSA","userId":"07921810654043438802"},"user_tz":-120},"id":"zoN0ONnYn1lq"},"outputs":[],"source":["# Set server and client IPs\n","server_ip = \"192.168.50.185\"\n","client_ip = \"192.168.50.128\"\n","\n","# Set the start and end time to filter the complete trace from the start of the communication between client and server (filter)\n","complete_trace_start_time = 7.5\n","complete_trace_end_time = 37.5\n","\n","# Set the start and end time to filter a specific portion of the filtered trace (zoom)\n","specific_portion_start_time = 5\n","specific_portion_end_time = 5.6\n","\n","# Grouping time\n","grouping_time = 0.000\n","\n","# Define the Google Drive folder path\n","directory = '/content/drive/Shareddrives/XR-Wi-Fi-WN/Unity XR streaming/'"]},{"cell_type":"markdown","metadata":{"id":"krMkqFDjn-dB"},"source":["# **BUILD**"]},{"cell_type":"markdown","metadata":{"id":"7YcWvVfxu0Hm"},"source":["## **Configuration**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXsP_TSLGSTQ"},"outputs":[],"source":["# Import necessary libraries\n","from google.colab import drive\n","import os\n","import ipywidgets as widgets\n","from IPython.display import display\n","from typing import List\n","import sys\n","import numpy as np\n","import pandas as pd\n","import plotly.io as pio\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import chardet\n","from collections import Counter\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import json\n","import ast\n","import warnings\n","from datetime import datetime"]},{"cell_type":"markdown","metadata":{"id":"TPx0cFUXvnVu"},"source":["## **File build**"]},{"cell_type":"markdown","metadata":{"id":"12fFVJbEgmlE"},"source":["#### **Mount Google Drive Folder**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zadb2B1cwKYz"},"outputs":[],"source":["# Mount the Google Drive folder\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"e_-zAQ2Ngv2O"},"source":["#### **Dropdown**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Weg6v1amxHX5"},"outputs":[],"source":["def create_dropdown(directory:str):\n","  \"\"\"\n","  Creates a dropdown menu with all the files and subdirectories in the specified directory.\n","\n","  Parameters:\n","    directory (str): The directory path to create the dropdown for.\n","\n","  Returns:\n","    A Dropdown widget containing all the files and subdirectories in the specified directory.\n","  \"\"\"\n","  # Get a list of all files and subdirectories in the specified directory\n","  files = os.listdir(directory)\n","\n","  # Create an empty dictionary to store the options for the dropdown\n","  options = {}\n","\n","  # Loop through all files and subdirectories and add them to the options dictionary\n","  for file in files:\n","    # Get the full path of the file or subdirectory\n","    full_path = os.path.join(directory, file)\n","\n","    # If the item is a file, add it to the options dictionary\n","    if os.path.isfile(full_path):\n","      options[file] = full_path\n","    # If the item is a subdirectory, create a sub-dropdown and add it to the options dictionary\n","    elif os.path.isdir(full_path):\n","      sub_dropdown = create_dropdown(full_path)\n","      options[file] = sub_dropdown\n","\n","  # Create the dropdown widget for the current directory\n","  dropdown = widgets.Dropdown(options=options, value=None, layout=widgets.Layout(width='800px', height='24px'))\n","  return dropdown\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0z7qIQHsvr6i"},"outputs":[],"source":["def update_dropdowns(change: object, dropdown: object, box: object, filePath: dict) -> None:\n","  '''\n","  This function updates the dropdowns based on the user's selection.\n","\n","  Parameters:\n","    change (object): An object representing the change event.\n","    dropdown (object): A Dropdown object.\n","    box (object): A container for the dropdowns.\n","    filePath (dict): A dictionary with a path key representing the selected file path.\n","\n","  Returns:\n","      None\n","  '''\n","  # Get the selected option\n","  selected = change.new\n","\n","  # If the selected option is a dropdown, display it\n","  if isinstance(selected, widgets.Dropdown):\n","    selected_index = list(box.children).index(dropdown)\n","\n","    # Remove the child dropdowns\n","    for child in box.children[selected_index+1:]:\n","      child.value = None\n","    box.children = box.children[:selected_index+1]\n","\n","    # Add the selected dropdown to the container\n","    box.children += (selected,)\n","\n","    # Listen for changes to the new dropdown\n","    selected.observe(lambda change: update_dropdowns(change, selected, box, filePath), names='value')\n","\n","    with output_widget:\n","      output_widget.clear_output()\n","      print(\"\\033[38;2;255;165;0mSelect a file from the directory\\033[0m\")\n","\n","  # If the selected option is a file, print a message\n","  else:\n","    with output_widget:\n","      output_widget.clear_output()\n","      print(\"\\033[38;2;0;255;0mFile selected succesfully.\\033[0m\")\n","      filePath['path'] = change.new"]},{"cell_type":"markdown","metadata":{"id":"1BmfveDbg2qY"},"source":["#### **File locator**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcRFkuDGxLc_"},"outputs":[],"source":["def select_file(directory:str, file_path: dict, output_widget: widgets.Output):\n","  \"\"\"\n","  Displays a dropdown menu with file paths from the given directory and listens for changes to it.\n","  When a file is selected, it updates the file path and clears the output widget.\n","\n","  Args:\n","    directory (str): The directory path to create the dropdown for.\n","    file_path (dict): A dictionary containing the file path to display and update.\n","    output_widget (widgets.Output): The output widget to clear after a file is selected.\n","  \"\"\"\n","  # Create the dropdown menu for the path\n","  top_dropdown = create_dropdown(directory)\n","\n","  # Add the dropdown menu to a Box container\n","  box = widgets.VBox(children=[top_dropdown])\n","\n","  # Listen for changes to the dropdown menu\n","  top_dropdown.observe(lambda change: update_dropdowns(change, top_dropdown, box, file_path), names='value')\n","\n","  # Display the dropdown menus\n","  display(box, output_widget)\n","\n","  # Clear the output widget and print a message\n","  with output_widget:\n","    output_widget.clear_output()\n","    print(\"\\033[38;2;255;165;0mSelect a file from the directory\\033[0m\")"]},{"cell_type":"markdown","metadata":{"id":"7uFE_k0og9KG"},"source":["#### **Extension validation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoxzlbfZyHpV"},"outputs":[],"source":["def check_extension(path: str, ext: str) -> bool:\n","  \"\"\"\n","  Checks if the file extension of the given file path matches the specified extension.\n","\n","  Parameters:\n","    path (str): The file path.\n","    ext (str): The desired file extension.\n","\n","  Returns:\n","    bool: True if the file extension matches the desired extension, False otherwise.\n","  \"\"\"\n","  # Check if the file path ends with the specified extension\n","  if not path.endswith(ext):\n","    return False\n","  else:\n","    # Print message indicating that the correct file has been selected\n","    print(\"\\033[38;2;0;255;0m\", ext, \" file selected.\\033[0m\")\n","    return True\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quCx2k1GZ7dC"},"outputs":[],"source":["def validate_extension(file: dict) -> None:\n","  \"\"\"\n","  Validates if the file extension is valid for visualization purposes.\n","\n","  Parameters:\n","    file: A dictionary that contains the file path and other information about the file.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  # Extract the file path from the dictionary\n","  file_path = file['path']\n","\n","  # Check if the file extension is .csv\n","  if check_extension(file_path, '.csv'):\n","    print(\"The selected file can be used for Wireshark traces visualization\")\n","\n","  # Check if the file extension is .txt or .json\n","  elif check_extension(file_path, '.txt') or check_extension(file_path, '.json'):\n","    print(\"The selected file can be used for WebRTC statistics visualization\")\n","\n","  # If the file extension is not .csv, .txt or .json, display an error message\n","  else:\n","    print(\"\\033[38;2;255;0;0mError: This file can not be used. Please select a .csv, .json or .txt file.\\033[0m\")\n"]},{"cell_type":"markdown","metadata":{"id":"JjiCqAQxhFxb"},"source":["#### **File path getter**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqwR8R5IawhN"},"outputs":[],"source":["def get_file_path(file: dict, directory: str) -> str:\n","  \"\"\"\n","  This function takes in a dictionary object and a path string and returns the file path.\n","\n","  Parameters:\n","    file (dict): A dictionary object that contains the path of the file.\n","    path (str): A string that represents the root directory path.\n","\n","  Returns:\n","    A string that represents the full path of the selected file.\n","  \"\"\"\n","  file_path = file['path']  # Get the path of the file from the dictionary object.\n","  selected_file = os.path.basename(file_path)  # Extract the selected file name.\n","  print(\"\\033[38;2;255;165;0mSelected file: \\033[0m\", selected_file)\n","  return file_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIFQ4BXducDy"},"outputs":[],"source":["def validate_and_retrieve_file_path(file: dict, directory: str) -> None:\n","  \"\"\"\n","  Validate the file path and extension and retrieve the file path.\n","\n","  Parameters:\n","    file (dict): A dictionary containing the path of the file.\n","    directory (str): The directory where the file is located.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  if file is not None:\n","    if file.get('path') is not None:\n","      validate_extension(file)\n","      file_path = get_file_path(file, directory)\n","      print(\"\\033[38;2;255;165;0mFile Path: \\033[0m\", file_path[len(directory):])\n","    else:\n","      print(\"\\033[38;2;255;165;0mPlease locate a file. \\033[0m\")\n","  else:\n","    print(\"\\033[38;2;255;165;0mPlease run the file locator code. \\033[0m\")\n"]},{"cell_type":"markdown","metadata":{"id":"DQYN9hYip3qP"},"source":["## **Wireshark**"]},{"cell_type":"markdown","metadata":{"id":"QMW3pcUoERXr"},"source":["### **Wireshark Traces Load**"]},{"cell_type":"markdown","metadata":{"id":"wFAd8GQJbrKR"},"source":["#### **Load CSV file**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxTS2-tbz4of"},"outputs":[],"source":["def load_csv_file(file_path: str, directory: str) -> pd.DataFrame:\n","  \"\"\"\n","  Load a CSV file into a pandas DataFrame.\n","\n","  Parameters:\n","    file_path (str): The relative or absolute path to the CSV file.\n","    directory (str): The base path to the directory containing the file.\n","\n","  Returns:\n","    A pandas DataFrame with the contents of the CSV file, or None if the file could not be loaded.\n","  \"\"\"\n","  # Combine the base path (if any) with the file path to get the complete path\n","  complete_path = os.path.join(directory, file_path)\n","\n","  df = None\n","\n","  # Detect the encoding of the file\n","  try:\n","      with open(complete_path, 'rb') as f:\n","        encoding_detection = chardet.detect(f.read())\n","\n","      # Load the CSV file into a pandas DataFrame - set delimiter and that first row is the header\n","      df = pd.read_csv(complete_path, encoding=encoding_detection['encoding'], delimiter='\\t', header=0)\n","  except FileNotFoundError:\n","      print(f\"\\033[38;2;255;0;0mError: The selected file \\033[0m '{file_path}'\\033[38;2;255;0;0m does not exist.\\033[0m\")\n","  except UnicodeDecodeError:\n","      print(f\"\\033[38;2;255;0;0mError: The encoding of the selected file \\033[0m'{file_path}'\\033[38;2;255;0;0m could not be detected.\\033[0m\")\n","\n","  return df"]},{"cell_type":"markdown","metadata":{"id":"DjDz0ly5bzvC"},"source":["#### **Dataframe format validation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0M4EE9Zdnda"},"outputs":[],"source":["def validate_wireshark_data(df: pd.DataFrame, file_path: str) -> bool:\n","  \"\"\"\n","  Validates if the input DataFrame has all the required columns.\n","\n","  Parameters:\n","    df (pandas.DataFrame): Input DataFrame to be validated.\n","    file_path (str): File path of the input DataFrame.\n","\n","  Returns:\n","    bool: True if all the required columns are present, else False.\n","  \"\"\"\n","\n","  # List of required columns\n","  required_columns = [\n","    \"ip.src\",\n","    \"ip.dst\",\n","    \"frame.time_relative\",\n","    \"frame.time_delta\",\n","    \"frame.len\",\n","    \"udp.length\",\n","    \"_ws.col.Protocol\",\n","    \"_ws.col.Info\"\n","  ]\n","\n","  # Check if all the required columns are present in the DataFrame\n","  if not all(col in df.columns for col in required_columns):\n","    print(\n","      \"\\033[38;2;255;0;0mError: The selected file \\033[0m\",\n","      file_path,\n","      \" \\033[38;2;255;0;0mdoes not have all the required columns.\\033[0m\"\n","    )\n","    print(\"\\033[38;2;255;0;0mRequired columns:\", required_columns, \"\\033[0m\")\n","    return False\n","  else:\n","    print(\n","      \"\\033[38;2;0;255;0mThe selected file\\033[0m\",\n","      file_path,\n","      \" \\033[38;2;0;255;0mhas all the required columns and will be considered.\\033[0m\"\n","    )\n","    return True\n"]},{"cell_type":"markdown","metadata":{"id":"6i5kX9_jl0sZ"},"source":["#### **Filter Downlink and Uplink traffic**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2wjouq6l1fL"},"outputs":[],"source":["def get_DL_UL_df(df: pd.DataFrame, server_ip: str, client_ip: str) -> tuple:\n","  \"\"\"\n","  Filters the input DataFrame for downlink and uplink traffic.\n","\n","  Parameters:\n","    df (pandas.DataFrame): Input DataFrame containing network traffic data.\n","    server_ip (str): IP address of the server.\n","    client_ip (str): IP address of the client.\n","\n","  Returns:\n","    tuple: A tuple of two DataFrames, where the first DataFrame is the downlink traffic\n","            (from server to client) and the second DataFrame is the uplink traffic\n","            (from client to server).\n","  \"\"\"\n","  df_copy = df.copy() # create a copy of the original DataFrame to avoid modifying it\n","  # Identify the first packet in the communication between server and client\n","  first_packet = df_copy[((df_copy['ip.src'] == server_ip) & (df_copy['ip.dst'] == client_ip)) | ((df_copy['ip.src'] == client_ip) & (df_copy['ip.dst'] == server_ip))].iloc[0]\n","\n","\n","  # Calculate the time offset\n","  time_offset = first_packet['frame.time_relative']\n","\n","\n","  # Adjust the time relative of all packets in the DataFrame\n","  df_copy['frame.time_relative'] -= time_offset\n","\n","\n","  # Filter the DataFrame for downlink traffic\n","  df_dl = df_copy[(df_copy['ip.src'] == server_ip) & (df_copy['ip.dst'] == client_ip)]\n","\n","  # Filter the DataFrame for uplink traffic\n","  df_ul = df_copy[(df_copy['ip.src'] == client_ip) & (df_copy['ip.dst'] == server_ip)]\n","\n","  # Return the filtered DataFrames as a tuple\n","  return df_dl, df_ul\n"]},{"cell_type":"markdown","metadata":{"id":"cbVdS_7YW_EL"},"source":["#### **Filter by time**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-x9F6nIW_bL"},"outputs":[],"source":["def get_time_filtered_dataset(df: pd.DataFrame, start_time: float, end_time: float) -> pd.DataFrame:\n","  \"\"\"\n","  Returns a filtered dataframe with rows between the specified start and end times.\n","\n","  Parameters:\n","    df (pd.DataFrame): Input dataframe containing timestamp information.\n","    start_time (float): The start time in seconds.\n","    end_time (float): The end time in seconds.\n","\n","  Returns:\n","    pd.DataFrame: A filtered dataframe containing rows within the specified time range.\n","  \"\"\"\n","  df_copy = df.copy() # create a copy of the original DataFrame to avoid modifying it\n","  filtered_df = df_copy[(df_copy[\"frame.time_relative\"] >= start_time) & (df_copy[\"frame.time_relative\"] <= end_time)] # filter the rows that fall within the frame.time_relative range\n","  return filtered_df\n"]},{"cell_type":"markdown","metadata":{"id":"KjVkjcOZbZnr"},"source":["#### **Filter SRTP type**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb5qQ4eQbaD8"},"outputs":[],"source":["def get_srtp_type(df: pd.DataFrame) -> pd.DataFrame:\n","  \"\"\"\n","  Checks whether the SRTP protocol is being used for audio or video, and replaces\n","  the value in the '_ws.col.Protocol' column accordingly, It then drops the\n","  '_ws.col.Info' column and returns  the modified DataFrame.\n","\n","  Parameters:\n","    df (Pandas DataFrame): Input DataFrame to be modified\n","\n","  Returns:\n","    df (Pandas DataFrame): Modified DataFrame with '_ws.col.Protocol' values replaced\n","    and '_ws.col.Info' column dropped\n","  \"\"\"\n","  df_copy = df.copy() # create a copy of the original DataFrame to avoid modifying it\n","  # Replace SRTP values in '_ws.col.Protocol' column based on type\n","  df_copy.loc[(df_copy[\"_ws.col.Protocol\"]==\"SRTP\") &\n","          (df_copy[\"_ws.col.Info\"].str.contains(\"PT=DynamicRTP-Type-96\")),\n","          \"_ws.col.Protocol\"] = \"SRTP Audio\"\n","  df_copy.loc[(df_copy[\"_ws.col.Protocol\"] == \"SRTP\") &\n","        (df_copy[\"_ws.col.Info\"].str.contains(\"PT=DynamicRTP-Type-104\") |\n","          df_copy[\"_ws.col.Info\"].str.contains(\"PT=DynamicRTP-Type-102\")|\n","          df_copy[\"_ws.col.Info\"].str.contains(\"PT=DynamicRTP-Type-98\")),\n","        \"_ws.col.Protocol\"] = \"SRTP Video\"\n","\n","  # Drop '_ws.col.Info' column\n","  #df_copy.drop(\"_ws.col.Info\", axis=1, inplace=True)\n","\n","  # Return modified DataFrame\n","  return df_copy\n"]},{"cell_type":"markdown","metadata":{"id":"wKkYEkq6rs1s"},"source":["#### **Get traffic dataframe**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KukSIovErOEQ"},"outputs":[],"source":["def get_traffic_df(df: pd.DataFrame, seconds: float) -> pd.DataFrame:\n","  \"\"\"\n","  Resamples a DataFrame of packet capture data to a specified time interval and calculates the sum of packet lengths.\n","\n","  Parameters:\n","    df (pd.DataFrame): DataFrame containing packet capture data.\n","    seconds (float): Time interval in seconds to resample the packets.\n","\n","  Returns:\n","    traffic_df (pd.DataFrame): DataFrame with resampled packet data and calculated traffic.\n","  \"\"\"\n","\n","  # Convert the time column to datetime format\n","  df_copy = df.copy() # create a copy of the original DataFrame to avoid modifying it\n","  min_time = df_copy['frame.time_relative'].min()\n","  df_copy['frame.time_relative'] = df_copy['frame.time_relative'] - min_time\n","  df_copy['frame.time_relative'] = pd.to_datetime(df_copy['frame.time_relative'], unit='s')\n","  # Resample the packets to the specified time interval and calculate the sum of the packet lengths\n","  resample_str = f'{seconds}S'\n","  traffic_df = df_copy.set_index('frame.time_relative').resample(resample_str).agg({'frame.len': 'sum'}).reset_index()\n","  # Calculate traffic in Mbps and elapsed time in seconds\n","  traffic_df['traffic'] = traffic_df['frame.len'] * 8 / 10**6\n","  traffic_df['seconds'] = (traffic_df['frame.time_relative'] - traffic_df['frame.time_relative'].iloc[0]).dt.total_seconds()\n","\n","  return traffic_df\n"]},{"cell_type":"markdown","metadata":{"id":"zqBKk7nqVYL1"},"source":["#### **Group dataframes by time and protocol**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLSZ3psjVmdW"},"outputs":[],"source":["def group_dataframes(df_list: List[pd.DataFrame], grouping_time: float) -> List[pd.DataFrame]:\n","  \"\"\"\n","  Groups DataFrames based on the time relative and protocol column.\n","\n","  Parameters:\n","    df_list: A list of DataFrames to group.\n","    grouping_time (float): Time interval for grouping the dataframes.\n","\n","  Returns:\n","    A list of grouped DataFrames.\n","  \"\"\"\n","  grouped_dfs = []\n","\n","  # Loop over each DataFrame in the list\n","  for df in df_list:\n","\n","    df_copy = df.copy() # create a copy of the original DataFrame to avoid modifying it\n","    # Sort DataFrame by protocol and frame time\n","    df_copy = df_copy.sort_values(by=['_ws.col.Protocol', 'frame.time_relative'])\n","\n","    # Determine the groups based on the protocol and frame time\n","    groups = (df_copy['_ws.col.Protocol'] != df_copy['_ws.col.Protocol'].shift()) | \\\n","              (df_copy['frame.time_relative'].diff() > grouping_time)\n","\n","    # Group the DataFrame by the calculated groups\n","    grouped_df = df_copy.groupby(groups.cumsum())\n","    # Compute the number of packets for each group\n","    num_pack = grouped_df.count()['ip.src'].rename('num.packets')\n","\n","    # Compute the mean of the frame length and udp length for each group.\n","    mean_frame_lengths = grouped_df['frame.len'].mean().rename('group.frame.len.mean')\n","    mean_udp_lengths = grouped_df['udp.length'].mean().rename('group.udp.length.mean')\n","\n","    # Compute the sum of the frame length and udp length for each group.\n","    tot_frame_lengths = grouped_df['frame.len'].sum().rename('group.frame.len.tot')\n","    tot_udp_lengths = grouped_df['udp.length'].sum().rename('group.udp.length.tot')\n","\n","    # Compute the start and end times for each group.\n","    start_times = grouped_df['frame.time_relative'].min().rename('start.time')\n","    end_times = grouped_df['frame.time_relative'].max().rename('end.time')\n","\n","    # Compute the difference between start and end times for each group.\n","    group_time = (end_times - start_times)*1000\n","\n","    # Compute the average time between packets in the group\n","    avg_time_diff = (group_time / (num_pack - 1))\n","    avg_time_diff = avg_time_diff.rename('avg.time.between.packets')\n","\n","\n","    # Combine the columns and reset the index.\n","    grouped_df = pd.concat([grouped_df.first()[['ip.src', 'ip.dst', '_ws.col.Protocol', 'frame.time_relative', '_ws.col.Info']],\n","                            num_pack, mean_frame_lengths, mean_udp_lengths, tot_frame_lengths, tot_udp_lengths, start_times, end_times ], axis=1)\n","    grouped_df['group.time'] =  group_time\n","    grouped_df['avg.time.between.packets'] = avg_time_diff\n","\n","    # Append the grouped DataFrame to the list of grouped DataFrames\n","    grouped_dfs.append(grouped_df.reset_index(drop=True))\n","\n","  return grouped_dfs\n"]},{"cell_type":"markdown","metadata":{"id":"1BZjW2Joe2pD"},"source":["#### **Get wireshark dataframes**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pD4hYpYgf1W"},"outputs":[],"source":["def get_dataframes(files_path: List[str], directory: str, server_ip: str, client_ip: str,\n","                   complete_trace_start_time : str, complete_trace_end_time : str, specific_portion_start_time : str,\n","                   specific_portion_end_time : str) -> tuple:\n","  \"\"\"\n","  Given a list of file paths, generates a set of pandas dataframes for each file.\n","\n","  Parameters:\n","    files_path (List[str]): A list of file paths.\n","    directory (str): The directory containing the files.\n","    server_ip (str): The server's IP address.\n","    client_ip (str): The client's IP address.\n","    complete_trace_start_time  (str): The start time to consider of the complete trace.\n","    complete_trace_end_time  (str): The end time to consider of the complete trace.\n","    specific_portion_start_time  (str): The start time of the filtered by time trace.\n","    specific_portion_end_time  (str): The end time of the filtered by time trace.\n","\n","  Returns:\n","    A tuple containing the following lists:\n","    - df_list: A list of all dataframes.\n","    - df_dl_list: A list of all downlink dataframes.\n","    - df_ul_list: A list of all uplink dataframes.\n","    - df_dl_z_list: A list of all downlink dataframes filtered by time.\n","    - df_ul_z_list: A list of all uplink dataframes filtered by time.\n","    - files_path_list: A list of file paths that were used to generate the dataframes.\n","  \"\"\"\n","  df_list = []\n","  df_dl_list = []\n","  df_ul_list = []\n","  df_dl_z_list = []\n","  df_ul_z_list = []\n","  files_path_list = []\n","\n","  for file_path in files_path:\n","      # Call the 'run_wireshark_traces' function to generate the dataframes.\n","      df, df_dl, df_ul, df_dl_z, df_ul_z = run_wireshark_traces(file_path, directory, server_ip,\n","                                                                client_ip, complete_trace_start_time , complete_trace_end_time ,\n","                                                                specific_portion_start_time , specific_portion_end_time )\n","\n","      # Append the dataframes to their respective lists.\n","      if df is not None:\n","          df_list.append(df)\n","          df_dl_list.append(df_dl)\n","          df_ul_list.append(df_ul)\n","          df_dl_z_list.append(df_dl_z)\n","          df_ul_z_list.append(df_ul_z)\n","\n","          # Append the file path to the list.\n","          files_path_list.append(file_path)\n","  # Return the lists of dataframes and file paths as a tuple.\n","  return df_list, df_dl_list, df_ul_list, df_dl_z_list, df_ul_z_list, files_path_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxPpX5ZQe4As"},"outputs":[],"source":["def run_wireshark_traces(file_path: str, directory: str, server_ip: str, client_ip: str,\n","                        complete_trace_start_time : float, complete_trace_end_time : float, specific_portion_start_time : str,\n","                        specific_portion_end_time : str) -> tuple:\n","  \"\"\"\n","  Reads a CSV file and extracts various DataFrames.\n","\n","  Parameters:\n","    file_path (str): The name of the CSV file.\n","    directory (str): The path to the directory containing the CSV file.\n","    server_ip (str): The IP address of the server.\n","    client_ip (str): The IP address of the client.\n","    complete_trace_start_time  (float): The start time of the data range to extract.\n","    complete_trace_end_time  (float): The end time of the data range to extract.\n","    specific_portion_start_time  (str): The start time of the filtered by time trace.\n","    specific_portion_end_time  (str): The end time of the filtered by time trace.\n","\n","  Returns:\n","    A tuple of five DataFrames:\n","      1. The original DataFrame loaded from the CSV file.\n","      2. The DataFrame containing the downlink data.\n","      3. The DataFrame containing the uplink data.\n","      4. The DataFrame containing only the downlink data filtered by time range (zoom).\n","      5. The DataFrame containing only the uplink data filtered by time range (zoom).\n","  \"\"\"\n","\n","  # Load the CSV file into a DataFrame\n","  df = load_csv_file(file_path, directory)\n","\n","  # If the DataFrame is not empty, proceed\n","  if df is not None:\n","    # Validate the Wireshark data in the DataFrame\n","    if validate_wireshark_data(df, file_path):\n","      # Filter the DataFrame by SRTP type\n","      df = get_srtp_type(df)\n","      # Split the DataFrame into separate downlink and uplink DataFrames\n","      df_dl, df_ul = get_DL_UL_df(df, server_ip, client_ip)\n","      df_dl = get_time_filtered_dataset(df_dl, complete_trace_start_time , complete_trace_end_time )\n","      df_ul = get_time_filtered_dataset(df_ul, complete_trace_start_time , complete_trace_end_time )\n","\n","      # Filter the downlink and uplink DataFrames by time range\n","      df_dl_z = get_time_filtered_dataset(df_dl, complete_trace_start_time+specific_portion_start_time , complete_trace_start_time+specific_portion_end_time )\n","      df_ul_z = get_time_filtered_dataset(df_ul, complete_trace_start_time+specific_portion_start_time , complete_trace_start_time+specific_portion_end_time )\n","\n","      # Return all five DataFrames\n","      return df, df_dl, df_ul, df_dl_z, df_ul_z\n","\n","    else:\n","      # If the Wireshark data is invalid, print an error message and return None for all DataFrames\n","      print(\"\\033[38;2;255;0;0mError: The selected file \\033[0m\", file_path, \" \\033[38;2;255;0;0mwill not be considered.\\033[0m\")\n","      return None, None, None, None, None\n","\n","  else:\n","    # If the DataFrame is empty, print an error message and return None for all DataFrames\n","    print(\"\\033[38;2;255;0;0mError: The selected file \\033[0m\", file_path, \" \\033[38;2;255;0;0mwill not be considered.\\033[0m\")\n","    return None, None, None, None, None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcO5jMkdiJeC"},"outputs":[],"source":["def get_grouped_dataframes(df_dl_list: List[pd.DataFrame], df_ul_list: List[pd.DataFrame],\n","                           df_dl_z_list: List[pd.DataFrame], df_ul_z_list: List[pd.DataFrame],\n","                           grouping_time: float) -> tuple:\n","  \"\"\"\n","  Groups dataframes by protocol and time_relatvie columns\n","\n","  Parameters:\n","    df_dl_list (List[pd.DataFrame]): List of pandas dataframes containing downlink data\n","    df_ul_list (List[pd.DataFrame]): List of pandas dataframes containing uplink data\n","    df_dl_z_list (List[pd.DataFrame]): List of pandas dataframes containing downlink data filtered by time (zoom).\n","    df_ul_z_list (List[pd.DataFrame]): List of pandas dataframes containing uplink data filtered by time (zoom).\n","    grouping_time (float): Time interval for grouping the dataframes.\n","\n","  Returns:\n","    A tuple of lists of pandas dataframes that are grouped by their timestamp column\n","  \"\"\"\n","  # Group the dataframes\n","  df_grouped_dl_list = group_dataframes(df_dl_list, grouping_time)\n","  df_grouped_ul_list = group_dataframes(df_ul_list, grouping_time)\n","  df_grouped_dl_z_list = group_dataframes(df_dl_z_list, grouping_time)\n","  df_grouped_ul_z_list = group_dataframes(df_ul_z_list, grouping_time)\n","\n","  # Return the grouped dataframes as a tuple\n","  return df_grouped_dl_list, df_grouped_ul_list, df_grouped_dl_z_list, df_grouped_ul_z_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_l_9RWqiYYw"},"outputs":[],"source":["def get_traffic_dataframes(df_dl_list: List[pd.DataFrame] , df_ul_list: List[pd.DataFrame]) -> tuple:\n","  \"\"\"\n","  Get the traffic dataframes for the given downlink and uplink dataframes.\n","\n","  Parameters:\n","    df_dl_list (List[pd.DataFrame]): A list of downlink dataframes.\n","    df_ul_list (List[pd.DataFrame]): A list of uplink dataframes.\n","\n","  Returns:\n","    A tuple of traffic dataframes for the given downlink and uplink dataframes.\n","  \"\"\"\n","  # Get the traffic dataframes for downlink dataframes\n","  df_traffic_dl_list = [get_traffic_df(df_dl, 1) for df_dl in df_dl_list]\n","\n","  # Get the traffic dataframes for uplink dataframes\n","  df_traffic_ul_list = [get_traffic_df(df_ul, 1) for df_ul in df_ul_list]\n","\n","  # Return the traffic dataframes tuple\n","  return df_traffic_dl_list, df_traffic_ul_list\n"]},{"cell_type":"markdown","source":["#### **SRTP Video Frame Grouping**"],"metadata":{"id":"k1Kwo4Lr555-"}},{"cell_type":"code","source":["def extract_time(info: str) -> str:\n","  \"\"\"\n","  Extracts the time value from the given info string.\n","\n","  Parameters:\n","    info (str): Info string containing time value.\n","\n","  Returns:\n","    str: Extracted time value.\n","  \"\"\"\n","  info_values = info.split(\",\")\n","  for value in info_values:\n","    if \"Time\" in value:\n","      return value\n","  return \"\"\n","\n","def group_vid_dataframes_by_time_info(df_list: List[pd.DataFrame]) -> List[pd.DataFrame]:\n","  \"\"\"\n","  Groups DataFrames based on the time relative and protocol column.\n","\n","  Parameters:\n","    df_list: A list of DataFrames to group.\n","\n","  Returns:\n","    A list of grouped DataFrames.\n","  \"\"\"\n","  grouped_dfs = []\n","  # Loop over each DataFrame in the list\n","  for df in df_list:\n","      df_copy = df.copy()  # create a copy of the original DataFrame to avoid modifying it\n","\n","      # Sort DataFrame by protocol and frame time\n","      df_copy = df_copy[df_copy['_ws.col.Protocol']=='SRTP Video']\n","      # Extract time from info column\n","      df_copy['_ws.col.Time'] = df_copy['_ws.col.Info'].apply(extract_time)\n","\n","      # Group the DataFrame based on the extracted time\n","      grouped_df = df_copy.groupby('_ws.col.Time')\n","       # Compute the number of packets for each group\n","      num_pack = grouped_df.count()['ip.src'].rename('num.packets')\n","\n","      # Compute the mean of the frame length and udp length for each group.\n","      mean_frame_lengths = grouped_df['frame.len'].mean().rename('group.frame.len.mean')\n","      mean_udp_lengths = grouped_df['udp.length'].mean().rename('group.udp.length.mean')\n","\n","      # Compute the sum of the frame length and udp length for each group.\n","      tot_frame_lengths = grouped_df['frame.len'].sum().rename('group.frame.len.tot')\n","      tot_udp_lengths = grouped_df['udp.length'].sum().rename('group.udp.length.tot')\n","\n","      # Compute the start and end times for each group.\n","      start_times = grouped_df['frame.time_relative'].min().rename('start.time')\n","      end_times = grouped_df['frame.time_relative'].max().rename('end.time')\n","\n","      # Compute the difference between start and end times for each group.\n","      group_time = (end_times - start_times)*1000\n","\n","      # Compute the average time between packets in the group\n","      avg_time_diff = (group_time / (num_pack - 1))\n","      avg_time_diff = avg_time_diff.rename('avg.time.between.packets')\n","\n","\n","      # Combine the columns and reset the index.\n","      grouped_df = pd.concat([grouped_df.first()[['ip.src', 'ip.dst', '_ws.col.Protocol', 'frame.time_relative']],\n","                              num_pack, mean_frame_lengths, mean_udp_lengths, tot_frame_lengths, tot_udp_lengths, start_times, end_times], axis=1)\n","      grouped_df['group.time'] =  group_time\n","      grouped_df['avg.time.between.packets'] = avg_time_diff\n","\n","      # Append the grouped DataFrame to the list of grouped DataFrames\n","      grouped_dfs.append(grouped_df.reset_index(drop=True))\n","      # Append the grouped DataFrame to the result list\n","  return grouped_dfs"],"metadata":{"id":"FnmyFWqc51Wg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def group_dataframes_temp(df_list: List[pd.DataFrame], grouping_time: float) -> List[pd.DataFrame]:\n","  \"\"\"\n","  Groups  DataFrames based on the time relative and protocol column only for SRTP Video and considering that they are from the same frame.\n","\n","  Parameters:\n","    df_list: A list of DataFrames to group.\n","    grouping_time (float): Time interval for grouping the dataframes.\n","\n","  Returns:\n","    A list of grouped DataFrames.\n","  \"\"\"\n","  grouped_dfs = []\n","\n","  # Loop over each DataFrame in the list\n","  for df in df_list:\n","\n","    df_copy = df.copy() # create a copy of the original DataFrame to avoid modifying it\n","    # Sort DataFrame by protocol and frame time\n","    df_copy = df_copy[df_copy['_ws.col.Protocol']=='SRTP Video']\n","    # Extract time from info column\n","    df_copy['_ws.col.Time'] = df_copy['_ws.col.Info'].apply(extract_time)\n","\n","    df_copy = df_copy.sort_values(by=['_ws.col.Protocol', '_ws.col.Time', 'frame.time_relative'])\n","\n","    # Determine the groups based on the protocol and frame time\n","    groups = (df_copy['_ws.col.Protocol'] != df_copy['_ws.col.Protocol'].shift()) | \\\n","              (df_copy['frame.time_relative'].diff() > grouping_time) | \\\n","              (df_copy['_ws.col.Time'] != df_copy['_ws.col.Time'].shift())\n","\n","    # Group the DataFrame by the calculated groups\n","    grouped_df = df_copy.groupby(groups.cumsum())\n","    # Compute the number of packets for each group\n","    num_pack = grouped_df.count()['ip.src'].rename('num.packets')\n","\n","    # Compute the mean of the frame length and udp length for each group.\n","    mean_frame_lengths = grouped_df['frame.len'].mean().rename('group.frame.len.mean')\n","    mean_udp_lengths = grouped_df['udp.length'].mean().rename('group.udp.length.mean')\n","\n","    # Compute the sum of the frame length and udp length for each group.\n","    tot_frame_lengths = grouped_df['frame.len'].sum().rename('group.frame.len.tot')\n","    tot_udp_lengths = grouped_df['udp.length'].sum().rename('group.udp.length.tot')\n","\n","    # Compute the start and end times for each group.\n","    start_times = grouped_df['frame.time_relative'].min().rename('start.time')\n","    end_times = grouped_df['frame.time_relative'].max().rename('end.time')\n","\n","    # Compute the difference between start and end times for each group.\n","    group_time = (end_times - start_times)*1000\n","\n","    # Compute the average time between packets in the group\n","    avg_time_diff = (group_time / (num_pack - 1))\n","    avg_time_diff = avg_time_diff.rename('avg.time.between.packets')\n","\n","\n","    # Combine the columns and reset the index.\n","    grouped_df = pd.concat([grouped_df.first()[['ip.src', 'ip.dst', '_ws.col.Protocol', 'frame.time_relative', '_ws.col.Info']],\n","                            num_pack, mean_frame_lengths, mean_udp_lengths, tot_frame_lengths, tot_udp_lengths, start_times, end_times ], axis=1)\n","    grouped_df['group.time'] =  group_time\n","    grouped_df['avg.time.between.packets'] = avg_time_diff\n","\n","    # Append the grouped DataFrame to the list of grouped DataFrames\n","    grouped_dfs.append(grouped_df.reset_index(drop=True))\n","\n","  return grouped_dfs\n","\n","\n","def group_grouped_vid_dataframes_by_time_info(df_list: List[pd.DataFrame]) -> List[pd.DataFrame]:\n","  \"\"\"\n","  Groups grouped DataFrames based on the time relative and protocol column.\n","\n","  Parameters:\n","    df_list: A list of DataFrames to group.\n","\n","  Returns:\n","    A list of grouped DataFrames.\n","  \"\"\"\n","  grouped_dfs = []\n","  grouped_dfs_not_na = []\n","  # Loop over each DataFrame in the list\n","  for df in df_list:\n","      df_copy = df.copy()  # create a copy of the original DataFrame to avoid modifying it\n","\n","      # Sort DataFrame by protocol and frame time\n","      df_copy = df_copy[df_copy['_ws.col.Protocol']=='SRTP Video']\n","      # Extract time from info column\n","      df_copy['_ws.col.Time'] = df_copy['_ws.col.Info'].apply(extract_time)\n","\n","      # Group the DataFrame based on the extracted time\n","      grouped_df = df_copy.groupby('_ws.col.Time')\n","\n","      df_without_last_row = grouped_df.apply(lambda x: x.drop(x.index[-1]))\n","      #print(df_without_last_row['num.packets'].mean())\n","      #print(df_without_last_row['num.packets'].std())\n","\n","      # Compute the number of packets for each group\n","      num_groups = grouped_df.count()['ip.src'].rename('num.groups')\n","\n","      start_times = grouped_df['frame.time_relative'].min().rename('start.time')\n","      end_times = grouped_df['frame.time_relative'].max().rename('end.time')\n","\n","      # Compute the difference between start and end times for each group.\n","      group_time = (end_times - start_times)*1000\n","\n","      # Compute the average time between packets in the group\n","      avg_time_diff = (group_time / (num_groups - 1))\n","      avg_time_diff = avg_time_diff.rename('avg.time.between.groups')\n","\n","\n","      # Combine the columns and reset the index.\n","      grouped_df = pd.concat([grouped_df.first()[['ip.src', 'ip.dst', '_ws.col.Protocol', 'frame.time_relative']],\n","                              num_groups, start_times, end_times], axis=1)\n","      grouped_df['group.time'] =  group_time\n","      grouped_df['avg.time.between.groups'] = avg_time_diff\n","      grouped_df_not_na = grouped_df[grouped_df['avg.time.between.groups'].notna()]\n","      # Append the grouped DataFrame to the list of grouped DataFrames\n","      grouped_dfs.append(grouped_df.reset_index(drop=True))\n","      grouped_dfs_not_na.append(grouped_df_not_na.reset_index(drop=True))\n","      # Append the grouped DataFrame to the result list\n","  return grouped_dfs, grouped_dfs_not_na\n"],"metadata":{"id":"iLblaVsP6BmC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBBAzXB6flZJ"},"source":["### **Wireshark Figures build**"]},{"cell_type":"markdown","metadata":{"id":"CgZz0-TC_R9b"},"source":["#### **Scatter plot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_Rpf_r1p37B"},"outputs":[],"source":["def create_scatter_plot(df_dl: pd.DataFrame, df_ul: pd.DataFrame, x_col: str, y_col: str, title: str,\n","                        x_title: str, y_title: str, protocols: bool = False, text_show: bool = False, number_column: str = None,\n","                        from0: bool = False, stem: bool = False, spec_prot: List[str] = None) -> None:\n","  \"\"\"\n","  Function to create a scatter plot with optional grouping by protocol and optional display of text labels.\n","\n","  Parameters:\n","    df_dl (pd.DataFrame): The downlink data as a Pandas DataFrame.\n","    df_ul (pd.DataFrame): The uplink data as a Pandas DataFrame.\n","    x_col (str): The column name to use for the x-axis.\n","    y_col (str): The column name to use for the y-axis.\n","    title (str): The title of the plot.\n","    x_title (str): The title of the x-axis.\n","    y_title (str): The title of the y-axis.\n","    protocols (bool, optional): Whether to group data by protocol. Default is False.\n","    text_show (bool, optional): Whether to display text labels on the plot. Default is False.\n","    number_column (str, optional): The column name to use for the text labels. Default is None.\n","    from0 (bool, optional): Whether to make the plot from 0.\n","    stem (bool, optional): Whether to include vertical lines to simulate a stem plot.\n","    spec_prot (List[str], optional): Whether to plot specific protocols\n","  Returns:\n","    None\n","  \"\"\"\n","  if protocols:\n","\n","    dl_protocols = ['DTLSv1.2', 'DTLS', 'SRTP Audio', 'SRTP Video','STUN' ,'UDP']  # Define manually all the DL protocols so that they always have the same assigned color\n","    # dl_protocols = sorted(df_dl['_ws.col.Protocol'].unique()) # Get unique protocols in DL and assign a color to each - might not produce same colors in different executions\n","    dl_colors = px.colors.qualitative.Plotly[:len(dl_protocols)] # Plotly has 10 colors\n","    protocol_color_map_dl = dict(zip(dl_protocols, dl_colors))\n","\n","    ul_protocols = ['DTLSv1.2', 'SRTCP', 'STUN', 'UDP'] # Define manually all the UL protocols so that they always have the same assigned color\n","    # ul_protocols = sorted(df_ul['_ws.col.Protocol'].unique())    # Get unique protocols in UL and assign a color to each - might not produce same colors in different executions\n","    ul_colors = px.colors.qualitative.Plotly[len(dl_protocols):len(dl_protocols) + len(ul_protocols)] # Plotly has 10 colors\n","    protocol_color_map_ul = dict(zip(ul_protocols, ul_colors))\n","\n","  else:\n","    dataframes = ['DL', 'UL']\n","    dataframes_col = px.colors.qualitative.Plotly[:len(dataframes)]\n","    color_map_dl = dict(zip(dataframes, dataframes_col))\n","\n","  # Set the mode of the plot\n","  if text_show:\n","    mode_plt = 'markers+text'\n","  else:\n","    mode_plt = 'markers' #default: markers. lines for some\n","\n","  fig = go.Figure()\n","\n","  if protocols:\n","    # Add traces for each protocol in dl and ul\n","    combined_df = pd.concat([df_dl, df_ul])  # Combine df_dl and df_ul\n","    combined_min = combined_df[x_col].min()  # Minimum value from combined data frames\n","    for i, df in enumerate([df_dl, df_ul]):\n","      #list_prot = [[ 'SRTP Video','SRTP Audio','UDP','DTLSv1.2'],[ 'SRTCP', 'UDP', 'DTLSv1.2']]\n","      for j, prot in enumerate(sorted(df['_ws.col.Protocol'].unique())):\n","        if spec_prot!=None and prot not in spec_prot:\n","          continue\n","        color = protocol_color_map_dl[prot] if i == 0 else protocol_color_map_ul[prot]\n","        filtered_df = df[df['_ws.col.Protocol'] == prot]\n","\n","        if from0: #plot x_axis starting from 0\n","          # Comparison because with min was giving problems\n","            x_val = filtered_df[x_col]- combined_min\n","        else:\n","          x_val = filtered_df[x_col]\n","\n","        if \"ms\" in x_title:\n","          x_val*=1000 # convert to ms\n","\n","        fig.add_trace(\n","          go.Scatter(\n","            mode = mode_plt,\n","            x = x_val,\n","            y = filtered_df[y_col],\n","            name = f\"{'DL' if i == 0 else 'UL'} {prot}\",\n","            marker = dict(color = color),\n","            text = filtered_df[number_column].tolist() if number_column else None,\n","            textfont = dict(size=10),\n","            textposition = 'top center',\n","                    )\n","        )\n","\n","        if stem:\n","          for x_v,y_v in zip(x_val,filtered_df[y_col]):\n","            fig.add_shape(\n","              type='line',\n","              x0=x_v,\n","              y0=0,\n","              x1=x_v,\n","              y1=y_v,\n","              line=dict(color = color),\n","              opacity=1\n","            )\n","\n","\n","\n","  else:\n","    for i, df in enumerate([df_dl, df_ul]):\n","      name_df = 'DL' if i == 0 else 'UL'\n","      color = color_map_dl[name_df]\n","      filtered_df = df\n","      if from0: #plot x_axis starting from 0\n","        if df_dl[x_col].min() >=df_ul[x_col].min():\n","          x_val = (filtered_df[x_col]- df_dl[x_col].min())\n","        else:\n","          x_val = (filtered_df[x_col]- df_ul[x_col].min())\n","      else:\n","        x_val = filtered_df[x_col]\n","\n","      if \"ms\" in x_title:\n","        x_val*=1000 # convert to ms\n","\n","      fig.add_trace(\n","        go.Scatter(\n","          mode = mode_plt,\n","          x = x_val,\n","          y = filtered_df[y_col],\n","          name = name_df,\n","          marker = dict(color = color),\n","          text = filtered_df[number_column].tolist() if number_column else None,\n","          textfont = dict(size=10),\n","          textposition = 'top center'\n","                  )\n","        )\n","\n","  fig.update_layout(\n","    title=title,\n","    font_family=\"Nimbus Roman\",\n","    xaxis=dict(title=x_title),\n","    yaxis=dict(title=y_title),\n","    showlegend=True,\n","    legend=dict(x=1, y=1),\n","  )\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False), #,dtick=2\n","      yaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200, #change width\n","      height=450) #change height\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"cd029UU7LeEM"},"source":["##### **Matlab stem plot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXddgY6mB24e"},"outputs":[],"source":["def create_stem_plot(df_dl: pd.DataFrame, df_ul: pd.DataFrame, x_col: str, y_col: str, title: str,\n","                      x_title: str, y_title: str, protocols: bool = False, text_show: bool = False,\n","                     number_column: str = None, from0: bool = False, spec_prot: List[str] = None) -> None:\n","  \"\"\"\n","  Function to create a scatter plot with optional grouping by protocol and optional display of text labels.\n","\n","  Parameters:\n","    df_dl (pd.DataFrame): The downlink data as a Pandas DataFrame.\n","    df_ul (pd.DataFrame): The uplink data as a Pandas DataFrame.\n","    x_col (str): The column name to use for the x-axis.\n","    y_col (str): The column name to use for the y-axis.\n","    title (str): The title of the plot.\n","    x_title (str): The title of the x-axis.\n","    y_title (str): The title of the y-axis.\n","    protocols (bool, optional): Whether to group data by protocol. Default is False.\n","    text_show (bool, optional): Whether to display text labels on the plot. Default is False.\n","    number_column (str, optional): The column name to use for the text labels. Default is None.\n","    from0 (bool, optional): Whether to make the plot from 0.\n","    spec_prot (List[str], optional): Whether to plot specific protocols\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  if protocols:\n","\n","    dl_protocols = ['DTLSv1.2','DTLS', 'SRTP Audio', 'SRTP Video','STUN' ,'UDP']  # Define manually all the DL protocols so that they always have the same assigned color\n","    # dl_protocols = sorted(df_dl['_ws.col.Protocol'].unique()) # Get unique protocols in DL and assign a color to each - might not produce same colors in different executions\n","    dl_colors = px.colors.qualitative.Plotly[:len(dl_protocols)] # Plotly has 10 colors\n","    protocol_color_map_dl = dict(zip(dl_protocols, dl_colors))\n","\n","    ul_protocols = ['DTLSv1.2', 'SRTCP', 'STUN', 'UDP'] # Define manually all the UL protocols so that they always have the same assigned color\n","    # ul_protocols = sorted(df_ul['_ws.col.Protocol'].unique())    # Get unique protocols in UL and assign a color to each - might not produce same colors in different executions\n","    ul_colors = px.colors.qualitative.Plotly[len(dl_protocols):len(dl_protocols) + len(ul_protocols)] # Plotly has 10 colors\n","    protocol_color_map_ul = dict(zip(ul_protocols, ul_colors))\n","\n","  else:\n","    dataframes = ['DL', 'UL']\n","    dataframes_col = px.colors.qualitative.Plotly[:len(dataframes)]\n","    color_map_dl = dict(zip(dataframes, dataframes_col))\n","\n","  plt.figure(figsize=(20, 5))\n","\n","  if protocols:\n","    for i, df in enumerate([df_dl, df_ul]):\n","      for j, prot in enumerate(sorted(df['_ws.col.Protocol'].unique())):\n","        if spec_prot!=None and prot not in spec_prot:\n","          continue\n","        color = protocol_color_map_dl[prot] if i == 0 else protocol_color_map_ul[prot]\n","\n","        filtered_df = df[df['_ws.col.Protocol'] == prot]\n","\n","        if from0: #plot x_axis starting from 0\n","          if df_dl[x_col].min() >=df_ul[x_col].min():\n","            x_val = (filtered_df[x_col]- df_dl[x_col].min())\n","          else:\n","            x_val = (filtered_df[x_col]- df_ul[x_col].min())\n","        else:\n","          x_val = filtered_df[x_col]\n","\n","        if \"ms\" in x_title:\n","          x_val*=1000 # convert to ms\n","\n","        plt.stem(x_val, filtered_df[y_col], linefmt=color, markerfmt=color, basefmt='k',\n","                label=f\"{'DL' if i == 0 else 'UL'} {prot}\")\n","  else:\n","    for i, df in enumerate([df_dl, df_ul]):\n","      name_df = 'DL' if i == 0 else 'UL'\n","      color = color_map_dl[name_df]\n","      filtered_df = df\n","      plt.stem(filtered_df[x_col], filtered_df[y_col], linefmt=color, markerfmt=color, basefmt='k',\n","                label=name_df)\n","\n","  plt.title(title)\n","  plt.xlabel(x_title)\n","  plt.ylabel(y_title)\n","  plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.45), ncol=2)  # Move the legend to the top and outside of the plot\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C9Ictxu5h9Ou"},"source":["#### **Traffic load**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBZebIAqh-f9"},"outputs":[],"source":["def plot_traffic_load_over_time(traffic_df_list: List[pd.DataFrame], title: str, x_title: str,  y_title: str,\n","                                label_list: list, color_list: list) -> None:\n","  \"\"\"\n","  Plots traffic load over time for multiple dataframes in a single plot.\n","\n","  Parameters:\n","    traffic_df_list (list): The list of dataframes containing the traffic data to be plotted.\n","    title (str): The title of the plot.\n","    x_title (str): The title of the x-axis.\n","    y_title (str): The title of the y-axis.\n","    label_list (list): The list of labels for the traces in the plot.\n","    color_list (list): The list of colors for the traces in the plot.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  fig = go.Figure()\n","\n","  # Add traces for each dataframe in traffic_df_list\n","  for i, df in enumerate(traffic_df_list):\n","    traffic_df = traffic_df_list[i]\n","\n","    # Add trace to the plot\n","    fig.add_trace(go.Scatter(x=traffic_df['seconds'],\n","                              y=traffic_df['traffic'],\n","                              name=label_list[i] if label_list else None,\n","                              marker_color = color_list[i] if color_list else None,\n","                              yaxis='y'))\n","\n","  # Update layout with title and axis labels\n","  fig.update_layout(title=title,\n","                    xaxis_title=x_title,\n","                    yaxis_title=y_title)\n","\n","  # Display plot\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"_iFKflA5Of-3"},"source":["#### **Correlation**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9l1ioRgPOggl"},"outputs":[],"source":["def plot_correlation(df_traffic_dl_list: List[pd.DataFrame], df_traffic_ul_list: List[pd.DataFrame], interval: float) -> None:\n","  \"\"\"\n","  Plots the correlation between uplink and downlink traffic for a list of DataFrames\n","\n","  Parameters:\n","    df_traffic_dl_list (List[pd.DataFrame]): List of DataFrames containing downlink traffic data\n","    df_traffic_ul_list (List[pd.DataFrame]): List of DataFrames containing uplink traffic data\n","    interval (float): The sampling interval in seconds\n","\n","  Returns:\n","    None\n","  \"\"\"\n","\n","  for df_downlink, df_uplink in zip(df_traffic_dl_list, df_traffic_ul_list):\n","    print()\n","    if df_downlink.shape[0] != df_uplink.shape[0]:\n","      # Find the maximum value of seconds\n","      max_seconds_df_dl = df_downlink['seconds'].max()\n","      max_seconds_df_ul = df_uplink['seconds'].max()\n","\n","      if max_seconds_df_ul < max_seconds_df_dl:\n","        # Truncate dataset to match the size based on the seconds column\n","        df_downlink = df_downlink.loc[df_downlink['seconds'] <= max_seconds_df_ul].reset_index(drop=True)\n","      else:\n","        # Truncate dataset to match the size based on the seconds column\n","        df_uplink = df_uplink.loc[df_uplink['seconds'] <= max_seconds_df_dl].reset_index(drop=True)\n","\n","    # Compute the cross-correlation between the two traffic columns\n","    cross_corr = np.correlate(df_uplink['traffic'], df_downlink['traffic'], mode='full')\n","\n","    # Compute the lag values\n","    lags = np.arange(-len(df_uplink)+1, len(df_downlink))\n","\n","    # Find the index of the maximum cross-correlation value\n","    max_index = np.argmax(cross_corr)\n","\n","    # Find the maximum lag that maximizes the cross-correlation\n","    max_lag = lags[max_index]\n","\n","    # Shift the uplink traffic column by the lag value\n","    df_uplink_shifted = df_uplink.shift(-max_lag)\n","\n","    # Compute the correlation coefficient between the shifted uplink traffic and the downlink traffic\n","    correlation_coefficient_shifted = df_uplink_shifted['traffic'].corr(df_downlink['traffic'])\n","\n","    # Create a scatter plot with a regression line for the shifted data\n","    fig = px.scatter(df_downlink, x='traffic', y=df_uplink_shifted['traffic'], trendline='ols',\n","                    labels={'traffic': 'Downlink Traffic', 'y': 'Uplink Traffic (Shifted)'},\n","                    title=f'Uplink and Downlink Traffic Correlation (Shifted by {max_lag} samples, resampled by {interval*1000} ms)')\n","    fig.update_traces(line=dict(color='#e5523e'))\n","    fig.add_annotation(x=0.5, y=0.95, text=f'Correlation coefficient: {correlation_coefficient_shifted:.4f}',\n","                      showarrow=False, xref='paper', yref='paper', font=dict(size=12))\n","\n","    # Display the plot\n","    fig.show()"]},{"cell_type":"markdown","metadata":{"id":"rDbFbaT9_YkC"},"source":["#### **ECDF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqOD-6of_LNz"},"outputs":[],"source":["def create_ecdf_plot( df_list: List[pd.DataFrame], column: str, title: str, x_title: str, y_title: str, label_list: list,\n","                     color_list: list, factor: float = 1, diff: bool = False) -> None:\n","  \"\"\"\n","  Create a plot of the empirical cumulative distribution function (ECDF) for the given column in each DataFrame\n","  in df_list.\n","\n","  Parameters:\n","    df_list (pd.DataFrame): The list of dataframes containing the traffic data to be plotted.\n","    column (str): The column name for which to create the ECDF plot.\n","    title (str): The title of the plot.\n","    x_title (str): The title of the x-axis.\n","    y_title (str): The title of the y-axis.\n","    label_list (list): The list of labels for the traces in the plot.\n","    color_list (list): The list of colors for the traces in the plot.\n","    factor (float, optional): A float representing the factor by which to scale the data in the column.\n","    diff (bool, optional): A boolean representing whether to compute the differences of the sorted column data.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  fig = go.Figure()\n","\n","  for i, df in enumerate(df_list):\n","    if diff:\n","      df_sorted = np.sort(df[column].diff())\n","    else:\n","      df_sorted = np.sort(df[column])\n","    df_sorted *= factor\n","    y = np.arange(1, len(df[column]) + 1) / len(df[column])\n","    fig.add_trace(\n","      go.Scatter(\n","        x=df_sorted,\n","        y=y,\n","        name=label_list[i] if label_list else None,\n","        marker_color=color_list[i] if color_list else None,\n","        yaxis='y'\n","      )\n","    )\n","\n","  fig.update_layout(\n","    title=title,\n","    xaxis_title=x_title,\n","    yaxis_title=y_title\n","  )\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False), #,dtick=10, ,range=[75, 95]\n","      yaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=600,\n","      height=400)\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"kUcHje42kppc"},"source":["#### **Histogram**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ng8Qpt5akrXs"},"outputs":[],"source":["def create_histogram_plot(df_list: List[pd.DataFrame], column: str, title: str, x_title: str, y_title: str,\n","                          label_list: List[str], color_list: List[str], factor: float = 1, diff: bool = False) -> None:\n","  \"\"\"\n","  Create a plot of histograms for the given column in each DataFrame in df_list.\n","\n","  Parameters:\n","    df_list (List[pd.DataFrame]): The list of dataframes containing the traffic data to be plotted.\n","    column (str): The column name for which to create the histogram plot.\n","    title (str): The title of the plot.\n","    x_title (str): The title of the x-axis.\n","    y_title (str): The title of the y-axis.\n","    label_list (List[str]): The list of labels for the histograms in the plot.\n","    color_list (List[str]): The list of colors for the histograms in the plot.\n","    factor (float, optional): A float representing the factor by which to scale the data in the column.\n","    diff (bool, optional): A boolean representing whether to compute the differences of the sorted column data.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  fig = go.Figure()\n","  for i, df in enumerate(df_list):\n","      if diff:\n","          df_sorted = np.sort(df[column].diff())\n","      else:\n","          df_sorted = np.sort(df[column])\n","      df_sorted *= factor\n","      fig.add_trace(\n","          go.Histogram(\n","              x=df_sorted,\n","              name=label_list[i] if label_list else None,\n","              histnorm='probability',\n","              histfunc='count',\n","              #autobinx=False,\n","              #xbins=dict(start=min(df_sorted), end=max(df_sorted), size=1)\n","\n","          )\n","      )\n","\n","  fig.update_layout(\n","      title=title,\n","      xaxis_title=x_title,\n","      yaxis_title=y_title\n","  )\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"cYRUVA0oTqwN"},"source":["#### **Box plot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPWdxq97TtNs"},"outputs":[],"source":["def box_plot(df_list: List[pd.DataFrame], title: str, parameter_list: List[str], column: str,\n","            unit: str, protocol: str = None, grouped: bool = False, factor: float = 1, diff: bool = False) -> None:\n","  \"\"\"\n","  Creates a box plot for the given dataframe(s) and column(s).\n","\n","  Parameters:\n","    df_list (pd.DataFrame): The list of dataframes containing the traffic data to be plotted.\n","    title (str): The title of the plot.\n","    parameter_list (ist[str]): A list of strings describing the dataframes in df_list.\n","    column (str): The column name to plot.\n","    unit (str): The unit of measurement for the column.\n","    protocol (str, optional): The protocol to filter by.\n","    grouped (bool, optional): Whether the dataframes are grouped by protocol.\n","    factor (float, optional): A float representing the factor by which to scale the data in the column.\n","    diff (bool, optional): A boolean representing whether to compute the differences of the sorted column data.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  data = []\n","  labels = []\n","\n","  if protocol:\n","    for i, df in enumerate(df_list):\n","      df_filtered = df[df['_ws.col.Protocol'] == protocol]\n","      if diff:\n","        df_sorted = np.sort(df_filtered[column].diff())\n","      else:\n","        df_sorted = np.sort(df_filtered[column])\n","      df_sorted *= factor\n","      data.append(df_sorted)\n","      labels.append(f\"{parameter_list[i]}{protocol}\")\n","  else:\n","    for i, df in enumerate(df_list):\n","      for protocol in df['_ws.col.Protocol'].unique():\n","        df_filtered = df[df['_ws.col.Protocol'] == protocol]\n","        if diff:\n","          df_sorted = np.sort(df_filtered[column].diff())\n","        else:\n","          df_sorted = np.sort(df_filtered[column])\n","        df_sorted *= factor\n","        data.append(df_sorted)\n","        labels.append(f\"{parameter_list[i]}{protocol}\")\n","\n","  # Create box plot using plotly\n","  fig = go.Figure()\n","  for i in range(len(data)):\n","    fig.add_trace(go.Box(y=data[i], name=labels[i]))\n","  if grouped:\n","    title = 'Grouped ' + title\n","\n","  if column == 'frame.time_relative':\n","      column = 'time.btw.groups'\n","  fig.update_layout(\n","    title=title + \" Box Plot\",\n","    xaxis_title=\"Protocol\",\n","    yaxis_title=column + unit\n","  )\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200,\n","      height=600)\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUD9qfQx9WC8"},"outputs":[],"source":["def comparative_box_plot(df_list: List[List[pd.DataFrame]], df_labels: List[str], title: str, category_list: List[str], column: str, y_title: str,\n","            unit: str, protocol: List[str] = None, grouped: bool = False, factor: float = 1, diff: bool = False) -> None:\n","  \"\"\"\n","  Creates a box plot for the given dataframe(s) and column(s).\n","\n","  Parameters:\n","    df_list (List[List[pd.DataFrame]]): The list of dataframes containing the traffic data to be plotted.\n","    df_labels (List[str]]): The list of labels for each dataframe list in dataframe\n","    title (str): The title of the plot.\n","    category_list (List[str]): A list of strings describing the dataframes in df_list.\n","    column (str): The column name to plot.\n","    y_title (str): Title for the y axis column\n","    unit (str): The unit of measurement for the column.\n","    protocol (List[str], optional): List of protocol to filter by.\n","    grouped (bool, optional): Whether the dataframes are grouped by protocol.\n","    factor (float, optional): A float representing the factor by which to scale the data in the column.\n","    diff (bool, optional): A boolean representing whether to compute the differences of the sorted column data.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  merged_list = []\n","  if len(df_list)==1:\n","    merged_list = df_list[0]\n","  elif len(df_list) == 2:\n","    df_list1 = []\n","    df_list2 = []\n","    for df in df_list[0]:\n","      df_copy = df.copy()  # Create a copy of the dataframe\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('STUN', 'STUN ' + df_labels[0])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('DTLSv1.2','DTLSv1.2 ' + df_labels[0])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('UDP', 'UDP ' + df_labels[0])\n","      df_list1.append(df_copy)\n","    for df in df_list[1]:\n","      df_copy = df.copy()  # Create a copy of the dataframe\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('STUN', 'STUN ' + df_labels[1])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('DTLSv1.2', 'DTLSv1.2 ' + df_labels[1])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('UDP', 'UDP ' + df_labels[1])\n","      df_list2.append(df_copy)\n","    for i in range(len(df_list1)):\n","      merged_df = pd.concat([df_list1[i], df_list2[i]], axis=0)\n","      merged_list.append(merged_df)\n","  else:\n","    return None\n","\n","  protocols = sorted(merged_list[0]['_ws.col.Protocol'].unique())\n","\n","  fig = go.Figure()\n","\n","  data_list = []\n","  labels_list = []\n","\n","  if protocol:\n","    protocols = protocol\n","\n","  for i, df in enumerate(merged_list):\n","    data = []\n","    labels = []\n","    for p in protocols:\n","      df_filtered = df[df['_ws.col.Protocol'] == p]\n","      if diff:\n","        df_sorted = np.sort(df_filtered[column].diff())\n","      else:\n","        df_sorted = np.sort(df_filtered[column])\n","      df_sorted *= factor\n","      data.append(df_sorted)\n","      labels.extend([p]*len(df_sorted))\n","    data_list.append(data)\n","    labels_list.append(labels)\n","\n","  print(labels_list)\n","  # Create grouped box plot using plotly\n","\n","  for i, data in enumerate(data_list):\n","    #must be 1D\n","    flat_data = np.concatenate(data)  # Flatten the nested arrays\n","    fig.add_trace(go.Box(y=flat_data, x=labels_list[i], name=category_list[i], boxmean=True\n","                         ))\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      title=title,\n","      yaxis_title=y_title + unit,\n","      font=dict(family=font_family, size=font_size),\n","      boxmode='group',\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(boxgap=0.2)\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200,\n","      height=600)\n","\n","  fig.update_layout(yaxis_type=\"log\")\n","\n","  fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMVSO47VVIOy"},"outputs":[],"source":["def comparative_box_plot_category(df_list: List[List[pd.DataFrame]], df_labels: List[str], title: str, category_list: List[str], column: str, y_title: str,\n","            unit: str, protocol: List[str] = None, grouped: bool = False, factor: float = 1, diff: bool = False) -> None:\n","  \"\"\"\n","  Creates a box plot for the given dataframe(s) and column(s). X labels the categories\n","\n","  Parameters:\n","    df_list (List[List[pd.DataFrame]]): The list of dataframes containing the traffic data to be plotted.\n","    df_labels (List[str]]): The list of labels for each dataframe list in dataframe\n","    title (str): The title of the plot.\n","    category_list (List[str]): A list of strings describing the dataframes in df_list.\n","    column (str): The column name to plot.\n","    y_title (str): Title for the y axis column\n","    unit (str): The unit of measurement for the column.\n","    protocol (List[str], optional): List of protocol to filter by.\n","    grouped (bool, optional): Whether the dataframes are grouped by protocol.\n","    factor (float, optional): A float representing the factor by which to scale the data in the column.\n","    diff (bool, optional): A boolean representing whether to compute the differences of the sorted column data.\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  merged_list = []\n","  if len(df_list)==1:\n","    merged_list = df_list[0]\n","  elif len(df_list) == 2:\n","    df_list1 = []\n","    df_list2 = []\n","    for df in df_list[0]:\n","      df_copy = df.copy()  # Create a copy of the dataframe\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('STUN', 'STUN ' + df_labels[0])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('DTLSv1.2','DTLSv1.2 ' + df_labels[0])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('UDP', 'UDP ' + df_labels[0])\n","      df_list1.append(df_copy)\n","    for df in df_list[1]:\n","      df_copy = df.copy()  # Create a copy of the dataframe\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('STUN', 'STUN ' + df_labels[1])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('DTLSv1.2', 'DTLSv1.2 ' + df_labels[1])\n","      df_copy['_ws.col.Protocol'] = df_copy['_ws.col.Protocol'].replace('UDP', 'UDP ' + df_labels[1])\n","      df_list2.append(df_copy)\n","    for i in range(len(df_list1)):\n","      merged_df = pd.concat([df_list1[i], df_list2[i]], axis=0)\n","      merged_list.append(merged_df)\n","  else:\n","    return None\n","\n","  protocols = sorted(merged_list[0]['_ws.col.Protocol'].unique())\n","  fig = go.Figure()\n","\n","  data_list = []\n","  labels_list = []\n","\n","  if protocol:\n","    protocols = protocol\n","\n","  for p in protocols:\n","    data = []\n","    labels = []\n","    for i, df in enumerate(merged_list):\n","      df_filtered = df[df['_ws.col.Protocol'] == p]\n","      if diff:\n","        df_sorted = np.sort(df_filtered[column].diff())\n","      else:\n","        df_sorted = np.sort(df_filtered[column])\n","      df_sorted *= factor\n","      data.append(df_sorted)\n","      labels.extend([category_list[i]]*len(df_sorted))\n","    data_list.append(data)\n","    labels_list.append(labels)\n","\n","  # Create grouped box plot using plotly\n","\n","  for i, data in enumerate(data_list):\n","    #must be 1D\n","    flat_data = np.concatenate(data)  # Flatten the nested arrays\n","    fig.add_trace(go.Box(y=flat_data, x=labels_list[i], name=protocols[i], boxmean=True\n","                         ))\n","\n","  fig.update_layout(\n","      title=title,\n","      yaxis_title=y_title + unit,\n","      boxmode='group'\n","  )\n","  fig.update_layout(boxgap=0.2)\n","\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False,dtick=2),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200,\n","      height=600)\n","\n","  fig.update_layout(yaxis_type=\"log\")\n","\n","  fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RagSPdgN-LKD"},"outputs":[],"source":["def traffic_load_box_plot(traffic_df_list: List[pd.DataFrame], title: str, parameter: str,\n","                                label_list: list) -> None:\n","  \"\"\"\n","  Creates a traffic box plot for the given dataframe(s).\n","\n","  Parameters:\n","    traffic_df_list (pd.DataFrame): The list of dataframes containing the traffic data to be plotted.\n","    title (str): The title of the plot.\n","    parameter (str): The name of the transmission link (Uplink or Downlink).\n","\n","\n","  Returns:\n","    None\n","  \"\"\"\n","  data = []\n","\n","  for df in traffic_df_list:\n","    df_sorted = np.sort(df['traffic'])\n","    data.append(df_sorted)\n","\n","\n","  # Create box plot using plotly\n","  fig = go.Figure()\n","  for i in range(len(data)):\n","    fig.add_trace(go.Box(y=data[i],\n","                         name=label_list[i],\n","                         boxmean=True,  # Show mean line\n","                         #boxpoints='all'\n","                         ))\n","\n","  fig.update_layout(\n","    title=title,\n","    xaxis_title='',\n","    yaxis_title= parameter + 'Throughput (Mbps)'\n","  )\n","\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200,\n","      height=600)\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"UN5png9vgyDe"},"source":["#### **Bar plot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpMlqHSNgzXE"},"outputs":[],"source":["def create_grouped_bar_plot(data: List[List[float]], title: str, x_labels: List[str], bar_labels: List[str] , y_label: str, type_y:str = None):\n","  \"\"\"\n","  Creates a grouped bar plot for the given data.\n","\n","  Parameters:\n","  data (List[List[float]]): A list of lists containing the values for each bar group.\n","  title (str): The title of the plot.\n","  x_labels (List[str]): A list of labels for each category.\n","  bar_labels (List[str]): A list of labels for each group of bars.\n","  y_label (str): The label for the y-axis.\n","  type_y (str): Defines the type of the y-axis\n","\n","  Returns:\n","  None\n","  \"\"\"\n","  colors = px.colors.qualitative.Plotly[:len(data)]\n","\n","  fig = go.Figure()\n","\n","  for i, values in enumerate(data):\n","    x = bar_labels\n","    y = values  # Values for the bar\n","\n","    fig.add_trace(go.Bar(\n","      x=x,\n","      y=y,\n","      name=x_labels[i],\n","      marker_color=colors[i]\n","    ))\n","\n","  fig.update_layout(\n","    title=title,\n","    yaxis=dict(title=y_label, type=type_y),\n","    barmode='group'\n","  )\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"vsrVXtuQjof9"},"source":["### **Wireshark Computations build**"]},{"cell_type":"markdown","metadata":{"id":"x4SkBYw68Eew"},"source":["#### **Streams characteristics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLA9X9oIwvqu"},"outputs":[],"source":["def streams_characteristics(df_list: List[pd.DataFrame], title: str, parameter_list: List[str], grouped: bool = False):\n","  \"\"\"\n","  Computes and and displays various characteristics of network streams from a list of pandas DataFrames.\n","\n","  Parameters:\n","    df_list (List[pd.DataFrame]): A list of pandas DataFrames, where each DataFrame represents a network stream.\n","    title (str): The title for the resulting DataFrame.\n","    parameter_list (List[str]): A list of strings representing additional parameters for each stream, such as source/destination IP addresses, port numbers, etc.\n","    grouped (bool): A boolean indicating whether the streams are grouped or not. Default is False.\n","\n","  Returns:\n","    None.\n","  \"\"\"\n","  percentage_th = 0.90\n","  num_intervals = 3\n","  num_most_common = 3\n","  round_val = 4\n","\n","  rows = []\n","  for i, df in enumerate(df_list):\n","\n","    for protocol in df['_ws.col.Protocol'].unique():\n","      df_filtered = df[df['_ws.col.Protocol'] == protocol]\n","\n","      row = [parameter_list[i] + protocol]\n","      col = ['Parameter']\n","\n","      if grouped:\n","        group_num = df_filtered.shape[0]\n","        row.extend([group_num])\n","        col.extend(['Number of groups'])\n","\n","      else:\n","        pkt_num = df_filtered.shape[0]\n","        load = round(df_filtered['frame.len'].sum() *8 / ((df['frame.time_relative'].max() - df['frame.time_relative'].min()) * 1e6), 5) # Mbps\n","        traffic_percentage = round(df_filtered['frame.len'].sum() / df['frame.len'].sum() * 100, 5)\n","        row.extend([pkt_num, load, traffic_percentage])\n","        col.extend(['Number of packets', 'Load (Mbps)', 'Traffic (%)'])\n","\n","      if grouped:\n","        column = ['num.packets','group.frame.len.tot', 'frame.time_relative', 'group.time', 'avg.time.between.packets' ]\n","        column_name = ['Packet number','Group size (bytes)', 'Time btw groups (ms)', 'Group time (ms)', 'Time between group packets (ms)']\n","      else:\n","        column = ['frame.len']\n","        column_name = ['Packet size (bytes)']\n","\n","      for j, col_p in  enumerate(column):\n","        if col_p == 'frame.time_relative':\n","          warnings.filterwarnings('ignore')\n","          df_filtered.loc[:, 'time.btw.group'] = df_filtered[col_p].diff() * 1000\n","          #df_filtered = df_filtered[df_filtered.index % 2 != 0] #just for some specific tests\n","          #df_filtered = df_filtered[df_filtered['num.packets'] == 9]#just for some specific tests\n","\n","          col_p = 'time.btw.group'\n","\n","        avg = round(df_filtered[col_p].mean(), round_val)\n","        stdev = round(df_filtered[col_p].std(), round_val)\n","        min_value = round(df_filtered[col_p].min(), round_val)\n","        max_value = round(df_filtered[col_p].max(), round_val)\n","        median_value = round(df_filtered[col_p].median(), round_val)\n","\n","        if col_p != 'time.btw.group' and col_p != 'group.time' and col_p != 'avg.time.between.packets':\n","          most_common = df_filtered[col_p].value_counts(normalize=True).nlargest(num_most_common)\n","\n","          most_common_str = '\\n '.join(['{} ({:.2%})'.format(x, y) for x, y in zip(most_common.index.tolist(), most_common.tolist())])\n","          if (protocol=='SRTP Video' and col_p == 'group.frame.len.tot'):\n","            most_common_str = most_common_by_interval(percentage_th, num_intervals, df_filtered, max_value, min_value, most_common, most_common_str, col_p, 200)\n","          else:\n","            most_common_str = most_common_by_interval(percentage_th, num_intervals, df_filtered, max_value, min_value, most_common, most_common_str, col_p)\n","\n","        row.extend([avg, stdev, min_value, max_value, median_value])\n","        col.extend(['Avg. ' + column_name[j], column_name[j] + ' Stdev', 'Min. ' + column_name[j], 'Max.' + ' ' + column_name[j], 'Median ' + column_name[j]])\n","\n","        if col_p != 'time.btw.group' and col_p != 'group.time' and col_p != 'avg.time.between.packets':\n","          row.extend([most_common_str] )\n","          col.extend([ 'Most common ' + str(num_most_common) + ' ' + column_name[j]])\n","\n","      rows.append(row)\n","\n","  # create a DataFrame with the rows\n","  df = pd.DataFrame(rows, columns=col)\n","  # format the DataFrame\n","  styled_df = df.style \\\n","    .set_caption(f'<b>{title}</b>') \\\n","    .set_table_styles([{'selector': 'th', 'props': [('border', '1px solid black')]}]) \\\n","    .set_properties(**{'border': '1px solid black', 'text-align': 'center', 'white-space': 'pre-wrap', 'font-size': '10pt'}) \\\n","    .format(format_numeric_values) \\\n","    .hide(axis=\"index\")\n","  # display the styled DataFrame\n","  display(styled_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryp4odDnDPoT"},"outputs":[],"source":["def format_numeric_values(val):\n","  \"\"\"\n","  Formats a numeric value to three decimal places and removes trailing zeros.\n","  If the formatted value ends with '.000', it returns the integer value of the input.\n","\n","  Parameters:\n","    val (str, int or float): The value to format.\n","\n","  Returns:\n","    str or int: The formatted value.\n","  \"\"\"\n","  if isinstance(val, (int, float)):\n","      formatted_val = f\"{val:.4f}\"\n","      if formatted_val.endswith(\".0000\"):\n","          return f\"{int(val)}\"\n","      return formatted_val.rstrip('0').rstrip('.')\n","  return val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzFdQlXo2-R-"},"outputs":[],"source":["def most_common_by_interval(percentage_th: float, num_intervals: int, df_filtered: pd.DataFrame,\n","                            max_value: float, min_value: float, most_common: pd.Series,\n","                            most_common_str: str, column: str, interval_inc: float = 1) -> str:\n","  \"\"\"\n","  Computes the most common intervals of a column in a given dataframe.\n","\n","  Parameters:\n","    percentage_th (float): The minimum percentage threshold for intervals to be considered most common.\n","    num_intervals (int): The number of most common intervals to display.\n","    df_filtered (pandas.DataFrame): The filtered dataframe to analyze.\n","    max_value (float): The maximum value for the column to be analyzed.\n","    min_value (float): The minimum value for the column to be analyzed.\n","    most_common (pandas.Series): A Pandas Series object containing the three most common values\n","                                  in the column, with their respective frequencies normalized.\n","    most_common_str (str): A string containing information about the most common intervals.\n","    column (str): The name of the column to be analyzed.\n","    interval_inc (float): The interval increment for computing intervals.\n","\n","  Returns:\n","    most_common_str (str): The updated string containing information about the most common intervals.\n","  \"\"\"\n","  top_intervals = None\n","\n","\n","  # Check if the sum of the most common values is less than the percentage threshold\n","  if most_common.sum() < percentage_th:\n","    interval_t = 0\n","    top_intervals_sum = 0\n","\n","    # Loop until the sum of the top intervals exceeds the percentage threshold\n","    while top_intervals_sum < percentage_th:\n","      bottom_limit_included = False\n","      interval_t += interval_inc\n","      intervals = []\n","      top_limit = max_value\n","\n","      # Compute intervals by iterating over top limit values\n","      while top_limit > min_value:\n","        bottom_limit = top_limit - interval_t\n","\n","        # Set bottom limit to min_value if it is less than min_value\n","        if bottom_limit < min_value:\n","            bottom_limit = min_value\n","\n","        # Compute the proportion of values in the current interval\n","        if bottom_limit == min_value:\n","          limit = ((df_filtered[column] >= bottom_limit) & (df_filtered[column] <= top_limit)).mean()\n","          bottom_limit_included = True\n","        else:\n","          limit = ((df_filtered[column] > bottom_limit) & (df_filtered[column] <= top_limit)).mean()\n","\n","        # Add the interval and its proportion to the list of intervals\n","        intervals.append((bottom_limit, top_limit, limit))\n","\n","        # Decrement top limit by interval increment\n","        top_limit -= interval_t\n","\n","      # Select the top num_intervals intervals based on their proportion\n","      top_intervals = sorted(intervals, key=lambda x: x[2], reverse=True)[:num_intervals]\n","      top_intervals_sum = sum([x[2] for x in top_intervals])\n","\n","      if interval_t >= max_value:\n","        break\n","\n","  # Add information about top intervals to the most_common_str\n","  if top_intervals is not None:\n","    most_common_str += f'\\n-INTERVALS-'\n","    for j, (bottom, top, limit) in enumerate(top_intervals):\n","      if bottom == min_value and bottom_limit_included:\n","        most_common_str += f'\\n[{top}, {bottom}] ({limit:.2%})'\n","      else:\n","        most_common_str += f'\\n[{top}, {bottom}) ({limit:.2%})'\n","\n","  return most_common_str\n"]},{"cell_type":"markdown","metadata":{"id":"qe8SJU1_HPpl"},"source":["#### **Traffic characteristics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzmIXt0VHPA2"},"outputs":[],"source":["def traffic_stats(df_list: List[pd.DataFrame], title: str, parameter_list: List[str]) -> None:\n","  \"\"\"\n","  Computes and displays traffic statistics in a styled Pandas DataFrame.\n","  To be used for DL and UL traffic characterization.\n","\n","  Args:\n","    df_list (list): A list of Pandas DataFrames containing traffic data.\n","    title (str): The title of the table to be displayed.\n","    parameter_list (list): A list of strings containing the parameters associated with each DataFrame.\n","\n","  Returns:\n","    None.\n","  \"\"\"\n","  round_val = 4\n","\n","  rows = []\n","  for i, df in enumerate(df_list):\n","    # Calculate traffic statistics\n","    avg_packet_size = round(df['frame.len'].mean(), round_val)\n","    stdev = round(df['frame.len'].std(), round_val)\n","    min_value = round(df['frame.len'].min(), round_val)\n","    max_value = round(df['frame.len'].max(), round_val)\n","    most_common = df['frame.len'].value_counts(normalize=True).nlargest(3)\n","    most_common_str = ', '.join([f\"{x} ({y:.2%})\" for x, y in zip(most_common.index.tolist(), most_common.tolist())])\n","    header_len = round((df['frame.len'] - df['udp.length']).mean(), round_val)\n","    time_diff = df['frame.time_relative'].diff() * 1000 # ms\n","    avg_time_diff = round(time_diff.mean(), round_val)\n","    load = round(df['frame.len'].sum() * 8 / ((df['frame.time_relative'].max() - df['frame.time_relative'].min()) * 1e6), round_val) # Mbps\n","\n","    num_pkt = df.shape[0]\n","\n","    # Add statistics to the row\n","    rows.append([parameter_list[i], avg_packet_size, stdev, min_value, max_value, most_common_str, header_len, avg_time_diff, load, num_pkt])\n","\n","  # Create a DataFrame with the rows\n","  df = pd.DataFrame(rows, columns=['Parameter', 'Avg. packet size (bytes)', 'Stdev', 'Min (bytes)', 'Max (bytes)', '3 Most Common (bytes)', 'Frame Header Length (bytes)', 'Avg. time btw packets (ms)', 'Load (Mbps)', 'Total packet number'])\n","\n","  # Format the DataFrame\n","  styled_df = (\n","    df.style\n","    .set_caption(f'<b>{title}</b>') # Set the table caption\n","    .set_table_styles([{'selector': 'th', 'props': [('border', '1px solid black')]}]) # Set the border style for the table header\n","    .set_properties(**{'border': '1px solid black', 'text-align': 'center', 'white-space': 'pre-wrap', 'font-size': '10pt'}) # Set the border style and text alignment for the table cells\n","    .set_table_attributes('style=\"margin-left:auto;margin-right:auto\"') # Center the table on the page\n","    .format({'Avg. packet size (bytes)': '{:.3f}', 'Stdev': '{:.3f}', 'Frame Header Length (bytes)': '{:.0f}', 'Avg. time between packets (ms)': '{:.3f}', 'Load (Mbps)': '{:.4f}'}) # Set the format for the specified columns\n","    .hide(axis=\"index\") # Hide the index column\n","  )\n","\n","  # Display the styled DataFrame\n","  display(styled_df)\n"]},{"cell_type":"markdown","metadata":{"id":"38x0hWq8p_5e"},"source":["## **WebRTC**"]},{"cell_type":"markdown","metadata":{"id":"m8uwKtEKHkBT"},"source":["### **WebRTC Statistics Load**"]},{"cell_type":"markdown","metadata":{"id":"pRA0BEeVvyeo"},"source":["Dataframe load\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRUdg01Cz3-s"},"outputs":[],"source":["def extract_webrtc_data(file_path,directory):\n","  path = directory\n","  completePath = path + file_path\n","  try:\n","    # Load the JSON file into a pandas DataFrame\n","    if file_path.endswith('.json') or file_path.endswith('.txt'):\n","        with open(completePath) as f:\n","          data = json.load(f)\n","          peer_connections = data.get(\"PeerConnections\", {})\n","          if not peer_connections:\n","            print(\"\\033[38;2;255;0;0mError: No PeerConnections data found in the file.\\033[0m\")\n","            return None, None, None, None\n","          # get the first PeerConnection\n","          first_peer_connection = list(peer_connections.values())[0]\n","          if len(list(peer_connections.values())) >1:\n","            print(\"\\033[38;2;255;0;0mError: More than one PeerConnection in the file.\\033[0m\")\n","            return None, None, None, None\n","          stats = first_peer_connection.get(\"stats\", {})\n","          if not stats:\n","            print(\"\\033[38;2;255;0;0mError: No stats data found for the first PeerConnection in the file.\\033[0m\")\n","            return None, None, None, None\n","          if file_path.endswith('.json'):\n","            df, info, timestamp_list =  extract_webrtc_data_json(stats)\n","            side = 'server'\n","          else:\n","            df, info, timestamp_list =  extract_webrtc_data_text(stats)\n","            side = 'client'\n","        if df is None:\n","          print(\"\\033[38;2;255;0;0mError: Something went wrong with \\033[0m\" , file_path, \"\\033[38;2;255;0;0m. Make sure the file exists and has the proper format. \\033[0m\")\n","          return None, None, None, None\n","        print(\"The\" , file_path, \"corresponds to the \\033[38;2;255;165;0m\", side, \"\\033[0mside WebRTC stats\")\n","        return df , info, side, timestamp_list\n","    else:\n","      print(\"\\033[38;2;255;0;0mError: The selected file \\033[0m\" , file_path, \" \\033[38;2;255;0;0mis not a .json or .txt file.\\033[0m\")\n","      return None, None, None, None\n","\n","  except Exception as e:\n","    print(file_path)\n","    print(\"\\033[38;2;255;0;0m\", e ,\":\\033[0m\")\n","    return None, None, None, None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rahXzdgaw9MS"},"outputs":[],"source":["# For data extracted from chrome-internals\n","## Note: This function retrieves data for a single video_id. It is assumed that there is only one candidate pair in the provided stats\n","def extract_webrtc_data_text(stats):\n","  try:\n","    info = {\n","      'startTime': None,\n","      'endTime': None,\n","      'vid_codec': None\n","      }\n","\n","    metrics = {\n","      # Inbound-rtp stats - Downlink - RTP stream received by the Client\n","      ## Focusing on video\n","      'vid_frames_received_per_sec': [],\n","      'vid_frames_per_sec': [],\n","      'vid_frames_decoded_per_sec': [],\n","      'vid_frames_dropped_per_sec': [],\n","      'vid_packets_rec_per_sec': [],\n","      'vid_bits_rec_per_sec': [],\n","      'vid_tot_packets_lost': [],\n","      'vid_avg_jitter_buffer_delay': [],\n","      'vid_jitter': [],\n","      'inter_frame_delay': [],\n","      'inter_frame_delay_std': [],\n","      'frames_rec_minus_decode_and_dropped_tot': [],\n","      'frames_rec_minus_decode_and_dropped': [],\n","\n","      'dec_time_per_frame': [],\n","      'process_del_per_frame': [],\n","      'assembly_time_per_frame': [],\n","      'discarded_pkt': [],\n","\n","      # NO - Remote-outbound-rtp stats - stats reported by the Server (in this case the remote peer): will be looked at the server side webRTC stats\n","\n","      # NO - Data-channel stats: are specific to the WebRTC data channel and does not include any RTP or RTCP packets. Includes packets to establish and maintain the data channel\n","\n","      # NO - Transport stats - as if there is only one candidate pair being used for the WebRTC connection, then the candidate-pair and transport statistics will report the same values!\n","\n","      # CandidatePair stats - reflects the traffic sent and received by the client\n","      ## Note: Stats only for the specific candidate pair\n","      'total_rtt': [], # time it took for a packet to travel from the client to the server and back again\n","      'average_rtt': [],\n","      'current_rtt': [],\n","      'packets_sent_per_sec': [],       # Uplink - RTCP stream sent by the Client\n","      'bits_sent_per_sec': [],          # Uplink - RTCP stream sent by the Client\n","      'packets_rec_per_sec': [],        # Downlink - RTP stream received by the Client\n","      'bits_rec_per_sec': []            # Downlink - RTP stream received by the Client\n","    }\n","\n","    timestamp_list = []\n","\n","    # To get the video inbound-rtp ID\n","    video_id=''\n","    for key, value in stats.items():\n","      if key.endswith(\"-kind\") and value.get(\"statsType\") == \"inbound-rtp\" and \"video\" in value.get(\"values\"):\n","        video_id = key.split(\"-kind\")[0]\n","\n","    print('\\033[38;2;255;165;0mVideo SDP identifier used:\\033[0m', video_id)\n","    for key, value in stats.items():\n","      if key.startswith(\"AP-totalSamplesDuration\"):\n","        info[\"startTime\"] = value.get(\"startTime\")\n","        info[\"endTime\"] = value.get(\"endTime\")\n","      # Video Inbound-rtp stats\n","      if key.startswith(video_id):\n","        if key.endswith(\"-[framesReceived/s]\"): # the ones with [] are computed in chrome-internals\n","          metrics[\"vid_frames_received_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-framesPerSecond\"):\n","          metrics[\"vid_frames_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[framesDecoded/s]\"):\n","          metrics[\"vid_frames_decoded_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-framesDropped\"):\n","          framesDropped = ast.literal_eval(value.get(\"values\")) #total frames dropped\n","          metrics[\"vid_frames_dropped_per_sec\"] = [framesDropped[i] - framesDropped[i-1] for i in range(1, len(framesDropped))]\n","        elif key.endswith(\"-[packetsReceived/s]\"):\n","          metrics[\"vid_packets_rec_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[bytesReceived_in_bits/s]\"):\n","          metrics[\"vid_bits_rec_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-packetsLost\"):\n","          metrics[\"vid_tot_packets_lost\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[jitterBufferDelay/jitterBufferEmittedCount_in_ms]\"):\n","          metrics[\"vid_avg_jitter_buffer_delay\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-jitter\"):\n","          metrics[\"vid_jitter\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[codec]\"):\n","          info[\"vid_codec\"] = json.loads(value.get(\"values\"))[0]\n","        elif key.endswith(\"[totalInterFrameDelay/framesDecoded_in_ms]\"):\n","          metrics[\"inter_frame_delay\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[interFrameDelayStDev_in_ms]\"):\n","          metrics[\"inter_frame_delay_std\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[framesReceived-framesDecoded-framesDropped]\"):\n","          framesLost = ast.literal_eval(value.get(\"values\")) #total frames lost\n","          metrics[\"frames_rec_minus_decode_and_dropped_tot\"] = framesLost\n","          metrics[\"frames_rec_minus_decode_and_dropped\"] = [framesLost[i] - framesLost[i-1] for i in range(1, len(framesLost))]\n","        elif key.endswith(\"-[totalDecodeTime/framesDecoded_in_ms]\"):\n","          metrics[\"dec_time_per_frame\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[totalProcessingDelay/framesDecoded_in_ms]\"):\n","          metrics[\"process_del_per_frame\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-[totalAssemblyTime/framesAssembledFromMultiplePackets_in_ms]\"):\n","          metrics[\"assembly_time_per_frame\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-packetsDiscarded\"):\n","          metrics[\"discarded_pkt\"] = ast.literal_eval(value.get(\"values\"))\n","      # CandidatePair stats\n","      elif value.get('statsType') == \"candidate-pair\":\n","        try:\n","          if info[\"startTime\"]== value.get(\"startTime\") and info[\"endTime\"] == value.get(\"endTime\"):\n","            if key.endswith(\"-totalRoundTripTime\"):\n","              metrics[\"total_rtt\"] = ast.literal_eval(value.get(\"values\"))\n","            elif key.endswith(\"-[totalRoundTripTime/responsesReceived]\"):\n","              metrics[\"average_rtt\"] = ast.literal_eval(value.get(\"values\"))\n","            elif key.endswith(\"-currentRoundTripTime\"):\n","              metrics[\"current_rtt\"] = ast.literal_eval(value.get(\"values\"))\n","            elif key.endswith(\"-[packetsSent/s]\"):\n","              metrics[\"packets_sent_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","            elif key.endswith(\"-[packetsReceived/s]\"):\n","              metrics[\"packets_rec_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","            elif key.endswith(\"-[bytesSent_in_bits/s]\"):\n","              metrics[\"bits_sent_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","            elif key.endswith(\"-[bytesReceived_in_bits/s]\"):\n","              metrics[\"bits_rec_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","            elif key.endswith(\"-[lastPacketSentTimestamp]\"):\n","              for i in range(1, len(ast.literal_eval(value.get(\"values\")))):\n","                current_time = datetime.strptime(ast.literal_eval(value.get(\"values\"))[i], \"%d/%m/%Y, %H:%M:%S\")\n","                previous_time = datetime.strptime(ast.literal_eval(value.get(\"values\"))[i-1], \"%d/%m/%Y, %H:%M:%S\")\n","                diff = (current_time - previous_time).total_seconds()\n","                timestamp_list.append(diff)\n","        except Exception as e:\n","            print(f\"An error occurred while processing {key}: {e}\")\n","            continue\n","    length_metrics = 0\n","    max_key = ''\n","    for key, val in metrics.items():\n","        length = len(val)\n","        if length > length_metrics:\n","            length_metrics = length\n","            max_key = key\n","\n","    #max_key = max(metrics, key=lambda x: len(metrics[x])) #this sometimes gives error do not know why\n","    #length_metrics = len(metrics[max_key])\n","    for key, val in metrics.items():\n","      if len(val) < length_metrics:\n","          print(\"\\033[38;2;255;165;0mWarning: The \\033[0m\", key, \" \\033[38;2;255;165;0m was empty or not complete for the\\033[0m\", filePath, \" \\033[38;2;255;165;0mfile. Its length was:\\033[0m\", len(val), \" \\033[38;2;255;165;0mand expected length was\\033[0m\",length_metrics )\n","          average = sum(val) / len(val) if len(val) > 0 else 0\n","          metrics[key] = [average] * (length_metrics - len(val)) + val\n","    #for m,v in metrics.items():\n","    # print(len(v))\n","    cumulative_timestamps = [0] + [sum(timestamp_list[:i+1]) for i in range(len(timestamp_list))]\n","    print(\"Statistics duration:\", cumulative_timestamps[-1], \"s\")\n","    df = pd.DataFrame(metrics)\n","    print(\"\\033[38;2;0;255;0mThe selected file\\033[0m\" , filePath, \" \\033[38;2;0;255;0m will be considered.\\033[0m\")\n","    return df , info, cumulative_timestamps\n","  except Exception as e:\n","    print(\"\\033[38;2;255;0;0m\", e ,\":\\033[0m\")\n","    return None, None, None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ky8ow4iW5FzC"},"outputs":[],"source":["# For data extracted from Unity webrtc stats\n","## Note: This function retrieves data for a single video_id. It is assumed that there is only one candidate pair in the provided stats\n","def extract_webrtc_data_json(stats):\n","  try:\n","    info = {\n","      'startTime': None,\n","      'endTime': None,\n","      'vid_codec': None\n","      }\n","\n","    metrics = {\n","      # RTCOutboundRTPVideoStream - Downlink - RTP stream sent by the server\n","      ## Video\n","      'vid_frames_sent_per_sec': [],\n","      'vid_frames_per_sec': [],\n","      'vid_frames_encoded_per_sec': [],\n","      'vid_packets_sent_per_sec': [],\n","      'vid_bits_sent_per_sec': [],\n","      'vid_packets_retransmitted_per_sec': [],\n","      'vid_bits_retransmitted_per_sec': [],\n","      'vid_encode_time_per_sec': [], #  number of seconds that have been spent encoding the framesEncoded frames of this stream\n","      'vid_avg_encode_time': [],\n","      'vid_pkt_send_delay_per_sec': [], # number of seconds that packets have spent buffered locally before being transmitted onto the network\n","      'quality_res_changes': [],\n","      'quality_res_reason': [],\n","\n","      # RTCRemoteInboundRtcVideoStream - stats reported by the Client (in this case the remote peer): will be looked in general at the client side\n","      'vid_packets_lost_per_sec': [],\n","      'vid_fraction_lost': [],\n","\n","      # NO - RTCMediaStreamTrack - stats moved to RTCOutboundRTPVideoStream so already considered\n","\n","      # NO - RTCDataChannel stats: are specific to the WebRTC data channel and does not include any RTP or RTCP packets. Includes packets to establish and maintain the data channel\n","\n","      # RTCTransport stats - as if there is only one candidate pair being used for the WebRTC connection, then the candidate-pair and transport statistics will report the same values!\n","      'packets_sent_per_sec': [], # Downlink - RTP stream sent by the Server\n","      'packets_rec_per_sec': [], # Uplink - RTP stream received by the Server\n","\n","      # CandidatePair stats - reflects the traffic sent and received by the server\n","      ## Note: Stats only for the specific candidate pair\n","      'total_rtt': [], # time it took for a packet to travel from the server to the client and back again\n","      'average_rtt': [],\n","      'current_rtt': [],\n","      'bits_sent_per_sec': [], # Downlink - RTP stream sent by the Server\n","      'bits_rec_per_sec': [] # Uplink - RTCP stream received by the Server\n","    }\n","\n","    responses_rec = []\n","    codecId = None\n","    framesEnc = []\n","    timestamp_list = []\n","\n","    #to get the codecId (used to get the codec) and responses received (used to compute the avg. rtt)\n","    for key, value in stats.items():\n","      if key.startswith('RTCOutboundRTPVideoStream'):\n","        if key.endswith(\"-codecId\"):\n","          codecId = json.loads(value.get(\"values\"))[0]\n","        elif key.endswith(\"-framesEncoded\"):\n","          framesEnc = ast.literal_eval(value.get(\"values\"))\n","      elif key.startswith('RTCIceCandidatePair'):\n","        if key.endswith(\"-responsesReceived\"):\n","          responses_rec = ast.literal_eval(value.get(\"values\"))\n","\n","    for key, value in stats.items():\n","      # Video Outbound-rtp stats\n","      if key.startswith('RTCOutboundRTPVideoStream'):\n","        if key.endswith(\"-framesSent\"):\n","          framesSent = ast.literal_eval(value.get(\"values\")) #total frames sent\n","          metrics[\"vid_frames_sent_per_sec\"] = [framesSent[i] - framesSent[i-1] for i in range(1, len(framesSent))]\n","          info[\"startTime\"] = value.get(\"startTime\")\n","          info[\"endTime\"] = value.get(\"endTime\")\n","        elif key.endswith(\"-framesPerSecond\"):\n","          metrics[\"vid_frames_per_sec\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-framesEncoded\"):\n","          metrics[\"vid_frames_encoded_per_sec\"] = [framesEnc[i] - framesEnc[i-1] for i in range(1, len(framesEnc))]\n","        elif key.endswith(\"-packetsSent\"):\n","          pktSent = ast.literal_eval(value.get(\"values\"))\n","          metrics[\"vid_packets_sent_per_sec\"] = [pktSent[i] - pktSent[i-1] for i in range(1, len(pktSent))]\n","        elif key.endswith(\"-bytesSent\"):\n","          bytesSent = ast.literal_eval(value.get(\"values\"))\n","          metrics[\"vid_bits_sent_per_sec\"] = [(bytesSent[i] - bytesSent[i-1])*8 for i in range(1, len(bytesSent))]\n","        elif key.endswith(\"-retransmittedPacketsSent\"):\n","          retPktSent = ast.literal_eval(value.get(\"values\"))\n","          metrics[\"vid_packets_retransmitted_per_sec\"] = [retPktSent[i] - retPktSent[i-1] for i in range(1, len(retPktSent))]\n","        elif key.endswith(\"-retransmittedBytesSent\"):\n","          retBytesSent = ast.literal_eval(value.get(\"values\"))\n","          metrics[\"vid_bits_retransmitted_per_sec\"] = [(retBytesSent[i] - retBytesSent[i-1])*8 for i in range(1, len(retBytesSent))]\n","        elif key.endswith(\"-totalEncodeTime\"):\n","          totEncTime = ast.literal_eval(value.get(\"values\"))\n","          metrics[\"vid_encode_time_per_sec\"] =  [(totEncTime[i] - totEncTime[i-1]) for i in range(1, len(totEncTime))]\n","          metrics[\"vid_avg_encode_time\"] = [x/y if y != 0 else 0 for x, y in zip(ast.literal_eval(value.get(\"values\")), framesEnc)]\n","        elif key.endswith(\"-totalPacketSendDelay\"):\n","          pcktDel = ast.literal_eval(value.get(\"values\"))\n","          metrics[\"vid_pkt_send_delay_per_sec\"] = [(pcktDel[i] - pcktDel[i-1]) for i in range(1, len(pcktDel))]\n","        elif key.endswith(\"-qualityLimitationResolutionChanges\"):\n","          metrics[\"quality_res_changes\"] = ast.literal_eval(value.get(\"values\"))\n","        elif key.endswith(\"-qualityLimitationReason\"):\n","          metrics[\"quality_res_reason\"] = ast.literal_eval(value.get(\"values\"))\n","\n","      # RTCRemoteInboundRtcVideoStream stats\n","      elif key.startswith('RTCRemoteInboundRtpVideoStream'):\n","        if key.endswith(\"-packetsLost\"):\n","          totPktLost = ast.literal_eval(value.get(\"values\")) #total frames sent\n","          metrics[\"vid_packets_lost_per_sec\"] = [(totPktLost[i] - totPktLost[i-1]) for i in range(1, len(totPktLost))]\n","        if key.endswith(\"-fractionLost\"):\n","          metrics[\"vid_fraction_lost\"] = ast.literal_eval(value.get(\"values\"))\n","\n","      # CandidatePair stats\n","      elif key.startswith('RTCIceCandidatePair'):\n","        if value.get(\"values\") is not None:\n","          try:\n","            if abs(len(ast.literal_eval(value.get(\"values\"))) - len(framesEnc)) <=2: # Check if the length of the two lists is equal or differs by 2. This is important for picking the correct candidate pair, and there  are cases where one list may have one less item than the other.\n","              if key.endswith(\"-totalRoundTripTime\"):\n","                metrics[\"total_rtt\"] = ast.literal_eval(value.get(\"values\"))\n","                metrics[\"average_rtt\"] = [x/y if y != 0 else 0 for x, y in zip(ast.literal_eval(value.get(\"values\")), responses_rec)]\n","              elif key.endswith(\"-currentRoundTripTime\"):\n","                metrics[\"current_rtt\"] = ast.literal_eval(value.get(\"values\"))\n","              elif key.endswith(\"-bytesSent\"):\n","                bytesSent = ast.literal_eval(value.get(\"values\"))\n","                metrics[\"bits_sent_per_sec\"] = [(bytesSent[i] - bytesSent[i-1])*8 for i in range(1, len(bytesSent))]\n","\n","              elif key.endswith(\"-bytesReceived\"):\n","                bytesRec = ast.literal_eval(value.get(\"values\"))\n","                metrics[\"bits_rec_per_sec\"] =[0,0] + [(bytesRec[i] - bytesRec[i-1])*8 for i in range(1, len(bytesRec))]\n","\n","          except Exception as e:\n","            print(f\"An error occurred while processing {key}: {e}\")\n","            continue\n","\n","      # Transport stats\n","      elif key.startswith('RTCTransport'):\n","        if abs(len(ast.literal_eval(value.get(\"values\"))) - len(framesEnc)) <=2:\n","          if key.endswith(\"-packetsSent\"):\n","            pktSent = ast.literal_eval(value.get(\"values\"))\n","            metrics[\"packets_sent_per_sec\"] = [pktSent[i] - pktSent[i-1] for i in range(1, len(pktSent))]\n","          elif key.endswith(\"-packetsReceived\"):\n","            pktRec = ast.literal_eval(value.get(\"values\"))\n","            metrics[\"packets_rec_per_sec\"] = [pktRec[i] - pktRec[i-1] for i in range(1, len(pktRec))]\n","\n","      # Codec\n","      elif key.startswith(codecId):\n","        if key.endswith(\"-mimeType\"):\n","          info[\"vid_codec\"] = json.loads(value.get(\"values\"))[0]\n","    length_metrics = 0\n","    max_key = ''\n","    for key, val in metrics.items():\n","        length = len(val)\n","        if length > length_metrics:\n","            length_metrics = length\n","            max_key = key\n","    #length_metrics = len(metrics[max(metrics, key=lambda x: len(metrics[x]))])  does not work sometime because i should use np.max?¿\n","    for key, val in metrics.items():\n","      if len(val) < length_metrics:\n","          print(\"\\033[38;2;255;165;0mWarning: The \\033[0m\", key, \" \\033[38;2;255;165;0m was empty or not complete for the\\033[0m\", filePath, \" \\033[38;2;255;165;0mfile. Its length was:\\033[0m\", len(val), \" \\033[38;2;255;165;0mand expected length was\\033[0m\",length_metrics )\n","          average = sum(val) / len(val) if len(val) > 0 else 0\n","          metrics[key] = [average] * (length_metrics - len(val)) + val\n","\n","    #for m,v in metrics.items():\n","     # print(len(v))\n","    df = pd.DataFrame(metrics)\n","    timestamp_list = [1]*length_metrics\n","    cumulative_timestamps = [0] + [sum(timestamp_list[:i+1]) for i in range(len(timestamp_list))]\n","\n","    print(\"\\033[38;2;0;255;0mThe selected file\\033[0m\" , filePath, \" \\033[38;2;0;255;0m will be considered.\\033[0m\")\n","    return df , info, cumulative_timestamps\n","  except Exception as e:\n","    print(\"\\033[38;2;255;0;0m\", e ,\":\\033[0m\")\n","    return None, None, None\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6-x5lcNp5_s9"},"source":["### **WebRTC Figures build**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThOJ4PLGBdbj"},"outputs":[],"source":["# Define function to create time plot\n","def create_time_plot(df_list, column, title, x_title, y_title, timestamp_list, label_list=None, color_list=None, factor=1):\n","  fig = go.Figure()\n","\n","  for i, df in enumerate(df_list):\n","    fig.add_trace(go.Scatter(x=timestamp_list[i], y=df[column]*factor, name=label_list[i], marker_color=color_list[i], yaxis='y', mode='lines'))\n","\n","  fig.update_layout(\n","    title=title,\n","    font_family=\"Nimbus Roman\",\n","    xaxis=dict(title=x_title),\n","    yaxis=dict(title=y_title),\n","    showlegend=True,\n","    legend=dict(x=1, y=1),\n","  )\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False), #dtick=10,range=[75, 95]\n","      yaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02, #1.02 default, #change to 3 for some plots\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200, #600 for some\n","      height=450) #450 for some or 300\n","\n","  fig.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4sUJPBX6TvJ"},"outputs":[],"source":["# Define function to create ECDF plot (same as for wireshark)\n","def create_ecdf_plot(df_list, column, title, x_title, y_title, label_list = None, color_list = None, factor = 1, diff = False):\n","  fig = go.Figure()\n","  for i, df in enumerate(df_list):\n","      if diff:\n","        df_sorted = np.sort(df[column].diff())\n","      else:\n","        df_sorted = np.sort(df[column])\n","      df_sorted = df_sorted * factor\n","      y= np.arange(1, len(df[column]) + 1) / len(df[column])\n","      fig.add_trace(go.Scatter(x=df_sorted, y=y, name=label_list[i], marker_color=color_list[i], yaxis='y', mode='lines'))\n","  fig.update_layout(title=title, xaxis_title=x_title, yaxis_title= y_title)\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrfEBUcEhMRx"},"outputs":[],"source":["def webrtc_grouped_bar_plot(df_list, column, title, y_title, label_list=None, color_list=None, factor=1, remove_name=True, diff=False, percentile = False):\n","  fig = go.Figure()\n","  colors = px.colors.qualitative.Plotly[:len(df_list)]\n","  for i, df in enumerate(df_list):\n","    values = df[column]*factor\n","    if diff:\n","      values = df[column].diff()*factor\n","    mean_value = np.mean(values)\n","    if not percentile:\n","      error = np.std(values)\n","      x = [label_list[i]]\n","      if remove_name:\n","        name_ = ''\n","      else:\n","        name_ = label_list[i]\n","      fig.add_trace(go.Bar(x=x, y=[mean_value], name=name_,marker_color=colors[i], yaxis='y', error_y=dict(\n","                  type='data',\n","                  array=[error],\n","                  visible=True,symmetric=True,\n","\n","              )))\n","    else:\n","      p_99 = np.nanpercentile(values, 95)\n","      x = [label_list[i]]\n","      if remove_name:\n","        name_ = ''\n","      else:\n","        name_ = label_list[i]\n","      fig.add_trace(go.Bar(\n","          x=x,\n","          y=[p_99],\n","          name=name_,\n","          marker_color=colors[i],\n","          opacity=0.3,\n","          yaxis='y',\n","          offset=-0.4  # negative offset to stack the bar on top of the mean bar\n","      ))\n","\n","      fig.add_trace(go.Bar(\n","          x=x,\n","          y=[mean_value],\n","          name=name_,\n","          marker_color=colors[i],\n","          yaxis='y',\n","      ))\n","\n","\n","\n","  fig.update_layout(\n","    title=title,\n","    font_family=\"Nimbus Roman\",\n","    yaxis=dict(title=y_title),\n","    showlegend=True,\n","    legend=dict(x=1, y=1),\n","\n","  )\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False), #dtick=10,range=[75, 95]\n","      yaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=600, #600\n","      height=600) #450\n","\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyyA_f90tBN5"},"outputs":[],"source":["def webrtc_grouped_box_plot(df_list, column, title, y_title, label_list=None, color_list=None, factor=1, remove_name=True):\n","    fig = go.Figure()\n","    colors = px.colors.qualitative.Plotly[:len(df_list)]\n","\n","    for i, df in enumerate(df_list):\n","        values = df[column] * factor\n","        x = [label_list[i]] * len(values)\n","        if remove_name:\n","            name_ = ''\n","        else:\n","            name_ = label_list[i]\n","        fig.add_trace(go.Box(x=x, y=values, name=name_, marker_color=colors[i]))\n","\n","    fig.update_layout(\n","        title=title,\n","        font_family=\"Nimbus Roman\",\n","        yaxis=dict(title=y_title),\n","        showlegend=True,\n","        legend=dict(x=1, y=1),\n","    )\n","    font_family = \"Times New Roman\"\n","    font_size = 20\n","    fig.update_layout(\n","        font=dict(family=font_family, size=font_size),\n","        template='plotly_white',\n","        xaxis=dict(showgrid=False),\n","        yaxis=dict(showgrid=False),\n","        legend=dict(\n","            orientation=\"h\",\n","            yanchor=\"bottom\",\n","            y=1.02,\n","            xanchor=\"right\",\n","            x=1\n","        )\n","    )\n","\n","    fig.update_layout(\n","        autosize=False,\n","        width=600,\n","        height=450\n","    )\n","\n","    fig.show()\n"]},{"cell_type":"markdown","metadata":{"id":"7Ttqkv4KJCHI"},"source":["### **WebRTC Computations build**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59VegBFSJCHI"},"outputs":[],"source":["def webRtc_traffic_characteristics(df, title, rtc_values, info):\n","    # create an empty DataFrame with columns for each parameter\n","    df_t = pd.DataFrame(columns=['startTime', 'endTime', 'vid_codec'] + list(rtc_values.values()))\n","\n","    # create a list with the statistics for the row\n","    row = [info['startTime'], info['endTime'], info['vid_codec']]\n","    # calculate the mean for each parameter and add it to the row\n","    for key, value in rtc_values.items():\n","      factor = 1\n","      if '(ms)' in value:\n","        factor = 1000\n","      elif  'Mbps' in value:\n","        factor = 1/1e6\n","      if 'Total' in value:\n","        calc_val = round(df[key].max()*factor, 4) #get the total (last value)\n","      else:\n","        calc_val = round(df[key].mean()*factor, 4) #compute the mean\n","        print(key, round(df[key].std()*factor, 4))\n","        print(key, round(df[key].max()*factor, 4))\n","      row.append(calc_val)\n","    # add the row to the DataFrame\n","    df_t.loc[0] = row\n","\n","    # format the DataFrame\n","    styled_df = df_t.style \\\n","        .set_caption(f'<b>{title}</b>') \\\n","        .set_table_styles([{'selector': 'th', 'props': [('border', '1px solid black')]}]) \\\n","        .set_properties(**{'border': '1px solid black', 'text-align': 'center'}) \\\n","        .set_table_attributes('style=\"margin-left:auto;margin-right:auto\"') \\\n","        .format(precision=4)\n","\n","    # display the styled DataFrame\n","    display(styled_df)"]},{"cell_type":"markdown","metadata":{"id":"RzBrGRlHeSTm"},"source":["## **Color palette**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZcCj5kceXkC"},"outputs":[],"source":["def generate_color_palette(df_list: List[pd.DataFrame]) -> List[tuple]:\n","  \"\"\"\n","  Generates a list of RGB tuples from a list of dataframes using Seaborn's color palette.\n","\n","  Parameters:\n","    df_list (List[pd.DataFrame]): A list of pandas dataframes.\n","\n","  Returns:\n","    color_list (List[tuple]): A list of RGB tuples.\n","  \"\"\"\n","  # Number of colors to generate\n","  n_colors = len(df_list)\n","\n","  # Generate a list of n_colors using the color palette\n","  color_palette = sns.color_palette(n_colors=n_colors)\n","\n","  # Convert the color_palette to a list of RGB tuples\n","  color_list = [tuple(map(lambda x: int(x*255), color)) for color in color_palette]\n","\n","  return color_list"]},{"cell_type":"markdown","metadata":{"id":"rbUx_5fyoF0u"},"source":["# **SUPPORT**"]},{"cell_type":"markdown","metadata":{"id":"aaN0OjR8j_78"},"source":["## **File Demo Helper**"]},{"cell_type":"markdown","metadata":{"id":"aIZCWwOzaTCB"},"source":["### **File locator**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phr5jidZjNoN"},"outputs":[],"source":["file = {'path': None}\n","\n","# Create an output widget to display messages\n","output_widget = widgets.Output()\n","\n","select_file(directory, file, output_widget)"]},{"cell_type":"markdown","metadata":{"id":"JRSslAGWu-2b"},"source":["### **File format validator and path retrieval**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hOSghhwu_FW"},"outputs":[],"source":["validate_and_retrieve_file_path(file, directory)"]},{"cell_type":"markdown","metadata":{"id":"rQN7SewkoIks"},"source":["# **RESULTS**"]},{"cell_type":"markdown","metadata":{"id":"2rpyAuOtpTaw"},"source":["## **Wireshark**"]},{"cell_type":"markdown","metadata":{"id":"6PL5GKMe6OT8"},"source":["### **Run Wireshark Demo**"]},{"cell_type":"markdown","metadata":{"id":"7WOZquzV-rhp"},"source":["Insert traces path in *files_path* list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4VsNmU8DgLh"},"outputs":[],"source":["# Insert the traces path\n","files_path = ['Datasets/90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/Tshark Server - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.csv'\n","              #,'Datasets/90fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz/Client/Tshark Client - 90fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz.csv'\n","              ]\n","\n","df_list, df_dl_list, df_ul_list, df_dl_z_list, df_ul_z_list, files_path_list = get_dataframes(files_path, directory, server_ip,\n","                                                                                              client_ip, complete_trace_start_time , complete_trace_end_time ,\n","                                                                                              specific_portion_start_time , specific_portion_end_time )\n","\n","df_grouped_dl_list, df_grouped_ul_list, df_grouped_dl_z_list, df_grouped_ul_z_list = get_grouped_dataframes(df_dl_list, df_ul_list,\n","                                                                                                            df_dl_z_list, df_ul_z_list, grouping_time)\n","\n","df_traffic_dl_list, df_traffic_ul_list = get_traffic_dataframes(df_dl_list, df_ul_list)\n","\n","\n","df_dl_list_grouped_mark = group_vid_dataframes_by_time_info(df_dl_list)\n","df_dl_list_grouped_z_mark = group_vid_dataframes_by_time_info(df_dl_z_list)\n","\n","label_list = [os.path.splitext(os.path.basename(file_path))[0] for file_path in files_path_list]\n","\n","color_list = generate_color_palette(df_list)\n"]},{"cell_type":"markdown","source":["#### **Add phone to demo**"],"metadata":{"id":"dwMmpHLsugeA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGSORumH0Iko"},"outputs":[],"source":["#to add the smartphone traces to the comparison set to True. If you want to use only the client traces just change the client_ip to the following in the Parameters section\n","client_traces = False\n","client_ip = \"192.168.50.105\"#phone\n","\n","files_path = ['Datasets/PhoneClientMoves - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/Tshark Server - PhoneClientMoves - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.csv'\n","              ]\n","if client_traces:\n","  df_list_2, df_dl_list_2, df_ul_list_2, df_dl_z_list_2, df_ul_z_list_2, files_path_list_2 = get_dataframes(files_path, directory, server_ip,\n","                                                                                                client_ip, complete_trace_start_time , complete_trace_end_time ,\n","                                                                                                specific_portion_start_time , specific_portion_end_time )\n","\n","  df_grouped_dl_list_2, df_grouped_ul_list_2, df_grouped_dl_z_list_2, df_grouped_ul_z_list_2 = get_grouped_dataframes(df_dl_list_2, df_ul_list_2,\n","                                                                                                              df_dl_z_list_2, df_ul_z_list_2, grouping_time)\n","\n","  df_traffic_dl_list_2, df_traffic_ul_list_2 = get_traffic_dataframes(df_dl_list_2, df_ul_list_2)\n","\n","  df_list = df_list + df_list_2\n","  df_dl_list = df_dl_list + df_dl_list_2\n","  df_ul_list = df_ul_list + df_ul_list_2\n","  df_dl_z_list = df_dl_z_list + df_dl_z_list_2\n","  df_ul_z_list = df_ul_z_list + df_ul_z_list_2\n","  files_path_list = files_path_list + files_path_list_2\n","  df_grouped_dl_list = df_grouped_dl_list + df_grouped_dl_list_2\n","  df_grouped_ul_list = df_grouped_ul_list + df_grouped_ul_list_2\n","  df_grouped_dl_z_list = df_grouped_dl_z_list + df_grouped_dl_z_list_2\n","  df_grouped_ul_z_list = df_grouped_ul_z_list + df_grouped_ul_z_list_2\n","  df_traffic_dl_list = df_traffic_dl_list + df_traffic_dl_list_2\n","  df_traffic_ul_list = df_traffic_ul_list + df_traffic_ul_list_2\n","\n","\n","  label_list = [os.path.splitext(os.path.basename(file_path))[0] for file_path in files_path_list]\n","\n","  color_list = generate_color_palette(df_list)"]},{"cell_type":"markdown","metadata":{"id":"my9NvJNMyE2f"},"source":["#### **Run for SRTP Video**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGmsctpODDzz"},"outputs":[],"source":["# Video stream packets grouped per frame\n","df_dl_list_grouped_mark = group_vid_dataframes_by_time_info(df_dl_list)\n","df_dl_list_grouped_z_mark = group_vid_dataframes_by_time_info(df_dl_z_list)\n","\n","df_dl_list_srtp_correct = []\n","for df in df_dl_list:\n","  df_copy = df.copy()\n","  df_copy['_ws.col.Time'] = df_copy['_ws.col.Info'].apply(extract_time)\n","  df_dl_list_srtp_correct.append(df_copy.reset_index(drop=True))\n","df_dl_list_z_srtp_correct = []\n","for df in df_dl_z_list:\n","  df_copy = df.copy()\n","  df_copy['_ws.col.Time'] = df_copy['_ws.col.Info'].apply(extract_time)\n","  df_dl_list_z_srtp_correct.append(df_copy.reset_index(drop=True))\n","\n","# Video stream packets grouped per batches within a frame\n","df_grouped_dl_list_srtp_correct = group_dataframes_temp(df_dl_list, grouping_time) #because the other grouping did not take into account if they were from the same frame\n","df_grouped_dl_list_z_srtp_correct = group_dataframes_temp(df_dl_z_list, grouping_time) #because the other grouping did not take into account if they were from the same frame\n","\n","df_grouped_dl_list_grouped_mark, df_grouped_dl_list_grouped_mark_not_nan = group_grouped_vid_dataframes_by_time_info(df_grouped_dl_list_srtp_correct)"]},{"cell_type":"markdown","metadata":{"id":"z-u44jV2EWXn"},"source":["### **Wireshark Demo Figures**"]},{"cell_type":"markdown","metadata":{"id":"OltgCjNjFmG0"},"source":["#### **Packet size vs Time**"]},{"cell_type":"markdown","metadata":{"id":"XrBHDyg12KLs"},"source":["##### **Time-filtered - Non-Grouped**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6mUGj5D2M9h"},"outputs":[],"source":["# Set to True to make the plot. Warning: if True, the plot can be computationally intensive.\n","\n","plot_time_filtered = True\n","\n","if plot_time_filtered:\n","  # Loop over all dataframes in df_list and plot packet size vs time for each one\n","  for i, df in enumerate(df_list):\n","    # Get the file name from the path\n","    file_name = os.path.splitext(os.path.basename(files_path_list[i]))[0]\n","    title = file_name\n","    \"\"\"\n","    Call the create_scatter_plot function to create the plot\n","\n","    Parameters:\n","      df_dl_t_list[i]: DataFrame containing downlink traffic data filtered by time.\n","      df_ul_t_list[i]: DataFrame containing upload traffic data filtered by time.\n","      'frame.time_relative': Column name for the x-axis data.\n","      'frame.len': Column name for the y-axis data.\n","      title: The title of the plot.\n","      'Time (s)': Label for the x-axis.\n","      'Packet size (bytes)': Label for the y-axis.\n","      True if the data should be grouped by protocol, False otherwise.\n","      False\n","      None\n","      True if x_axis should start from 0\n","      True if vertical lines should be plotted (stem plot)\n","      None if not specific protocol should be plotted, list of str specifying the specific protocl otherwise.\n","\n","    \"\"\"\n","    create_scatter_plot(df_dl_list[i], df_ul_list[i], 'frame.time_relative',\n","                        'frame.len', '', 'Time (s)', 'Packet size (bytes)', True, False, None, True, False, 'DTLSv1.2')"]},{"cell_type":"markdown","metadata":{"id":"iiF41_xh2RPH"},"source":["##### **Time-filtered - Grouped**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3XKK-2X2VDs"},"outputs":[],"source":["# Set to True to make the plot. Warning: if True, the plot can be computationally intensive.\n","plot_time_filtered = True\n","\n","if plot_time_filtered:\n","  # Loop over all dataframes and plot packet size vs time for each one\n","  for i, df in enumerate(df_list):\n","    # Get the file name from the path\n","    file_name = os.path.splitext(os.path.basename(files_path_list[i]))[0]\n","    title = 'Grouped ' + file_name\n","    \"\"\"\n","    Call the create_scatter_plot function to create the plot\n","\n","    Parameters:\n","      df_grouped_dl_t_list[i]: DataFrame containing downlink traffic data grouped and filtered by time.\n","      df_grouped_ul_t_list[i]: DataFrame containing upload traffic data grouped and filtered by time.\n","      'frame.time_relative': Column name for the x-axis data.\n","      'group.frame.len.tot': Column name for the y-axis data.\n","      title: The title of the plot.\n","      'Time (s)': Label for the x-axis.\n","      'Total Group Packet size (bytes)': Label for the y-axis.\n","      True if the data should be grouped by protocol, False otherwise.\n","      True if the text should be shown on markers, False otherwise.\n","      'num.packets': The column name to use for the text labels.\n","      True if x_axis should start from 0\n","      True if vertical lines should be plotted (stem plot)\n","      None if not specific protocol should be plotted, list of str specifying the specific protocl otherwise\n","    \"\"\"\n","    create_scatter_plot(df_grouped_dl_list[i], df_grouped_ul_list[i], 'frame.time_relative',\n","                       'group.frame.len.tot', '', 'Time (s)', 'Total Group Packet size (bytes)', True, False, 'num.packets', True, False, ['SRTP Video'])\n","    #for video by frames\n","    #create_scatter_plot(df_grouped_dl_list_srtp_correct[i], df_grouped_ul_list[i], 'frame.time_relative',\n","     #                     'group.frame.len.tot', '', 'Time (s)', 'Total Group Packet size (bytes)', True, False, 'num.packets', True, False, ['SRTP Video'])"]},{"cell_type":"markdown","metadata":{"id":"7ocMUHzF15jr"},"source":["##### **Zoomed - Non-Grouped**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6y6y3Qc95NPM"},"outputs":[],"source":["# Loop over all dataframes in df_list and plot packet size vs time for each one\n","for i, df in enumerate(df_list):\n","  # Get the file name from the path\n","  file_name = os.path.splitext(os.path.basename(files_path_list[i]))[0]\n","  title = file_name\n","  \"\"\"\n","  Call the create_scatter_plot function to create the plot\n","\n","  Parameters:\n","    df_dl_z_list[i]: DataFrame containing downlink traffic data filtered by time.\n","    df_ul_z_list[i]: DataFrame containing upload traffic data filtered by time.\n","    'frame.time_relative': Column name for the x-axis data.\n","    'frame.len': Column name for the y-axis data.\n","    title: The title of the plot.\n","    x_label: Label for the x-axis.\n","    'Packet size (bytes)': Label for the y-axis.\n","    True if the data should be grouped by protocol, False otherwise.\n","    False\n","    None\n","    True if x_axis should start from 0\n","    True if vertical lines should be plotted (stem plot)\n","    None if not specific protocol should be plotted, list of str specifying the specific protocl otherwise\n","\n","  \"\"\"\n","  #create_scatter_plot(df_dl_z_list[i], df_ul_z_list[i], 'frame.time_relative',\n","        #              'frame.len', '', 'Time (ms)', 'Packet size (bytes)', True, False, None, True, False)\n","  #create_scatter_plot_temp(df_dl_z_list[i], df_ul_z_list[i], 'frame.time_relative',\n","                    #  'frame.len', '', 'Time (ms)', 'Packet size (bytes)', True, False, None, True, True, ['SRTP Audio'])\n","  #create_scatter_plot_temp(df_dl_z_list[i], df_ul_z_list[i], 'frame.time_relative',\n","        #              'frame.len', '', 'Time (ms)', 'Packet size (bytes)', True, False, None, True, False, ['SRTP Video'])\n","  #create_scatter_plot(df_dl_z_list[i], df_ul_z_list[i], 'frame.time_relative',\n","              #      'frame.len', '', 'Time (ms)', 'Packet size (bytes)', True, False, None, True, False, ['SRTP Video'])\n","  create_scatter_plot_temp_client_vid(df_dl_list_z_srtp_correct[i], df_ul_z_list[i], 'frame.time_relative',\n","                      'frame.len', '', 'Time (ms)', 'Packet size (bytes)', True, False, None, True, True, ['SRTP Video'])"]},{"cell_type":"markdown","metadata":{"id":"rRPK5RQSqiTV"},"source":["##### **Zoomed - Grouped**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-0ZuPQGqm1h"},"outputs":[],"source":["\n","# Loop over all dataframes and plot packet size vs time for each one\n","for i, df in enumerate(df_list):\n","  # Get the file name from the path\n","  file_name = os.path.splitext(os.path.basename(files_path_list[i]))[0]\n","  title = 'Grouped ' + file_name\n","  \"\"\"\n","  Call the create_scatter_plot function to create the plot\n","\n","  Parameters:\n","    df_grouped_dl_z_list[i]: DataFrame containing downlink traffic data grouped and filtered by time.\n","    df_grouped_ul_z_list[i]: DataFrame containing upload traffic data grouped and filtered by time.\n","    'frame.time_relative': Column name for the x-axis data.\n","    'group.frame.len.tot': Column name for the y-axis data.\n","    title: The title of the plot.\n","    'Time (s)': Label for the x-axis.\n","    'Total Group Packet size (bytes)': Label for the y-axis.\n","    True if the data should be grouped by protocol, False otherwise.\n","    True if the text should be shown on markers, False otherwise.\n","    'num.packets': The column name to use for the text labels.\n","    True if x_axis should start from 0\n","    True if vertical lines should be plotted (stem plot)\n","    None if not specific protocol should be plotted, list of str specifying the specific protocl otherwise\n","  \"\"\"\n","  #create_scatter_plot(df_grouped_dl_z_list[i], df_grouped_ul_z_list[i], 'frame.time_relative',\n","            #          'group.frame.len.tot', title, 'Time (ms)', 'Total Group Packet size (bytes)', True, True, 'num.packets', True, False, ['SRTP Audio'])\n"," # create_scatter_plot(df_grouped_dl_z_list[i], df_grouped_ul_z_list[i], 'frame.time_relative',\n","  #                    'group.frame.len.mean', title, 'Time (ms)', 'Packet size (bytes)', True, True, 'num.packets', True, True, ['SRTP Video'])\n","\n","  create_scatter_plot_temp_client_vid(df_grouped_dl_list_z_srtp_correct[i], df_grouped_ul_z_list[i], 'frame.time_relative',\n","                      'group.frame.len.tot', '', 'Time (ms)', 'Group Size (bytes)', True, True, 'num.packets', True, False, ['SRTP Video'])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtqemT6ZPoYH"},"outputs":[],"source":["\n","# Loop over all dataframes and plot packet size vs time for each one\n","for i, df in enumerate(df_list):\n","  # Get the file name from the path\n","  file_name = os.path.splitext(os.path.basename(files_path_list[i]))[0]\n","  title = 'Grouped ' + file_name\n","  \"\"\"\n","  Call the create_scatter_plot function to create the plot\n","\n","  Parameters:\n","    df_grouped_dl_z_list[i]: DataFrame containing downlink traffic data grouped and filtered by time.\n","    df_grouped_ul_z_list[i]: DataFrame containing upload traffic data grouped and filtered by time.\n","    'frame.time_relative': Column name for the x-axis data.\n","    'group.frame.len.tot': Column name for the y-axis data.\n","    title: The title of the plot.\n","    'Time (s)': Label for the x-axis.\n","    'Total Group Packet size (bytes)': Label for the y-axis.\n","    True if the data should be grouped by protocol, False otherwise.\n","    True if the text should be shown on markers, False otherwise.\n","    'num.packets': The column name to use for the text labels.\n","    True if x_axis should start from 0\n","    True if vertical lines should be plotted (stem plot)\n","    None if not specific protocol should be plotted, list of str specifying the specific protocl otherwise\n","  \"\"\"\n","  create_scatter_plot(df_dl_list_grouped_z_mark[i], df_grouped_ul_z_list[i], 'frame.time_relative',\n","                      'group.frame.len.tot', title, 'Time (ms)', 'Total Group Packet size (bytes)', True, True, 'num.packets', True, False, ['SRTP Video'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2jGbIQWptLb"},"outputs":[],"source":["#Colored\n","def create_scatter_plot_temp(df_dl: pd.DataFrame, df_ul: pd.DataFrame, x_col: str, y_col: str, title: str,\n","                        x_title: str, y_title: str, protocols: bool = False, text_show: bool = False, number_column: str = None,\n","                        from0: bool = False, stem: bool = False, spec_prot: List[str] = None) -> None:\n","  \"\"\"\n","  Function to create a scatter plot with optional grouping by protocol and optional display of text labels.\n","\n","\n","  Parameters:\n","    df_dl (pd.DataFrame): The downlink data as a Pandas DataFrame.\n","    df_ul (pd.DataFrame): The uplink data as a Pandas DataFrame.\n","    x_col (str): The column name to use for the x-axis.\n","    y_col (str): The column name to use for the y-axis.\n","    title (str): The title of the plot.\n","    x_title (str): The title of the x-axis.\n","    y_title (str): The title of the y-axis.\n","    protocols (bool, optional): Whether to group data by protocol. Default is False.\n","    text_show (bool, optional): Whether to display text labels on the plot. Default is False.\n","    number_column (str, optional): The column name to use for the text labels. Default is None.\n","    from0 (bool, optional): Whether to make the plot from 0.\n","    stem (bool, optional): Whether to include vertical lines to simulate a stem plot.\n","    spec_prot (List[str], optional): Whether to plot specific protocols\n","  Returns:\n","    None\n","  \"\"\"\n","  if protocols:\n","\n","\n","    dl_protocols = ['DTLSv1.2', 'DTLS', 'SRTP Audio', 'SRTP Video','STUN' ,'UDP']  # Define manually all the DL protocols so that they always have the same assigned color\n","    # dl_protocols = sorted(df_dl['_ws.col.Protocol'].unique()) # Get unique protocols in DL and assign a color to each - might not produce same colors in different executions\n","    dl_colors = px.colors.qualitative.Plotly[:len(dl_protocols)] # Plotly has 10 colors\n","    protocol_color_map_dl = dict(zip(dl_protocols, dl_colors))\n","\n","\n","    ul_protocols = ['DTLSv1.2', 'SRTCP', 'STUN', 'UDP'] # Define manually all the UL protocols so that they always have the same assigned color\n","    # ul_protocols = sorted(df_ul['_ws.col.Protocol'].unique())    # Get unique protocols in UL and assign a color to each - might not produce same colors in different executions\n","    ul_colors = px.colors.qualitative.Plotly[len(dl_protocols):len(dl_protocols) + len(ul_protocols)] # Plotly has 10 colors\n","    protocol_color_map_ul = dict(zip(ul_protocols, ul_colors))\n","\n","\n","  else:\n","    dataframes = ['DL', 'UL']\n","    dataframes_col = px.colors.qualitative.Plotly[:len(dataframes)]\n","    color_map_dl = dict(zip(dataframes, dataframes_col))\n","\n","\n","  # Set the mode of the plot\n","  if text_show:\n","    mode_plt = 'markers+text'\n","  else:\n","    mode_plt = 'markers'\n","\n","\n","  fig = go.Figure()\n","\n","  if protocols:\n","    # Add traces for each protocol in dl and ul\n","    for i, df in enumerate([df_dl, df_ul]):\n","      for j, prot in enumerate(sorted(df['_ws.col.Protocol'].unique())):\n","        if spec_prot!=None and prot not in spec_prot:\n","          continue\n","        color = protocol_color_map_dl[prot] if i == 0 else protocol_color_map_ul[prot]\n","        filtered_df = df[df['_ws.col.Protocol'] == prot]\n","        groups = []\n","        if prot=='SRTP Audio':\n","          # Define the number of rows per group\n","\n","          group_size = 8\n","          filtered_df.reset_index(drop=True, inplace=True)\n","\n","\n","          # Create an additional column to indicate the group for each row\n","          #filtered_df['Group'] = (filtered_df.index // group_size) % 2  # Alternate between 0 and 1 for every group_size rows\n","          #filtered_df['Group'] = filtered_df.apply(lambda row: 0 if row['num.packets'] == 9 else 1, axis=1)\n","\n","          # Split the DataFrame into two groups based on the 'Group' column\n","          #group_1 = filtered_df[filtered_df['Group'] == 0]\n","          #group_2 = filtered_df[filtered_df['Group'] == 1]\n","          #groups = [group_1,group_2]\n","\n","          #Zoomed:\n","          df_group_1 = pd.concat([filtered_df[:9].copy(), filtered_df[16:25].copy()], ignore_index=True) #the escape room\n","          #df_group_1 = pd.concat([filtered_df[:8].copy(), filtered_df[16:24].copy()], ignore_index=True)\n","          df_group_1['Group'] = 0\n","\n","          df_group_2 = pd.concat([filtered_df[9:16].copy(), filtered_df[25:32].copy()], ignore_index=True) #the escape room\n","          #df_group_2 = pd.concat([filtered_df[8:16].copy(), filtered_df[24:32].copy()], ignore_index=True)\n","          df_group_2['Group'] = 1\n","          groups = [df_group_1,df_group_2]\n","\n","          color_groups = [color,'#02aeba']\n","\n","        if prot=='SRTP Video':\n","          filtered_df.reset_index(drop=True, inplace=True)\n","          num_pkt = [110,164,162,174] #VP9\n","          num_pkt = [41,49,42, 108,35,88,45,61,94,105,57] #H264\n","\n","          filtered_df['Group'] = ''\n","          group_idx = 0\n","          color_groups = ['#6f42f5','#a442f5','#d442f5' , '#f774c7', '#f5426f', '#f57842', '#f5b942', '#d6cc3e', '#7ef542', '#42f5c5','#3ec2d6']\n","          row_idx = 0\n","          groups = []\n","          for num in num_pkt:\n","            filtered_df.loc[row_idx:row_idx +num-1, 'Group'] = group_idx\n","            groups.append(filtered_df[filtered_df['Group'] == group_idx])\n","            group_idx+=1\n","            row_idx+=num\n","        for z in range(len(groups)):\n","          filtered_df = groups[z]\n","          if from0: #plot x_axis starting from 0\n","            #x_val = (filtered_df[x_col]- min(df_dl[x_col].min(), df_ul[x_col].min()))\n","            x_val = filtered_df[x_col]- df_dl[x_col].min()\n","\n","          else:\n","            x_val = filtered_df[x_col]\n","\n","          if \"ms\" in x_title:\n","            x_val*=1000 # convert to ms\n","\n","\n","          fig.add_trace(\n","            go.Scatter(\n","              mode = mode_plt,\n","              x = x_val,\n","              y = filtered_df[y_col],\n","              name = f\"{prot} Batch Type {z+1}\", # Change to group or frame\n","              marker = dict(color = color_groups[z]),\n","              text = filtered_df[number_column].tolist() if number_column else None,\n","              textfont = dict(size=10),\n","              textposition = 'top center'\n","                      )\n","          )\n","          if stem:\n","            for x_v,y_v in zip(x_val,filtered_df[y_col]):\n","              fig.add_shape(\n","                type='line',\n","                x0=x_v,\n","                y0=0,\n","                x1=x_v,\n","                y1=y_v,\n","                line=dict(color = color_groups[z]),\n","                opacity=1\n","              )\n","\n","\n","\n","  fig.update_layout(\n","    title=title,\n","    xaxis=dict(title=x_title),\n","    yaxis=dict(title=y_title),\n","    showlegend=True,\n","    legend=dict(x=1, y=1),\n","  )\n","\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False), #,dtick=2\n","      yaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200,\n","      height=300)\n","\n","\n","\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGcZ80DPJbTx"},"outputs":[],"source":["#Colored\n","def create_scatter_plot_temp_client_vid(df_dl: pd.DataFrame, df_ul: pd.DataFrame, x_col: str, y_col: str, title: str,\n","                        x_title: str, y_title: str, protocols: bool = False, text_show: bool = False, number_column: str = None,\n","                        from0: bool = False, stem: bool = False, spec_prot: List[str] = None) -> None:\n","  \"\"\"\n","  Function to create a scatter plot with optional grouping by protocol and optional display of text labels.\n","\n","\n","  Parameters:\n","    df_dl (pd.DataFrame): The downlink data as a Pandas DataFrame.\n","    df_ul (pd.DataFrame): The uplink data as a Pandas DataFrame.\n","    x_col (str): The column name to use for the x-axis.\n","    y_col (str): The column name to use for the y-axis.\n","    title (str): The title of the plot.\n","    x_title (str): The title of the x-axis.\n","    y_title (str): The title of the y-axis.\n","    protocols (bool, optional): Whether to group data by protocol. Default is False.\n","    text_show (bool, optional): Whether to display text labels on the plot. Default is False.\n","    number_column (str, optional): The column name to use for the text labels. Default is None.\n","    from0 (bool, optional): Whether to make the plot from 0.\n","    stem (bool, optional): Whether to include vertical lines to simulate a stem plot.\n","    spec_prot (List[str], optional): Whether to plot specific protocols\n","  Returns:\n","    None\n","  \"\"\"\n","  if protocols:\n","\n","\n","    dl_protocols = ['DTLSv1.2', 'DTLS', 'SRTP Audio', 'SRTP Video','STUN' ,'UDP']  # Define manually all the DL protocols so that they always have the same assigned color\n","    # dl_protocols = sorted(df_dl['_ws.col.Protocol'].unique()) # Get unique protocols in DL and assign a color to each - might not produce same colors in different executions\n","    dl_colors = px.colors.qualitative.Plotly[:len(dl_protocols)] # Plotly has 10 colors\n","    protocol_color_map_dl = dict(zip(dl_protocols, dl_colors))\n","\n","\n","    ul_protocols = ['DTLSv1.2', 'SRTCP', 'STUN', 'UDP'] # Define manually all the UL protocols so that they always have the same assigned color\n","    # ul_protocols = sorted(df_ul['_ws.col.Protocol'].unique())    # Get unique protocols in UL and assign a color to each - might not produce same colors in different executions\n","    ul_colors = px.colors.qualitative.Plotly[len(dl_protocols):len(dl_protocols) + len(ul_protocols)] # Plotly has 10 colors\n","    protocol_color_map_ul = dict(zip(ul_protocols, ul_colors))\n","\n","\n","  else:\n","    dataframes = ['DL', 'UL']\n","    dataframes_col = px.colors.qualitative.Plotly[:len(dataframes)]\n","    color_map_dl = dict(zip(dataframes, dataframes_col))\n","\n","\n","  # Set the mode of the plot\n","  if text_show:\n","    mode_plt = 'markers+text'\n","  else:\n","    mode_plt = 'markers'\n","\n","\n","  fig = go.Figure()\n","\n","  if protocols:\n","    # Add traces for each protocol in dl and ul\n","    for i, df in enumerate([df_dl, df_ul]):\n","      for j, prot in enumerate(sorted(df['_ws.col.Protocol'].unique())):\n","        if spec_prot!=None and prot not in spec_prot:\n","          continue\n","        color = protocol_color_map_dl[prot] if i == 0 else protocol_color_map_ul[prot]\n","        filtered_df = df[df['_ws.col.Protocol'] == prot]\n","        groups = []\n","\n","        if prot=='SRTP Video':\n","          filtered_df['_ws.col.Time'] = filtered_df['_ws.col.Info'].apply(extract_time)\n","          filtered_df.reset_index(drop=True, inplace=True)\n","\n","          filtered_df['Group'] = ''\n","          color_groups = ['#6f42f5','#a442f5','#d442f5' , '#f774c7', '#f5426f', '#f57842', '#f5b942', '#d6cc3e', '#7ef542', '#42f5c5','#3ec2d6', '#153359']\n","          row_idx = 0\n","          groups = []\n","\n","          for val in  filtered_df['_ws.col.Time'].unique():\n","            filtered_df_temp = filtered_df.copy()\n","            filtered_df_temp = filtered_df_temp[filtered_df_temp['_ws.col.Time']==val]\n","            groups.append(filtered_df_temp)\n","\n","        for z in range(len(groups)):\n","          filtered_df = groups[z]\n","          if from0: #plot x_axis starting from 0\n","            #x_val = (filtered_df[x_col]- min(df_dl[x_col].min(), df_ul[x_col].min()))\n","            x_val = filtered_df[x_col]- df_dl[x_col].min()\n","\n","          else:\n","            x_val = filtered_df[x_col]\n","\n","          if \"ms\" in x_title:\n","            x_val*=1000 # convert to ms\n","\n","\n","          fig.add_trace(\n","            go.Scatter(\n","              mode = mode_plt,\n","              x = x_val,\n","              y = filtered_df[y_col],\n","              name = f\"{prot} Frame {z+1}\", # Change to group or frame\n","              marker = dict(color = color_groups[z]),\n","              text = filtered_df[number_column].tolist() if number_column else None,\n","              textfont = dict(size=10),\n","              textposition = 'top center'\n","                      )\n","          )\n","          if stem:\n","            for x_v,y_v in zip(x_val,filtered_df[y_col]):\n","              fig.add_shape(\n","                type='line',\n","                x0=x_v,\n","                y0=0,\n","                x1=x_v,\n","                y1=y_v,\n","                line=dict(color = color_groups[z]),\n","                opacity=1\n","              )\n","\n","\n","\n","  fig.update_layout(\n","    title=title,\n","    xaxis=dict(title=x_title),\n","    yaxis=dict(title=y_title),\n","    showlegend=True,\n","    legend=dict(x=1, y=1),\n","  )\n","\n","  font_family = \"Times New Roman\"\n","  font_size = 20\n","  fig.update_layout(\n","      font=dict(family=font_family, size=font_size),\n","      template='plotly_white',\n","      xaxis=dict(showgrid=False), #,dtick=2\n","      yaxis=dict(showgrid=False),\n","      legend=dict(\n","      orientation=\"h\",\n","      yanchor=\"bottom\",\n","      y=1.02,\n","      xanchor=\"right\",\n","      x=1)\n","  )\n","\n","  fig.update_layout(\n","      autosize=False,\n","      width=1200,\n","      height=400)\n","\n","\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"fIGFGCXSVDun"},"source":["##### **Stem plot matlab (much faster doing the stem than previous)**"]},{"cell_type":"markdown","metadata":{"id":"-bm43cx8Ip_R"},"source":["###### **Stem Time-filtered - Non-Grouped**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ql77HJ5Iq02"},"outputs":[],"source":["plot_stem_matlab = True\n","if plot_stem_matlab:\n","  # Loop over all dataframes in df_list and plot packet size vs time for each one\n","  for i, df in enumerate(df_list):\n","    # Get the file name from the path\n","    file_name = os.path.splitext(os.path.basename(files_path_list[i]))[0]\n","    title = file_name\n","    \"\"\"\n","    Call the create_scatter_plot function to create the plot\n","\n","    Parameters:\n","      df_dl_z_list[i]: DataFrame containing downlink traffic data filtered by time.\n","      df_ul_z_list[i]: DataFrame containing upload traffic data filtered by time.\n","      'frame.time_relative': Column name for the x-axis data.\n","      'frame.len': Column name for the y-axis data.\n","      title: The title of the plot.\n","      'Time (s)': Label for the x-axis.\n","      'Packet size (bytes)': Label for the y-axis.\n","      True if the data should be grouped by protocol, False otherwise.\n","      False\n","      None\n","      True if x_axis should start from 0\n","      None if not specific protocol should be plotted, list of str specifying the specific protocl otherwise\n","\n","    \"\"\"\n","    create_stem_plot(df_dl_list[i], df_ul_list[i], 'frame.time_relative',\n","                        'frame.len', title, 'Time (ms)', 'Packet size (bytes)', True, False, None, True, ['UDP'])"]},{"cell_type":"markdown","metadata":{"id":"gPqL1ymUImui"},"source":["###### **Stem Zoomed - Non-Grouped**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AArEE2_cCqC9"},"outputs":[],"source":["plot_stem_matlab = True\n","if plot_stem_matlab:\n","  # Loop over all dataframes in df_list and plot packet size vs time for each one\n","  for i, df in enumerate(df_list):\n","    # Get the file name from the path\n","    file_name = os.path.splitext(os.path.basename(files_path_list[i]))[0]\n","    title = file_name\n","    \"\"\"\n","    Call the create_scatter_plot function to create the plot\n","\n","    Parameters:\n","      df_dl_z_list[i]: DataFrame containing downlink traffic data filtered by time.\n","      df_ul_z_list[i]: DataFrame containing upload traffic data filtered by time.\n","      'frame.time_relative': Column name for the x-axis data.\n","      'frame.len': Column name for the y-axis data.\n","      title: The title of the plot.\n","      'Time (s)': Label for the x-axis.\n","      'Packet size (bytes)': Label for the y-axis.\n","      True if the data should be grouped by protocol, False otherwise.\n","      False\n","      None\n","      True if x_axis should start from 0\n","      None if not specific protocol should be plotted, list of str specifying the specific protocl otherwise\n","\n","    \"\"\"\n","    create_stem_plot(df_dl_z_list[i], df_ul_z_list[i], 'frame.time_relative',\n","                        'frame.len', title, 'Time (ms)', 'Packet size (bytes)', True, False, None, True, None)"]},{"cell_type":"markdown","metadata":{"id":"I8kfAbXVhBbg"},"source":["#### **Traffic load (1s intervals)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeXsAmoKhTF0"},"outputs":[],"source":["# Create a list with the dataframes to plot\n","df_data = [df_traffic_dl_list, df_traffic_ul_list]\n","\n","# Create a list with the names of the dataframes\n","df_data_name = ['DL', 'UL']\n","\n","# Loop through the list of dataframes and their names\n","for dataframe, df_name in zip(df_data, df_data_name):\n","\n","  # Call the plot_traffic_load_over_time function with the dataframe, title, x label, y label, label list, and color list as arguments\n","  plot_traffic_load_over_time(dataframe, df_name + ' traffic over Time', 'Time (s)', 'Traffic Load (Mbps)', label_list , color_list )"]},{"cell_type":"markdown","metadata":{"id":"uRYiekZsNZJc"},"source":["#### **Traffic load (specific interval)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_u9OanM8M-Ci"},"outputs":[],"source":["# Set to True to make the plot.\n","plot_t_load_specific = False\n","# Specify the interval in seconds\n","interval = 500/1000 # (s)\n","\n","if plot_t_load_specific:\n","\n","  df_traffic_dl_list_r = [get_traffic_df(df_dl, interval) for df_dl in df_dl_list]\n","\n","  df_traffic_ul_list_r = [get_traffic_df(df_ul, interval) for df_ul in df_ul_list]\n","\n","  # Create a list with the dataframes to plot\n","  df_data = [df_traffic_dl_list_r, df_traffic_ul_list_r]\n","\n","  # Create a list with the names of the dataframes\n","  df_data_name = ['DL', 'UL']\n","\n","  # Loop through the list of dataframes and their names\n","  for dataframe, df_name in zip(df_data, df_data_name):\n","\n","    # Call the plot_traffic_load_over_time function with the dataframe, title, x label, y label, label list, and color list as arguments\n","    plot_traffic_load_over_time(dataframe, df_name + ' traffic over Time ' + 'resampled by ' + str(interval*1000) +   'ms', 'Time (s)', 'Traffic Load (Mbps)', label_list , color_list )"]},{"cell_type":"markdown","metadata":{"id":"R5nuovA6N9yi"},"source":["#### **Correlation (specific interval)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMzEDn6TBvYx"},"outputs":[],"source":["# Specify the interval in seconds\n","interval = 1000/1000 # (s)\n","\n","df_traffic_dl_list_r = [get_traffic_df(df_dl, interval) for df_dl in df_dl_list]\n","\n","df_traffic_ul_list_r = [get_traffic_df(df_ul, interval) for df_ul in df_ul_list]\n","\n","plot_correlation(df_traffic_dl_list_r, df_traffic_ul_list_r, interval)"]},{"cell_type":"markdown","metadata":{"id":"89pStBxhT8XV"},"source":["#### **ECDF**"]},{"cell_type":"markdown","metadata":{"id":"6aFKlP04XG-R"},"source":["##### **Non-grouped - Time-Filtered - Non by protocol**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K27M-AmMUR83"},"outputs":[],"source":["# Create a list with the dataframes to plot\n","df_data = [df_traffic_dl_list, df_traffic_ul_list]\n","\n","# Create a list with the names of the dataframes\n","df_data_name = ['DL', 'UL']\n","\n","plot_data = [('traffic', 'Traffic (Mbps)')]\n","\n","# Loop through the list of dataframes and their names using enumerate\n","for i, dataframe in enumerate(df_data):\n","\n","  # Get the name of the dataframe\n","  df_name = df_data_name[i]\n","\n","  # Loop through plot data and create ECDF plots\n","  for data in plot_data:\n","\n","    # Create the plot title using the dataframe name and ecdf column label\n","    plot_title = df_name + ' ' + data[1]\n","\n","    # Call the create_ecdf_plot function with the dataframe, title, x label, y label, label list, and color list as arguments\n","    create_ecdf_plot(dataframe, data[0] , plot_title, data[1], 'ECDF', label_list , color_list)"]},{"cell_type":"markdown","metadata":{"id":"_X6T4o6KYS8W"},"source":["##### **Non-grouped -Time-Filtered - By protocol**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbJYSNGWduuq"},"outputs":[],"source":["# Create a list with the dataframes to plot\n","df_data = [df_dl_list, df_ul_list]\n","\n","# Create a list with the names of the dataframes\n","df_data_name = ['Downlink', 'Uplink']\n","\n","# List of tuples containing the column for the ecdf and its label\n","plot_data = [('frame.len', 'Packet size (bytes)')\n","             ,('frame.time_relative', 'Inter-Packet time (ms)')\n","             ]\n","\n","# Loop through the list of dataframes and their names using enumerate\n","for i, df_list in enumerate(df_data): # df_dl_list\n","\n","  # Get the name of the dataframe\n","  df_name = df_data_name[i]\n","\n","  protocol_list = df_list[0]['_ws.col.Protocol'].unique()\n","\n","  filtered_df_list_by_protocol = []\n","\n","  # Loop through protocols\n","  for protocol in protocol_list:\n","\n","    filtered_df_list= []\n","\n","    # Loop through dataframes in list, to have them filtered by protocol\n","    for dataframe in df_list:\n","\n","        # Filter the dataframe by protocol\n","        filtered_df = dataframe[dataframe['_ws.col.Protocol'] == protocol]\n","        filtered_df_list.append(filtered_df)\n","\n","    filtered_df_list_by_protocol.append(filtered_df_list)\n","\n","  # Loop through plot data and create ECDF plots\n","  for data in plot_data:\n","    # Loop through filtered dataframe list for each protocol in the protocol list\n","    for j, protocol in enumerate(protocol_list):\n","      factor = 1\n","      diff = False\n","\n","      if data[0] == 'frame.time_relative':\n","        factor = 1000\n","        diff = True\n","\n","      # Create the plot title using the dataframe name, protocol, and ecdf column label\n","      plot_title = df_name + ' ' + protocol\n","\n","      # Call the create_ecdf_plot function with the protocol_df, title, x label, y label, label list, and color list as arguments\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'ECDF', ['Server', 'Client'], color_list, factor, diff)\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'ECDF', label_list, color_list, factor, diff)\n","      create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'ECDF', ['90 FPS - H264','60 FPS - H264', '30 FPS - H264', '30 FPS - VP9'], color_list, factor, diff)\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'ECDF', ['Ideal', 'Iperf 50 Mbps','Iperf 100 Mbps', 'Iperf 200 Mbps'], color_list, factor, diff)\n"]},{"cell_type":"markdown","metadata":{"id":"CsxK8C_rYV3E"},"source":["##### **Grouped - Time-Filtered - By protocol**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mL9qJjDu7qO"},"outputs":[],"source":["# Create a list with the dataframes to plot\n","df_data = [df_grouped_dl_list, df_grouped_ul_list]\n","#df_data = [[df_grouped_dl_list[0]], [df_grouped_ul_list[0]]]\n","#df_data = [df_grouped_ul_list]\n","df_data = [df_dl_list_grouped_mark, df_grouped_ul_list] # For video frames inter frame time\n","#df_data = [df_grouped_dl_list_srtp_correct, df_grouped_ul_list] # For batch time\n","# Create a list with the names of the dataframes\n","df_data_name = ['Downlink', 'Uplink']\n","\n","# List of tuples containing the column for the ecdf and its label\n","plot_data = [('group.frame.len.tot', 'Group size (Bytes)')\n","             ,('frame.time_relative', 'Inter-Frame time (ms)')\n","             ,('num.packets', 'Group Packet Count')\n","             ,('avg.time.between.packets', 'Group Inter-Packet time (ms)')\n","             ,('group.time', 'Group time (ms)')\n","             ]\n","\n","# Others - For video batches\n","#df_data = [df_grouped_dl_list_grouped_mark]\n","df_data = [df_grouped_dl_list_grouped_mark_not_nan]\n","\n","#df_data_name = ['Downlink']\n","plot_data = [('avg.time.between.groups', 'Inter-Batch time (ms)'), #inter batch time\n","            ('num.groups', 'Frame Batch number')]\n","\n","\n","# Loop through the list of dataframes and their names using enumerate\n","for i, df_list in enumerate(df_data): # df_dl_list\n","\n","  # Get the name of the dataframe\n","  df_name = df_data_name[i]\n","\n","  protocol_list = df_list[0]['_ws.col.Protocol'].unique()\n","\n","  filtered_df_list_by_protocol = []\n","\n","  # Loop through protocols\n","  for protocol in protocol_list:\n","\n","    filtered_df_list= []\n","\n","    # Loop through dataframes in list, to have them filtered by protocol\n","    for dataframe in df_list:\n","\n","        # Filter the dataframe by protocol\n","        filtered_df = dataframe[dataframe['_ws.col.Protocol'] == protocol]\n","        filtered_df_list.append(filtered_df)\n","\n","    filtered_df_list_by_protocol.append(filtered_df_list)\n","\n","  # Loop through plot data and create ECDF plots\n","  for data in plot_data:\n","    # Loop through filtered dataframe list for each protocol in the protocol list\n","    for j, protocol in enumerate(protocol_list):\n","\n","      # Create the plot title using the dataframe name, protocol, and ecdf column label\n","      plot_title = df_name + ' ' + protocol\n","\n","      factor = 1\n","      diff = False\n","\n","      if data[0] == 'frame.time_relative':\n","        factor = 1000\n","        diff = True\n","      # Call the create_ecdf_plot function with the protocol_df, title, x label, y label, label list, and color list as arguments\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'ECDF', label_list, color_list, factor, diff)\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'ECDF', ['Server','Client'], color_list, factor, diff)\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], '', data[1], 'ECDF', ['90 FPS - H264','60 FPS - H264', '30 FPS - H264', '30 FPS - VP9'], color_list, factor, diff)\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], '', data[1], 'ECDF', ['Server','Client'], color_list, factor, diff)\n","      #Change the colors when studying the backgroundbackground colors:\n","      background_colors = px.colors.qualitative.Plotly[0:1] + px.colors.qualitative.Plotly[7:7 + len(df_list)]\n","      colors_bk_study = background_colors\n","      create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], '', data[1], 'ECDF', ['0 Mbps', 'Iperf 50Mbps','Iperf 100Mbps','Iperf 200Mbps'], colors_bk_study, factor, diff)\n","      #create_ecdf_plot(filtered_df_list_by_protocol[j], data[0], '', data[1], 'ECDF', ['Laptop','Phone'], color_list, factor, diff)\n"]},{"cell_type":"markdown","metadata":{"id":"nHd0R141lcJU"},"source":["#### **Histogram**"]},{"cell_type":"markdown","metadata":{"id":"OE9x2ufPliQx"},"source":["##### **Non-grouped -Time-Filtered - By protocol**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KfDPZPmlatw"},"outputs":[],"source":["# Create a list with the dataframes to plot\n","df_data = [df_dl_list, df_ul_list]\n","\n","# Create a list with the names of the dataframes\n","df_data_name = ['DL', 'UL']\n","\n","# List of tuples containing the column for the ecdf and its label\n","plot_data = [('frame.len', 'Packet size (Bytes)'),\n","             ('frame.time_relative', 'Inter packet time (ms)')]\n","\n","# Loop through the list of dataframes and their names using enumerate\n","for i, df_list in enumerate(df_data): # df_dl_list\n","\n","  # Get the name of the dataframe\n","  df_name = df_data_name[i]\n","\n","  protocol_list = df_list[0]['_ws.col.Protocol'].unique()\n","\n","  filtered_df_list_by_protocol = []\n","\n","  # Loop through protocols\n","  for protocol in protocol_list:\n","\n","    filtered_df_list= []\n","\n","    # Loop through dataframes in list, to have them filtered by protocol\n","    for dataframe in df_list:\n","\n","        # Filter the dataframe by protocol\n","        filtered_df = dataframe[dataframe['_ws.col.Protocol'] == protocol]\n","        filtered_df_list.append(filtered_df)\n","\n","    filtered_df_list_by_protocol.append(filtered_df_list)\n","\n","  # Loop through plot data and create ECDF plots\n","  for data in plot_data:\n","    # Loop through filtered dataframe list for each protocol in the protocol list\n","    for j, protocol in enumerate(protocol_list):\n","\n","      # Create the plot title using the dataframe name, protocol, and ecdf column label\n","      plot_title = df_name + ' ' + protocol + ' ' + data[1]\n","\n","      factor = 1\n","      diff = False\n","\n","      if data[0] == 'frame.time_relative':\n","        factor = 1000\n","        diff = True\n","      # Call the create_ecdf_plot function with the protocol_df, title, x label, y label, label list, and color list as arguments\n","      create_histogram_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'Probability Density', label_list, color_list, factor, diff)\n"]},{"cell_type":"markdown","source":["##### **Grouped -Time-Filtered - By protocol**"],"metadata":{"id":"RXY9QJFu6zgT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyy6E5N5812i"},"outputs":[],"source":["# Create a list with the dataframes to plot\n","df_data = [df_grouped_dl_list, df_grouped_ul_list]\n","\n","# Create a list with the names of the dataframes\n","df_data_name = ['DL', 'UL']\n","\n","# List of tuples containing the column for the ecdf and its label\n","plot_data = [\n","             ('frame.time_relative', 'Inter Group time (ms)')]\n","\n","# Loop through the list of dataframes and their names using enumerate\n","for i, df_list in enumerate(df_data): # df_dl_list\n","\n","  # Get the name of the dataframe\n","  df_name = df_data_name[i]\n","\n","  protocol_list = df_list[0]['_ws.col.Protocol'].unique()\n","\n","  filtered_df_list_by_protocol = []\n","\n","  # Loop through protocols\n","  for protocol in protocol_list:\n","\n","    filtered_df_list= []\n","\n","    # Loop through dataframes in list, to have them filtered by protocol\n","    for dataframe in df_list:\n","\n","        # Filter the dataframe by protocol\n","        filtered_df = dataframe[dataframe['_ws.col.Protocol'] == protocol]\n","        filtered_df_list.append(filtered_df)\n","\n","    filtered_df_list_by_protocol.append(filtered_df_list)\n","\n","  # Loop through plot data and create ECDF plots\n","  for data in plot_data:\n","    # Loop through filtered dataframe list for each protocol in the protocol list\n","    for j, protocol in enumerate(protocol_list):\n","\n","      # Create the plot title using the dataframe name, protocol, and ecdf column label\n","      plot_title = df_name + ' ' + protocol + ' ' + data[1]\n","\n","      factor = 1\n","      diff = False\n","\n","      if data[0] == 'frame.time_relative':\n","        factor = 1000\n","        diff = True\n","      # Call the create_ecdf_plot function with the protocol_df, title, x label, y label, label list, and color list as arguments\n","      create_histogram_plot(filtered_df_list_by_protocol[j], data[0], plot_title, data[1], 'Probability Density', label_list, color_list, factor, diff)\n"]},{"cell_type":"markdown","metadata":{"id":"CFsybDqUZ1DM"},"source":["##### **Probability density functions fit**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmmCa-0IcwWa"},"outputs":[],"source":["from scipy import stats\n","\n","data = df_ul_list[0][df_ul_list[0]['_ws.col.Protocol'] == 'SRTCP']['frame.len']\n","\n","#data = df_dl_list[0][df_dl_list[0]['_ws.col.Protocol'] == 'UDP']['frame.time_relative'].diff()*1000\n","#data = data.dropna()\n","# Check if data is empty after removing NaN values\n","if data.empty:\n","    print(\"Data is empty after removing NaN values.\")\n","else:\n","    # Fit normal distribution\n","    loc, scale = stats.norm.fit(data)\n","\n","    # Generate x-values for plotting\n","    x = np.linspace(data.min(), data.max(), 100)\n","\n","    # Calculate the corresponding y-values for the fitted normal distribution\n","    y = stats.norm.pdf(x, loc=loc, scale=scale)\n","\n","    # Create histogram trace for data\n","    hist_trace = go.Histogram(x=data, histnorm='probability density', opacity=0.7, name='Data')\n","\n","    # Create line trace for fitted normal distribution\n","    line_trace = go.Scatter(x=x, y=y, mode='lines', name='Fitted Normal Distribution')\n","\n","    # Create layout\n","    layout = go.Layout(\n","        xaxis=dict(title='Value'),\n","        yaxis=dict(title='Density'),\n","        legend=dict(x=0.7, y=0.9)\n","    )\n","\n","    # Create figure\n","    fig = go.Figure(data=[hist_trace, line_trace], layout=layout)\n","    font_family = \"Times New Roman\"\n","    font_size = 20\n","    fig.update_layout(\n","        font=dict(family=font_family, size=font_size),\n","        template='plotly_white',\n","        xaxis=dict(showgrid=False), #,dtick=10, ,range=[75, 95]\n","        yaxis=dict(showgrid=False),\n","        legend=dict(\n","        orientation=\"h\",\n","        yanchor=\"bottom\",\n","        y=1.02,\n","        xanchor=\"right\",\n","        x=1)\n","    )\n","    fig.update_layout(\n","        autosize=False,\n","        width=600, #600 for some\n","        height=500)\n","    # Show the plot\n","    fig.show()\n","    print(loc, scale)\n","\n","# Check if data is empty after removing NaN values\n","if data.empty:\n","    print(\"Data is empty after removing NaN values.\")\n","else:\n","    # Fit Student's t-distribution\n","    df, loc, scale = stats.t.fit(data)\n","\n","    # Generate x-values for plotting\n","    x = np.linspace(data.min(), data.max(), 100)\n","\n","    # Calculate the corresponding y-values for the fitted Student's t-distribution\n","    y = stats.t.pdf(x, df=df, loc=loc, scale=scale)\n","\n","    # Create histogram trace for data\n","    hist_trace = go.Histogram(x=data,histnorm='probability density', opacity=0.7, name='Data')\n","\n","    # Create line trace for fitted Student's t-distribution\n","    line_trace = go.Scatter(x=x, y=y, mode='lines', name=\"Fitted Student's t-Distribution\")\n","\n","    # Create layout\n","    layout = go.Layout( xaxis=dict(title='Value'), yaxis=dict(title='Probability Density'),\n","                       legend=dict(x=0.7, y=0.9))\n","\n","    # Create figure\n","    fig = go.Figure(data=[hist_trace, line_trace], layout=layout)\n","    font_family = \"Times New Roman\"\n","    font_size = 20\n","    fig.update_layout(\n","        font=dict(family=font_family, size=font_size),\n","        template='plotly_white',\n","        xaxis=dict(showgrid=False), #,dtick=10, ,range=[75, 95]\n","        yaxis=dict(showgrid=False),\n","        legend=dict(\n","        orientation=\"h\",\n","        yanchor=\"bottom\",\n","        y=1.02,\n","        xanchor=\"right\",\n","        x=1)\n","    )\n","    fig.update_layout(\n","        autosize=False,\n","        width=600, #600 for some\n","        height=500)\n","    # Show the plot\n","    fig.show()\n","    print(loc, scale)\n","\n","# Check if data is empty after removing NaN values\n","if data.empty:\n","    print(\"Data is empty after removing NaN values.\")\n","else:\n","    # Fit Laplace distribution\n","    loc, scale = stats.laplace.fit(data)\n","\n","    # Generate x-values for plotting\n","    x = np.linspace(data.min(), data.max(), 100)\n","\n","    # Calculate the corresponding y-values for the fitted Laplace distribution\n","    y = stats.laplace.pdf(x, loc=loc, scale=scale)\n","\n","    # Create histogram trace for data\n","    hist_trace = go.Histogram(x=data, histnorm='probability density', opacity=0.7, name='SRTCP Data')\n","\n","    # Create line trace for fitted Laplace distribution\n","    line_trace = go.Scatter(x=x, y=y, mode='lines', name='Fitted Laplace Distribution')\n","\n","    # Create layout\n","    layout = go.Layout( xaxis=dict(title='Size (bytes)'), yaxis=dict(title='Probability Density'),\n","                       legend=dict(x=0.7, y=0.9))\n","\n","    # Create figure\n","    fig = go.Figure(data=[hist_trace, line_trace], layout=layout)\n","    font_family = \"Times New Roman\"\n","    font_size = 20\n","    fig.update_layout(\n","        font=dict(family=font_family, size=font_size),\n","        template='plotly_white',\n","        xaxis=dict(showgrid=False), #,dtick=10, ,range=[75, 95]\n","        yaxis=dict(showgrid=False),\n","        legend=dict(\n","        orientation=\"h\",\n","        yanchor=\"bottom\",\n","        y=1.02,\n","        xanchor=\"right\",\n","        x=1)\n","    )\n","    fig.update_layout(\n","        autosize=False,\n","        width=600, #600 for some\n","        height=500)\n","    # Show the plot\n","    fig.show()\n","    print(loc, scale)\n","\n","# Check if data is empty after removing NaN values\n","if data.empty:\n","    print(\"Data is empty after removing NaN values.\")\n","else:\n","    # Fit exponential distribution\n","    loc, scale = stats.expon.fit(data)\n","\n","    # Generate x-values for plotting\n","    x = np.linspace(data.min(), data.max(), 100)\n","\n","    # Calculate the corresponding y-values for the fitted exponential distribution\n","    y = stats.expon.pdf(x, loc=loc, scale=scale)\n","\n","    # Create histogram trace for data\n","    hist_trace = go.Histogram(x=data, histnorm='probability density', opacity=0.7, name='Data')\n","\n","    # Create line trace for fitted exponential distribution\n","    line_trace = go.Scatter(x=x, y=y, mode='lines', name='Fitted Exponential Distribution')\n","\n","    # Create layout\n","    layout = go.Layout(xaxis=dict(title='Value'), yaxis=dict(title='Probability Density'),\n","                       legend=dict(x=0.7, y=0.9))\n","\n","    # Create figure\n","    fig = go.Figure(data=[hist_trace, line_trace], layout=layout)\n","    font_family = \"Times New Roman\"\n","    font_size = 20\n","    fig.update_layout(\n","        font=dict(family=font_family, size=font_size),\n","        template='plotly_white',\n","        xaxis=dict(showgrid=False),\n","        yaxis=dict(showgrid=False),\n","        legend=dict(\n","            orientation=\"h\",\n","            yanchor=\"bottom\",\n","            y=1.02,\n","            xanchor=\"right\",\n","            x=1)\n","    )\n","    fig.update_layout(\n","        autosize=False,\n","        width=600,\n","        height=500)\n","    # Show the plot\n","    fig.show()\n","    print(loc, scale)\n","\n","# Check if data is empty after removing NaN values\n","if data.empty:\n","    print(\"Data is empty after removing NaN values.\")\n","else:\n","    # Fit gamma distribution\n","    shape, loc, scale = stats.gamma.fit(data, floc=0)\n","\n","    # Generate x-values for plotting\n","    x = np.linspace(data.min(), data.max(), 100)\n","\n","    # Calculate the corresponding y-values for the fitted gamma distribution\n","    y = stats.gamma.pdf(x, shape, loc, scale)\n","\n","    # Create histogram trace for data\n","    hist_trace = go.Histogram(x=data, histnorm='probability density', opacity=0.7, name='Data')\n","\n","    # Create line trace for fitted gamma distribution\n","    line_trace = go.Scatter(x=x, y=y, mode='lines', name='Fitted Gamma Distribution')\n","\n","    # Create layout\n","    layout = go.Layout(xaxis=dict(title='Value'), yaxis=dict(title='Probability Density'),\n","                       legend=dict(x=0.7, y=0.9))\n","\n","    # Create figure\n","    fig = go.Figure(data=[hist_trace, line_trace], layout=layout)\n","    font_family = \"Times New Roman\"\n","    font_size = 20\n","    fig.update_layout(\n","        font=dict(family=font_family, size=font_size),\n","        template='plotly_white',\n","        xaxis=dict(showgrid=False),\n","        yaxis=dict(showgrid=False),\n","        legend=dict(\n","            orientation=\"h\",\n","            yanchor=\"bottom\",\n","            y=1.02,\n","            xanchor=\"right\",\n","            x=1)\n","    )\n","    fig.update_layout(\n","        autosize=False,\n","        width=600,\n","        height=500)\n","    # Show the plot\n","    fig.show()\n","    print(loc, scale)\n","\n","\n","from sklearn.mixture import GaussianMixture\n","\n","# Convert pandas Series to NumPy array and reshape\n","data_array = data.values.reshape(-1, 1)\n","\n","# Fit Gaussian Mixture Model\n","gm = GaussianMixture(n_components=2)\n","gm.fit(data_array)\n","\n","# Generate x-values for plotting\n","x = np.linspace(data.min(), data.max(), 100).reshape(-1, 1)\n","\n","# Calculate the corresponding y-values for each component of the GMM\n","y = np.exp(gm.score_samples(x))\n","#y_normalized = y / np.sum(y)\n","# Create a scatter trace for data\n","hist_trace = go.Histogram(x=data, histnorm='probability density', opacity=0.7, name='DL Generic UDP Data')\n","\n","# Create a line trace for the GMM\n","gmm_trace = go.Scatter(x=x.flatten(), y=y, mode='lines', name='Gaussian Mixture Model')\n","\n","# Create layout\n","layout = go.Layout(\n","    xaxis=dict(title='Packet arrival (ms)'),\n","    yaxis=dict(title='Probability Density'),\n","    legend=dict(x=0.7, y=0.9)\n",")\n","\n","# Create figure\n","fig = go.Figure(data=[hist_trace, gmm_trace], layout=layout)\n","# Create figure\n","font_family = \"Times New Roman\"\n","font_size = 20\n","fig.update_layout(\n","    font=dict(family=font_family, size=font_size),\n","    template='plotly_white',\n","    xaxis=dict(showgrid=False),\n","    yaxis=dict(showgrid=False),\n","    legend=dict(\n","        orientation=\"h\",\n","        yanchor=\"bottom\",\n","        y=1.02,\n","        xanchor=\"right\",\n","        x=1)\n",")\n","fig.update_layout(\n","    autosize=False,\n","    width=600,\n","    height=500)\n","# Show the plot\n","fig.show()\n","\n","# Get the parameters of the GMM components\n","means = gm.means_  # Mean values of each component\n","covariances = gm.covariances_  # Covariance matrices of each component\n","\n","# Print the parameters\n","for i in range(gm.n_components):\n","    print(f\"Component {i+1}:\")\n","    print(f\"Mean: {means[i]}\")\n","    print(f\"Covariance Matrix:\\n{covariances[i]}\")\n","    print()\n","\n","# Example representation for the first component:\n","component_1 = f\"Gaussian({means[0]}, {covariances[0]})\"\n","\n","# Example representation for the second component:\n","component_2 = f\"Gaussian({means[1]}, {covariances[1]})\"\n","\n","# Full representation of the GMM:\n","gmm_representation = f\"GMM({component_1}, {component_2})\"\n","\n","print(\"GMM Representation:\")\n","print(gmm_representation)\n"]},{"cell_type":"markdown","metadata":{"id":"jkS5MIamFqJ4"},"source":["#### **Box Plots**"]},{"cell_type":"markdown","metadata":{"id":"gHY3t4PjTV6c"},"source":["##### **Non-grouped - Time-filtered**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTVDoq1xTWck"},"outputs":[],"source":["for i, dataframe in enumerate(df_list):\n","  box_plot([df_dl_list[i], df_ul_list[i]], os.path.splitext(os.path.basename(files_path_list[i]))[0], ['DL ', 'UL '], 'frame.len', '(bytes)')"]},{"cell_type":"markdown","metadata":{"id":"QLtgXhYRTWvL"},"source":["##### **Grouped - Time-filtered**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41xgXIqQyK5e"},"outputs":[],"source":["for i, dataframe in enumerate(df_list):\n","  box_plot([df_grouped_dl_list[i], df_grouped_ul_list[i]], os.path.splitext(os.path.basename(files_path_list[i]))[0], ['DL ', 'UL '], 'group.frame.len.tot', '(bytes)', None, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zouw9CsMyUK3"},"outputs":[],"source":["for i, dataframe in enumerate(df_list):\n","  box_plot([df_grouped_dl_z_list[i], df_grouped_ul_z_list[i]], os.path.splitext(os.path.basename(files_path_list[i]))[0], ['DL ', 'UL '], 'frame.time_relative', '(ms)', None, True, 1000, True)"]},{"cell_type":"markdown","metadata":{"id":"839F9Ki0eEqh"},"source":["##### **Other boxplots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWtaQKgZ91v1"},"outputs":[],"source":["#comparative_box_plot([df_dl_list , df_ul_list],['DL', 'UL'], '', ['AH','ITK ','ER'], 'frame.len', 'Packet size ', '(bytes)', None, True)\n","#comparative_box_plot([df_dl_list],['DL'], '', ['Alteration Hunting ','Interaction Toolkit ','The Escape Room '], 'frame.len', 'Packet size ', '(bytes)', None, True)\n","#comparative_box_plot([df_dl_list],['DL'], '', ['Alteration Hunting ','Interaction Toolkit ','The Escape Room '], 'frame.len', 'Packet size ', '(bytes)', None, True)\n","comparative_box_plot([df_dl_list , df_ul_list],['DL', 'UL'], '', ['Laptop','Phone'], 'frame.len', 'Packet size ', '(bytes)', None, True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ppXjdz-ax-Z"},"outputs":[],"source":["#comparative_box_plot([df_dl_list , df_ul_list],['DL', 'UL'], '', ['AH','ITK ','ER'], 'frame.time_relative', 'Inter-Packet Time ' ,'(ms)', None, True, 1000, True)\n","#comparative_box_plot([df_grouped_dl_list],['DL'], '', ['Alteration Hunting ','Interaction Toolkit ','The Escape Room '], 'frame.time_relative', 'Inter Packet Time ' ,'(ms)', None, True, 1000, True)\n","#comparative_box_plot([df_grouped_dl_list],['DL'], '', ['Alteration Hunting ','Interaction Toolkit ','The Escape Room '], 'frame.time_relative', 'Inter Packet Time ' ,'(ms)', ['STUN'], True, 1000, True)\n","comparative_box_plot([df_dl_list , df_ul_list],['DL', 'UL'], '', ['Laptop','Phone'], 'frame.time_relative', 'Inter-Packet Time ' ,'(ms)', None, True, 1000, True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aok8LMPBTKt8"},"outputs":[],"source":["comparative_box_plot_category([df_grouped_dl_list , df_grouped_ul_list],['DL', 'UL'], '', ['Alteration Hunting ','Interaction Toolkit ','The Escape room '], 'frame.time_relative', 'Inter Packet Time ' ,'(ms)', None, True, 1000, True)\n","#comparative_box_plot_category([df_grouped_dl_list],['DL'], '', ['Alteration Hunting ','Interaction Toolkit ','The Escape Room '], 'frame.time_relative', 'Inter Packet Time ' ,'(ms)', None, True, 1000, True)\n","#comparative_box_plot_category([df_grouped_dl_list],['DL'], '', ['Alteration Hunting ','Interaction Toolkit ','The Escape Room '], 'frame.time_relative', 'Inter Packet Time ' ,'(ms)', ['STUN'], True, 1000, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OREJ22NvoJf"},"outputs":[],"source":["comparative_box_plot([df_dl_list , df_ul_list],['DL', 'UL'], '', ['90FPS H264 ','60FPS H264 ','30FPS H264 ','90FPS VP9 ','60FPS VP9 ','30FPS VP9 '], 'frame.len', 'Packet size ' ,'(bytes)', ['SRTP Video'], True, 1, False)"]},{"cell_type":"markdown","metadata":{"id":"oKWaAqUY9Ksf"},"source":["##### **Traffic load comparison**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqmBS71y9p2G"},"outputs":[],"source":["# Create a list with the dataframes to plot\n","df_data = [df_traffic_dl_list, df_traffic_ul_list]\n","\n","# Create a list with the names of the dataframes\n","df_data_name = ['Downlink ', 'Uplink ']\n","game_label_list = ['Alteration Hunting', 'Interaction Toolkit Sample', 'The Escape Room']\n","# Loop through the list of dataframes and their names\n","for dataframe, df_name in zip(df_data, df_data_name):\n","  # Call the traffic_load_box_plot function with the arguments\n","  if game_label_list is None:\n","    game_label_list = label_list\n","  traffic_load_box_plot(dataframe, df_name + 'traffic', df_name , game_label_list)\n",""]},{"cell_type":"markdown","metadata":{"id":"6FcBAsmBGCD1"},"source":["### **Wireshark Demo Computations**"]},{"cell_type":"markdown","metadata":{"id":"LKrLa6Q_3Ari"},"source":["#### **Streams characteristics**"]},{"cell_type":"markdown","metadata":{"id":"KIpGexw97ts7"},"source":["##### **Non-grouped - Time-filtered**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bp2OC7ra9LsU"},"outputs":[],"source":["for i, dataframe in enumerate(df_list):\n","  streams_characteristics([df_dl_list[i], df_ul_list[i]], os.path.splitext(os.path.basename(files_path_list[i]))[0], ['DL ', 'UL '])"]},{"cell_type":"markdown","metadata":{"id":"TqwT9MqD2cfl"},"source":["##### **Grouped - Time-filtered**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxUWhLoWExqk"},"outputs":[],"source":["for i, dataframe in enumerate(df_list):\n","  # For any protocol\n","  streams_characteristics([df_grouped_dl_list[i], df_grouped_ul_list[i]], 'Grouped '+ os.path.splitext(os.path.basename(files_path_list[i]))[0], ['DL ', 'UL '], True)\n","  # For SRTP Video grouped by frame\n","  #streams_characteristics([df_dl_list_grouped_mark[i], df_grouped_ul_list[i]], 'Grouped '+ os.path.splitext(os.path.basename(files_path_list[i]))[0], ['DL ', 'UL '], True)\n","\n","  #SRTP Audio\n","  srtp_audio = False\n","  if srtp_audio:\n","    df_func = df_grouped_dl_list[i]\n","    df_filtered = df_func[df_func['_ws.col.Protocol'] == 'SRTP Audio']\n","    df_filtered['time_difference'] = df_filtered['frame.time_relative'].diff(periods=-1)*(-1)\n","    df_filtered['inter_batch_time'] = (df_filtered['end.time'] - df_filtered['start.time'].shift(-1))*(-1)\n","    avg_inter_batch_time = round(df_filtered['inter_batch_time'].mean()*1000, 4)\n","    std_inter_batch_time = round(df_filtered['inter_batch_time'].std()*1000, 4)\n","    num_pkt = [9,7]\n","    print('GLOBAL Avg. Inter-batch time',avg_inter_batch_time)\n","    print('GLOBAL Std. Inter-batch time',std_inter_batch_time)\n","    for j in num_pkt:\n","      df_filtered_copy = df_filtered.copy()\n","      df_filtered_copy = df_filtered_copy[df_filtered_copy['num.packets'] == j]\n","      avg_inter_group_time = round(df_filtered_copy['time_difference'].mean()*1000, 4)\n","      std_inter_group_time = round(df_filtered_copy['time_difference'].std()*1000, 4)\n","      avg_inter_batch_time = round(df_filtered_copy['inter_batch_time'].mean()*1000, 4)\n","      std_inter_batch_time = round(df_filtered_copy['inter_batch_time'].std()*1000, 4)\n","      print('------',j, '------')\n","      print('Avg. Inter-group time',avg_inter_group_time)\n","      print('Std. Inter-group time',std_inter_group_time)\n","      print(' Avg. Inter-batch time',avg_inter_batch_time)\n","      print(' Std. Inter-batch time',std_inter_batch_time)\n","\n","      #must be commented the part os streams_characteristics that computes intervals. TODO: add parameter\n","      streams_characteristics([df_filtered_copy, df_grouped_ul_list[i]], 'Grouped '+ os.path.splitext(os.path.basename(files_path_list[i]))[0], ['DL ', 'UL '], True)"]},{"cell_type":"markdown","metadata":{"id":"XwxiNPehJYex"},"source":["#### **Traffic characteristics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11of04BwGSsP"},"outputs":[],"source":["for i, dataframe in enumerate(df_list):\n","  traffic_stats([df_dl_list[i], df_ul_list[i]], os.path.splitext(os.path.basename(files_path_list[i]))[0], ['Downlink', 'Uplink'])"]},{"cell_type":"markdown","metadata":{"id":"FeIpRbQ8pgFx"},"source":["## **WebRTC**"]},{"cell_type":"markdown","metadata":{"id":"4qZdpMavvt-1"},"source":["### **Run WebRTC statistics Demo:**"]},{"cell_type":"markdown","metadata":{"id":"eJIuvb_Lu4DB"},"source":["**Insert files in list to plot results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1pv5FoWu4DB"},"outputs":[],"source":["webRTCfilesPath = [#'Datasets/90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","             # ,'Datasets/60fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - 60fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","            'Datasets/90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","              ,'Datasets/60fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - 60fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","              ,'Datasets/30fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - 30fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","             ,'Datasets/90fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz/Client/WebRTC Client - 90fps - 50Mbps - 3664x1920 - VP9- 80 Mhz.txt'\n","              ,'Datasets/60fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz/Client/WebRTC Client - 60fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz.txt'\n","              ,'Datasets/30fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz/Client/WebRTC Client - 30fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz.txt'\n","              , 'Datasets/90fps - 50Mbps - 2880x1600- H264 - 80 Mhz/Client/WebRTC Client - 90fps - 50Mbps - 2800x1600 - H264 - 80 Mhz.txt'\n","              ,'Datasets/90fps - 100Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - 90fps - 100Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","            #  ,'Datasets/Iperf 50Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - Iperf 50Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","             # ,'Datasets/Iperf 100Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - Iperf 100Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","              #,'Datasets/Iperf 200Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Client/WebRTC Client - Iperf 200Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.txt'\n","\n","                ]\n","\n","start_time_webrtc = 10\n","end_time_webrtc = 40\n","\n","from0 = True #x axis from 0\n","\n","###### DO NOT MODIFY THE CODE BELOW #################################################################################################################################################\n","df_client_rtc_list = []\n","info_client__list = []\n","timestamp_client_list = []\n","\n","df_server_rtc_list = []\n","info_server_list = []\n","timestamp_server_list = []\n","\n","webRTCfilesPath_client_list = []\n","webRTCfilesPath_server_list = []\n","\n","for filePath in webRTCfilesPath:\n","    df_rtc, info, side, timestamps_list = extract_webrtc_data(filePath, directory)\n","    if df_rtc is not None:\n","      if side == 'client':\n","        # Find the index of start_time_webrtc\n","        print(timestamps_list)\n","        start_index = timestamps_list.index(start_time_webrtc)\n","\n","        # Find the index of end_time_webrtc\n","        end_index = timestamps_list.index(end_time_webrtc)\n","        df_client_rtc_list.append(df_rtc[start_index:end_index+1])\n","        info_client__list.append(info)\n","        webRTCfilesPath_client_list.append(filePath)\n","        if from0:\n","          timestamp_list = timestamps_list[start_index:end_index+1]\n","          timestamp_client_list.append([timestamp - np.min(timestamp_list) for timestamp in timestamp_list])\n","\n","        else:\n","          timestamp_client_list.append(timestamps_list[start_index:end_index+1])\n","\n","      else:\n","        start_index = timestamps_list.index(start_time_webrtc)\n","\n","        # Find the index of end_time_webrtc\n","        end_index = timestamps_list.index(end_time_webrtc)\n","        df_server_rtc_list.append(df_rtc[start_index:end_index+1])\n","        info_server_list.append(info)\n","        webRTCfilesPath_server_list.append(filePath)\n","        if from0:\n","          timestamp_list = timestamps_list[start_index:end_index+1]\n","          timestamp_server_list.append([timestamp - min(timestamp_list)for timestamp in timestamp_list])\n","        else:\n","          timestamp_server_list.append(timestamps_list[start_index:end_index+1])\n","\n","\n","\n","#CLIENT\n","rtc_label_client_list = [os.path.splitext(os.path.basename(file_path))[0] for file_path in webRTCfilesPath_client_list]\n","# Number of colors to generate\n","rtc_n_colors_client = len(webRTCfilesPath_client_list)\n","# Generate a list of n_colors using the color palette\n","rtc_client_color_palette = sns.color_palette(n_colors=rtc_n_colors_client)\n","# Convert the color_palette to a list of RGB tuples\n","rtc_client_color_list = [tuple(map(lambda x: int(x*255), color)) for color in rtc_client_color_palette]\n","\n","#SERVER\n","rtc_label_server_list = [os.path.splitext(os.path.basename(file_path))[0] for file_path in webRTCfilesPath_server_list]\n","# Number of colors to generate\n","rtc_n_colors_server = len(webRTCfilesPath_server_list)\n","# Generate a list of n_colors using the color palette\n","rtc_server_color_palette = sns.color_palette(n_colors=rtc_n_colors_server)\n","# Convert the color_palette to a list of RGB tuples\n","rtc_server_color_list = [tuple(map(lambda x: int(x*255), color)) for color in rtc_server_color_palette]\n","\n","# WebRTC stats web: https://w3c.github.io/webrtc-stats/"]},{"cell_type":"markdown","metadata":{"id":"Q4cFINYF73rV"},"source":["### **Client**"]},{"cell_type":"markdown","metadata":{"id":"81w4OG0J6evy"},"source":["#### **WebRTC statistics Demo Figures**"]},{"cell_type":"markdown","metadata":{"id":"uqI-iz2IGEi9"},"source":["##### **Time plot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3WIMkFWFDDL"},"outputs":[],"source":["# Select which ones to plot\n","rtc_values = {\n","            'vid_frames_received_per_sec': 'Frames received per second',\n","            'vid_frames_per_sec': 'Decoded FPS',\n","            'vid_frames_decoded_per_sec': 'Frames decoded per second',\n","            'frames_rec_minus_decode_and_dropped': 'Frames lost per second',\n","            'frames_rec_minus_decode_and_dropped_tot': 'Total Frames lost',\n","            'vid_frames_dropped_per_sec': 'Frames dropped per second',\n","            'vid_packets_rec_per_sec': 'Video packets received per second',\n","            'vid_bits_rec_per_sec': 'Video Throughput (Mbps)',\n","            'vid_tot_packets_lost': 'Total Packets lost',\n","\n","            'vid_avg_jitter_buffer_delay': 'Jitter Buffer Delay ( ms)',\n","            'vid_jitter': 'Jitter (ms)',\n","            'total_rtt': 'Total RTT (ms)',\n","            'average_rtt': 'Avg. RTT (ms)',\n","            'current_rtt': 'RTT (ms)',\n","            'packets_sent_per_sec': 'Traffic sent (packets/s)',\n","            'bits_sent_per_sec': 'Traffic sent (Mbps)',\n","            'packets_rec_per_sec': 'Traffic received (packets/s)',\n","            'bits_rec_per_sec': 'Traffic received (Mbps)',\n","            'inter_frame_delay': 'Inter frame delay ( ms)',\n","            'inter_frame_delay_std': 'Inter frame delay std ( ms)',\n","            'dec_time_per_frame': 'Decoding time per frame ( ms)',\n","            'process_del_per_frame': 'Processing time per frame ( ms)',\n","            'assembly_time_per_frame': 'Assembly time per frame ( ms)',\n","            'discarded_pkt': 'Total discarded packets'\n","          }\n","# Plot metric vs time\n","for key, value in rtc_values.items():\n","  factor = 1\n","  if '(ms)' in value:\n","    factor = 1000\n","  elif  'Mbps' in value:\n","    factor = 1/1e6\n","  #create_time_plot(df_client_rtc_list, key, '', 'Time (s)', value, timestamp_client_list, rtc_label_client_list, rtc_client_color_list, factor)\n","\n","  # create_time_plot(df_client_rtc_list, key, '', 'Time (s)', value, timestamp_client_list, ['', '','','','','','','',''], rtc_client_color_list, factor)\n","  #create_time_plot(df_client_rtc_list, key, '', 'Time (s)', value, timestamp_client_list, ['H.264 - 90FPS', 'H.264 - 60FPS','H.264 - 30FPS','VP9 - 90FPS','VP9 - 60FPS','VP9 - 30FPS','2880x1600p','100Mbps'], rtc_client_color_list, factor)\n","  #Change the colors when studying the backgroundbackground colors:\n","  background_colors = px.colors.qualitative.Plotly[7:7 + len(df_client_rtc_list)]\n","  colors_bk_study = px.colors.qualitative.Plotly[0:1]+ background_colors\n","  create_time_plot(df_client_rtc_list, key, '', 'Time (s)', value, timestamp_client_list, ['Ideal', 'Iperf 50Mbps','Iperf 100Mbps','Iperf 200Mbps','','','','',''], colors_bk_study, factor)"]},{"cell_type":"markdown","metadata":{"id":"SreizCFTGHEb"},"source":["##### **ECDF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_dpayHu6qfh"},"outputs":[],"source":["# Select which ones to plot\n","rtc_values = {\n","            #'vid_frames_received_per_sec': 'Frames received per second',\n","            'vid_frames_per_sec': 'FPS',\n","            #'vid_frames_decoded_per_sec': 'Frames decoded per second',\n","            #'vid_tot_frames_dropped': 'Total Frames dropped',\n","            #'vid_packets_rec_per_sec': 'Video packets received per second',\n","            #'vid_bits_rec_per_sec': 'Video traffic received (Mbps)',\n","            #'vid_tot_packets_lost': 'Total Packets lost',\n","            'vid_avg_jitter_buffer_delay': 'Jitter Buffer Delay ( ms)',\n","            'vid_jitter': 'Jitter (ms)',\n","\n","            #'total_rtt': ' Total RTT (ms)',\n","            'average_rtt': 'Avg. RTT (ms)',\n","            'current_rtt': 'RTT (ms)',\n","\n","            #'packets_sent_per_sec': 'Traffic sent (packets/s)',\n","            'bits_sent_per_sec': 'Traffic sent (Mbps)',\n","            #'packets_rec_per_sec': 'Traffic received (packets/s)',\n","            'bits_rec_per_sec': 'Traffic received (Mbps)'\n","          }\n","\n","# Plot ECDF\n","for key, value in rtc_values.items():\n","  factor = 1\n","  if '(ms)' in value:\n","    factor = 1000\n","  elif  'Mbps' in value:\n","    factor = 1/1e6\n","  create_ecdf_plot(df_client_rtc_list, key, key, value, 'ECDF', rtc_label_client_list , rtc_client_color_list, factor)"]},{"cell_type":"markdown","metadata":{"id":"G6oy8mVjju9r"},"source":["##### **Bar plot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXDt-6KWjv6m"},"outputs":[],"source":["# Select which ones to plot\n","rtc_values = {\n","            'vid_frames_received_per_sec': 'Frames received per second',\n","            'vid_frames_per_sec': 'Decoded FPS',\n","            'vid_frames_decoded_per_sec': 'Frames decoded per second',\n","            'vid_frames_dropped_per_sec': 'Frames dropped per second',\n","            'frames_rec_minus_decode_and_dropped': 'Frames lost',\n","\n","            'vid_packets_rec_per_sec': 'Video packets received per second',\n","            'vid_bits_rec_per_sec': 'Video Throughput (Mbps)',\n","            'vid_tot_packets_lost': 'Total Packets lost',\n","            'vid_avg_jitter_buffer_delay': 'Jitter Buffer Delay ( ms)',\n","            'vid_jitter': 'Jitter (ms)',\n","            'total_rtt': 'Total RTT (ms)',\n","            'average_rtt': 'Avg. RTT (ms)',\n","            'current_rtt': 'RTT (ms)',\n","\n","            'packets_sent_per_sec': 'Traffic sent (packets/s)',\n","            'bits_sent_per_sec': 'Traffic sent (Mbps)',\n","            'packets_rec_per_sec': 'Traffic received (packets/s)',\n","            'bits_rec_per_sec': 'Traffic received (Mbps)',\n","            'inter_frame_delay': 'Inter frame delay ( ms)',\n","            'inter_frame_delay_std': 'Inter frame delay std ( ms)',\n","            'dec_time_per_frame': 'Avg. decoding delay ( ms)',\n","            'process_del_per_frame': 'Avg. processing delay ( ms)',\n","            'assembly_time_per_frame': 'Avg. assembly delay ( ms)'\n","          }\n","# Plot metric vs time\n","for key, value in rtc_values.items():\n","  factor = 1\n","  if '(ms)' in value:\n","    factor = 1000\n","  elif  'Mbps' in value:\n","    factor = 1/1e6\n","  #webrtc_grouped_bar_plot(df_client_rtc_list, key, '', value, ['', '','','','','','',''], rtc_client_color_list, factor)\n","  webrtc_grouped_bar_plot(df_client_rtc_list, key, '', value, ['H.264 - 90FPS', 'H.264 - 60FPS','H.264 - 30FPS','VP9 - 90FPS','VP9 - 60FPS','VP9 - 30FPS','2880x1600p','100Mbps'], rtc_client_color_list, factor) #last to True to show 99percentile"]},{"cell_type":"code","source":["# RTT with 99.999th\n","rtc_values = {\n","            'current_rtt': 'RTT (ms)',\n","          }\n","# Plot metric vs time\n","for key, value in rtc_values.items():\n","  factor = 1\n","  if '(ms)' in value:\n","    factor = 1000\n","  elif  'Mbps' in value:\n","    factor = 1/1e6\n","\n","  webrtc_grouped_bar_plot(df_client_rtc_list, key, '', value, ['H.264 - 90FPS', 'H.264 - 60FPS','H.264 - 30FPS','VP9 - 90FPS','VP9 - 60FPS','VP9 - 30FPS','2880x1600p','100Mbps'], rtc_client_color_list, factor, True, False, True) #last to True to show 99percentile"],"metadata":{"id":"o6flH6BHqwJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CKlG-mwKH_nQ"},"source":["#### **WebRTC Statistics Demo Computations**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNcF-ZwXIFyL"},"outputs":[],"source":["# Select which ones to get the statistics\n","rtc_values = {\n","            'vid_frames_received_per_sec': 'Avg. Frames received per second',\n","            'vid_frames_per_sec': 'Avg. FPS',\n","            'vid_frames_decoded_per_sec': 'Avg. Frames decoded per second',\n","            'vid_frames_dropped_per_sec': 'Avg. Frames dropped',\n","            'frames_rec_minus_decode_and_dropped': 'Avg. Frames lost per sec',\n","            'frames_rec_minus_decode_and_dropped_tot': 'Total Frames lost',\n","\n","            'vid_packets_rec_per_sec': 'Avg. Video packets received per second',\n","            'vid_bits_rec_per_sec': 'Avg. Video traffic received (Mbps)',\n","            'vid_tot_packets_lost': 'Total Packets lost',\n","            'vid_avg_jitter_buffer_delay': 'Avg. Jitter Buffer Delay ( ms)',\n","            'vid_jitter': 'Avg. Jitter (ms)',\n","\n","            'total_rtt': 'Total RTT (ms)',\n","            'average_rtt': 'Avg. RTT (ms)',\n","            'current_rtt': 'RTT (ms)',\n","            'packets_sent_per_sec': 'Avg. Traffic sent (packets/s)',\n","            'bits_sent_per_sec': 'Avg. Traffic sent (Mbps)',\n","            'packets_rec_per_sec': 'Avg. Traffic received (packets/s)',\n","            'bits_rec_per_sec': 'Avg. Traffic received (Mbps)',\n","             'inter_frame_delay': 'Inter frame delay ( ms)',\n","            'inter_frame_delay_std': 'Inter frame delay std ( ms)',\n","            'dec_time_per_frame': 'Decoding time per frame ( ms)',\n","            'process_del_per_frame': 'Processing time per frame ( ms)',\n","            'assembly_time_per_frame': 'Assembly time per frame ( ms)'\n","          }\n","for i in range(len(df_client_rtc_list)):\n","  webRtc_traffic_characteristics(df_client_rtc_list[i], os.path.splitext(os.path.basename(webRTCfilesPath_client_list[i]))[0] ,rtc_values, info_client__list[i])\n"]},{"cell_type":"markdown","metadata":{"id":"FkBBAGkYhFF9"},"source":["### **Server**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCbebT-9s_yU"},"outputs":[],"source":["webRTCfilesPath = [\n","            'Datasets/90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","            # ,'Datasets/60fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server - 60fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","             # ,'Datasets/30fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server - 30fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","             # ,'Datasets/90fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz/Server/WebRTC Server - 90fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz.json'\n","            #  ,'Datasets/60fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz/Server/WebRTC Server - 60fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz.json'\n","            #  ,'Datasets/30fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz/Server/WebRTC Server - 30fps - 50Mbps - 3664x1920 - VP9 - 80 Mhz.json'\n","            #  , 'Datasets/90fps - 50Mbps - 2880x1600- H264 - 80 Mhz/Server/WebRTC Server - 90fps - 50Mbps - 2800x1600 - H264 - 80 Mhz.json'\n","             # , 'Datasets/90fps - 100Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server - 90fps - 100Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","              ,'Datasets/Iperf 50Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server - Iperf 50Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","              ,'Datasets/Iperf 100Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server - Iperf 100Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","              ,'Datasets/Iperf 200Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server- Iperf 200Mbps - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","             # ,'Datasets/OculusLink - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz/Server/WebRTC Server - OculusLink - 90fps - 50Mbps - 3664x1920 - H264 - 80 Mhz.json'\n","                ]\n","\n","start_time_webrtc_server = 11 #has 1 second more\n","end_time_webrtc_server =41\n","\n","from0 = True #x axis from 0\n","\n","###### DO NOT MODIFY THE CODE BELOW #################################################################################################################################################\n","df_server_rtc_list = []\n","info_server_list = []\n","timestamp_server_list = []\n","\n","webRTCfilesPath_server_list = []\n","\n","for filePath in webRTCfilesPath:\n","    df_rtc, info, side, timestamps_list = extract_webrtc_data(filePath, directory)\n","    if df_rtc is not None:\n","      start_index = timestamps_list.index(start_time_webrtc_server)\n","\n","      # Find the index of end_time_webrtc\n","      end_index = timestamps_list.index(end_time_webrtc_server)\n","      df_server_rtc_list.append(df_rtc[start_index:end_index+1])\n","      info_server_list.append(info)\n","      webRTCfilesPath_server_list.append(filePath)\n","      if from0:\n","        timestamp_list = timestamps_list[start_index:end_index+1]\n","        timestamp_server_list.append([timestamp - np.min(timestamp_list) for timestamp in timestamp_list])\n","      else:\n","        timestamp_server_list.append(timestamps_list[start_index:end_index+1])\n","\n","\n","#SERVER\n","rtc_label_server_list = [os.path.splitext(os.path.basename(file_path))[0] for file_path in webRTCfilesPath_server_list]\n","# Number of colors to generate\n","rtc_n_colors_server = len(webRTCfilesPath_server_list)\n","# Generate a list of n_colors using the color palette\n","rtc_server_color_palette = sns.color_palette(n_colors=rtc_n_colors_server)\n","# Convert the color_palette to a list of RGB tuples\n","rtc_server_color_list = [tuple(map(lambda x: int(x*255), color)) for color in rtc_server_color_palette]\n","# WebRTC stats web: https://w3c.github.io/webrtc-stats/"]},{"cell_type":"markdown","metadata":{"id":"QnKtEmm-hFGA"},"source":["#### **WebRTC statistics Demo Figures**"]},{"cell_type":"markdown","metadata":{"id":"xRR9Bz4ehFGA"},"source":["##### **Time plot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AzWaJwAhFGA"},"outputs":[],"source":["# Select which ones to plot\n","rtc_values = {\n","            'vid_frames_sent_per_sec': 'Frames sent per second', #frames sent on the RTP stream\n","            'vid_frames_per_sec': 'Encoded FPS', # frames encoded\n","            'vid_frames_encoded_per_sec': 'Frames encoded per second', #frames succesfully encoded\n","            'vid_packets_sent_per_sec': 'Video packets sent per second',\n","            'vid_bits_sent_per_sec': 'Video traffic sent (Mbps)',\n","            'vid_packets_retransmitted_per_sec': 'Video packets retransmitted per second',\n","            'vid_bits_retransmitted_per_sec': 'Video traffic retransmitted (Mbps)',\n","            'vid_packets_lost_per_sec': 'Video packets lost per second',\n","            'vid_fraction_lost': 'Video Fraction lost',\n","            'vid_encode_time_per_sec': 'Video encode time per second (ms)',\n","            'vid_avg_encode_time': 'Video Avg. encode time (ms)',\n","            'vid_pkt_send_delay_per_sec': 'Video packet send delay per second (ms)',\n","            'total_rtt': ' Total RTT (ms)',\n","            'average_rtt': 'Avg. RTT (ms)',\n","            'current_rtt': 'RTT (ms)',\n","            'packets_sent_per_sec': 'Traffic sent (packets/s)',\n","            'bits_sent_per_sec': 'Traffic sent (Mbps)',\n","            'packets_rec_per_sec': 'Traffic received (packets/s)',\n","            'bits_rec_per_sec': 'Traffic received (Mbps)' ,\n","            'quality_res_changes': 'Avg. Number of quality res changes'\n","\n","          }\n","# Plot metric vs time\n","for key, value in rtc_values.items():\n","  factor = 1\n","  if '(ms)' in value:\n","    factor = 1000\n","  elif  'Mbps' in value:\n","    factor = 1/1e6\n","  #create_time_plot(df_server_rtc_list, key, '', 'Time (s)', value, timestamp_server_list, rtc_label_server_list, rtc_server_color_list, factor)\n","  #create_time_plot(df_server_rtc_list, key, '', 'Time (s)', value, timestamp_server_list, ['', '','','','','','','','',''], rtc_server_color_list, factor)\n"," # background_colors = px.colors.qualitative.Plotly[7:7 + len(df_server_rtc_list)]\n"," # colors_bk_study = px.colors.qualitative.Plotly[0:1]+ background_colors\n","  #create_time_plot(df_server_rtc_list, key, '', 'Time (s)', value, timestamp_server_list, ['Ideal', 'Iperf 50Mbps','Iperf 100Mbps','Iperf 200Mbps','','','','',''], colors_bk_study, factor)\n","  oculus_colors = px.colors.qualitative.Plotly[4:4 + len(df_server_rtc_list)]\n","  colors_oculus_study = px.colors.qualitative.Plotly[0:1]+ oculus_colors\n","  create_time_plot(df_server_rtc_list, key, '', 'Time (s)', value, timestamp_server_list, ['Laptop Remote', 'Oculus Link','','','',''], colors_oculus_study, factor)\n"]},{"cell_type":"markdown","metadata":{"id":"hvEfb1wmhFGA"},"source":["##### **ECDF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3oXg851hFGA"},"outputs":[],"source":["# Select which ones to plot\n","rtc_values = {\n","            'vid_frames_sent_per_sec': 'Frames sent per second',\n","            'vid_frames_per_sec': 'FPS',\n","            'vid_frames_encoded_per_sec': 'Frames encoded per second',\n","            'vid_packets_sent_per_sec': 'Video packets sent per second',\n","            'vid_bits_sent_per_sec': 'Video traffic sent (Mbps)',\n","            'vid_packets_retransmitted_per_sec': 'Video packets retransmitted per second',\n","            'vid_bits_retransmitted_per_sec': 'Video traffic retransmitted (Mbps)',\n","            'vid_encode_time_per_sec': 'Video encode time per second (ms)',\n","            'vid_avg_encode_time': 'Video Avg. encode time (ms)',\n","            'vid_pkt_send_delay_per_sec': 'Video packet send delay per second (ms)',\n","            'vid_packets_lost_per_sec': ' Video packet lost per second',\n","            'total_rtt': ' Total RTT (ms)',\n","            'average_rtt': 'Avg. RTT (ms)',\n","            'packets_sent_per_sec': 'Traffic sent (packets/s)',\n","            'bits_sent_per_sec': 'Traffic sent (Mbps)',\n","            'packets_rec_per_sec': 'Traffic received (packets/s)',\n","            'bits_rec_per_sec': 'Traffic received (Mbps)'\n","          }\n","\n","# Plot ECDF\n","for key, value in rtc_values.items():\n","  factor = 1\n","  if '(ms)' in value:\n","    factor = 1000\n","  elif  'Mbps' in value:\n","    factor = 1/1e6\n","  create_ecdf_plot(df_server_rtc_list, key, key, value, 'ECDF', rtc_label_server_list , rtc_server_color_list, factor)\n"]},{"cell_type":"markdown","metadata":{"id":"j33m_gwChFGA"},"source":["#### **WebRTC Statistics Demo Computations**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdVK5N4ohFGA"},"outputs":[],"source":["# Select which ones to plot\n","rtc_values = {\n","            'vid_frames_sent_per_sec': 'Avg. Frames sent per second',\n","            'vid_frames_per_sec': 'Avg. FPS',\n","            'vid_frames_encoded_per_sec': 'Avg. Frames encoded per second',\n","            'vid_packets_sent_per_sec': 'Avg. Video packets sent per second',\n","            'vid_bits_sent_per_sec': 'Avg. Video traffic sent (Mbps)',\n","            'vid_packets_retransmitted_per_sec': 'Avg. Video packets retransmitted per second',\n","            'vid_bits_retransmitted_per_sec': 'Avg. Video traffic retransmitted (Mbps)',\n","            'vid_encode_time_per_sec': 'Avg.Video encode time per second (ms)',\n","            'vid_avg_encode_time': 'Avg. Video encode time (ms)',\n","            'vid_pkt_send_delay_per_sec': 'Avg. Video packet send delay per second (ms)',\n","            'vid_packets_lost_per_sec': 'Avg. Video packet lost per second',\n","            'total_rtt': ' Total RTT (ms)',\n","            'average_rtt': 'Avg. RTT (ms)',\n","            'current_rtt': 'RTT (ms)',\n","            'packets_sent_per_sec': 'Avg. Traffic sent (packets/s)',\n","            'bits_sent_per_sec': 'Avg. Traffic sent (Mbps)',\n","            'packets_rec_per_sec': 'Avg. Traffic received (packets/s)',\n","            'bits_rec_per_sec': 'Avg. Traffic received (Mbps)' ,\n","            'quality_res_changes': 'Avg. Number of quality res changes'\n","          }\n","\n","for i in range(len(df_server_rtc_list)):\n","  webRtc_traffic_characteristics(df_server_rtc_list[i], os.path.splitext(os.path.basename(webRTCfilesPath_server_list[i]))[0] ,rtc_values, info_server_list[i])\n"]}],"metadata":{"colab":{"collapsed_sections":["652i80gCdCcc","7YcWvVfxu0Hm","TPx0cFUXvnVu","12fFVJbEgmlE","e_-zAQ2Ngv2O","1BmfveDbg2qY","7uFE_k0og9KG","JjiCqAQxhFxb","QMW3pcUoERXr","wFAd8GQJbrKR","DjDz0ly5bzvC","6i5kX9_jl0sZ","cbVdS_7YW_EL","KjVkjcOZbZnr","wKkYEkq6rs1s","1BZjW2Joe2pD","CgZz0-TC_R9b","cd029UU7LeEM","C9Ictxu5h9Ou","_iFKflA5Of-3","kUcHje42kppc","cYRUVA0oTqwN","UN5png9vgyDe","vsrVXtuQjof9","qe8SJU1_HPpl","38x0hWq8p_5e","m8uwKtEKHkBT","6-x5lcNp5_s9","RzBrGRlHeSTm","JRSslAGWu-2b","OltgCjNjFmG0","XrBHDyg12KLs","iiF41_xh2RPH","7ocMUHzF15jr","fIGFGCXSVDun","-bm43cx8Ip_R","gPqL1ymUImui","I8kfAbXVhBbg","uRYiekZsNZJc","R5nuovA6N9yi","6aFKlP04XG-R","CsxK8C_rYV3E","nHd0R141lcJU","CFsybDqUZ1DM","jkS5MIamFqJ4","gHY3t4PjTV6c","QLtgXhYRTWvL","839F9Ki0eEqh","KIpGexw97ts7","DPc-XcMnMDSi","8Etzl3JG9g9a","FeIpRbQ8pgFx","4qZdpMavvt-1","Q4cFINYF73rV","hvEfb1wmhFGA"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}